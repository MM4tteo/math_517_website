<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Non-parametric Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="04_Smoothing_files/libs/clipboard/clipboard.min.js"></script>
<script src="04_Smoothing_files/libs/quarto-html/quarto.js"></script>
<script src="04_Smoothing_files/libs/quarto-html/popper.min.js"></script>
<script src="04_Smoothing_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="04_Smoothing_files/libs/quarto-html/anchor.min.js"></script>
<link href="04_Smoothing_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="04_Smoothing_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="04_Smoothing_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="04_Smoothing_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="04_Smoothing_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#local-constant-regression" id="toc-local-constant-regression" class="nav-link active" data-scroll-target="#local-constant-regression">1. Local Constant Regression</a></li>
  <li><a href="#local-polynomial-regression" id="toc-local-polynomial-regression" class="nav-link" data-scroll-target="#local-polynomial-regression">2. Local Polynomial Regression</a></li>
  <li><a href="#local-linear-regression" id="toc-local-linear-regression" class="nav-link" data-scroll-target="#local-linear-regression">3. Local Linear Regression</a>
  <ul class="collapse">
  <li><a href="#bandwidth-selection" id="toc-bandwidth-selection" class="nav-link" data-scroll-target="#bandwidth-selection">3.1 Bandwidth Selection</a></li>
  <li><a href="#why-local-linear-is-the-order-of-choice" id="toc-why-local-linear-is-the-order-of-choice" class="nav-link" data-scroll-target="#why-local-linear-is-the-order-of-choice">3.2 Why Local Linear is the Order of Choice?</a></li>
  </ul></li>
  <li><a href="#loess-and-lowess" id="toc-loess-and-lowess" class="nav-link" data-scroll-target="#loess-and-lowess">4. <code>loess()</code> and <code>lowess()</code></a></li>
  <li><a href="#smoothing-splines" id="toc-smoothing-splines" class="nav-link" data-scroll-target="#smoothing-splines">5. Smoothing Splines</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Non-parametric Regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Suppose we observe i.i.d. copies of a bivariate random vector <span class="math inline">\((X,Y)^\top\)</span>, that is a random sample <span class="math inline">\((X_1,Y_1)^\top, \ldots, (X_n, Y_n)^\top\)</span>, and we are interested in modeling the conditional expectation of the response variable <span class="math inline">\(Y\)</span> given the single predictor variable <span class="math inline">\(X\)</span>, i.e.&nbsp;<span class="math display">\[ m(x) := \mathbb{E}\big[ Y \big| X = x \big] \]</span> In linear regression, we assume a parametric model <span class="math inline">\(m(x) = \beta_0 + \beta_1 x\)</span>. Here, we would like to make no such structural assumption, instead only assuming that <span class="math inline">\(m\)</span> is sufficiently smooth. How to estimate <span class="math inline">\(m = m(x)\)</span> from the observed data non-parametrically?</p>
<section id="local-constant-regression" class="level1">
<h1>1. Local Constant Regression</h1>
<p>The simplest idea would be to use what we learned in the previous chapter, i.e.&nbsp;kernel density estimation. Denoting <span class="math inline">\(f_X(x)\)</span> the marginal density of <span class="math inline">\(X\)</span>, <span class="math inline">\(f_{X,Y}(x,y)\)</span> the joint density of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and <span class="math inline">\(f_{Y|X}(y|x)\)</span> the marginal density of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, we can express the conditional expectation above as <span class="math display">\[ m(x) = \mathbb{E}\big[ Y \big| X = x \big] = \int_\mathbb{R} y f_{Y|X}(y|x) dy = \frac{\int_\mathbb{R} y f_{X,Y}(x,y) dy}{f_X(x)} \]</span> Plugging in the KDEs of <span class="math inline">\(f_{X|Y}(x,y)\)</span> and <span class="math inline">\(f_X(x)\)</span> (with a fixed bandwidth <span class="math inline">\(h\)</span>, a fixed kernel <span class="math inline">\(K(\cdot)\)</span> for the univariate density, and the separable kernel <span class="math inline">\(K(\cdot)K(\cdot)\)</span> for the bivariate density), i.e.&nbsp;<span class="math display">\[ \widehat{f}_X(x) = \frac{1}{nh} \sum_{i=1}^n K\left( \frac{X_i-x}{h} \right) \qquad \&amp; \qquad \widehat{f}_{X,Y}(x,y) = \frac{1}{n h^2} \sum_{i=1}^n K\left( \frac{X_i-x}{h} \right) K\left( \frac{Y_i-y}{h} \right), \]</span> into the previous formula, we obtain an estimator of the conditional expectation: <span class="math display">\[ \widehat{m}(x) = \frac{\sum_{i=1}^n K\left( \frac{X_i-x}{h} \right) Y_i}{ \sum_{i=1}^n K\left( \frac{X_i-x}{h} \right)} \]</span> where we used that <span class="math inline">\(\frac{1}{h} \int_\mathbb{R} y K\left( \frac{Y_i-x}{h} \right) dy = \int_\mathbb{R} (Y + th) K(t) dt = Y\)</span>, where we used the symmetry of the kernel (twice).</p>
<p>Note that the previous estimator is just a weighted average, and a weighted average is a solution to the weighted least squares problem, in this case: <span class="math display">\[ \widehat{m}(x) = \underset{\beta_0 \in \mathbb{R}}{\mathrm{arg \; min}} \sum_{i=1}^n K\left( \frac{X_i-x}{h} \right) (Y_i - \beta_0)^2 \]</span> So the conditional expectation can be estimated at a fixed location <span class="math inline">\(x\)</span> (i.e.&nbsp;locally) by an intercept-only linear model (i.e.&nbsp;constant). Hence we call this estimator <em>local constant regression</em>.</p>
</section>
<section id="local-polynomial-regression" class="level1">
<h1>2. Local Polynomial Regression</h1>
<p>Assuming now that <span class="math inline">\(m\)</span> is <span class="math inline">\(p\)</span>-times differentiable on the neighborhood of <span class="math inline">\(x\)</span>, we can invoke the following Taylor approximation: <span class="math display">\[m(X_i) \approx m(x) + (X_i-x) m'(x) + \frac{(X_i-x)^2}{2!} m''(x) + \ldots + \frac{(X_i-x)^p}{p!} m^{p}(x).\]</span></p>
<p>In words, <span class="math inline">\(m\)</span> can be locally viewed as a polynomial of <span class="math inline">\(p\)</span>-th order, and we can write <span class="math display">\[Y_i = m(X_i) + \epsilon_i,\]</span> where <span class="math inline">\(\epsilon_i = Y_i - m(X_i)\)</span> is the error. For our imagination, we can think of <span class="math inline">\(\epsilon\)</span>’s being i.i.d. Gaussians, which is often assumed, but less often really needed (e.g.&nbsp;it is not needed for the bias-variance calculations we perform below).</p>
<p>For a fixed <span class="math inline">\(x\)</span>, consider the following least squares problem: <span class="math display">\[\widehat{\beta}(x) = (\widehat{\beta}_0(x),\ldots, \widehat{\beta}_p(x)) = \mathrm{arg\,min}_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}} \sum_{i=1}^n [Y_i - \beta_0 - \beta1(X_i-x) - \ldots - \beta_p(X_i-x)^p]^2 K\left(\frac{X_i-x}{h}\right),\]</span> which is again a weighted least squares problem with</p>
<ul>
<li>weights governed by the kernel function <span class="math inline">\(K(\cdot)\)</span>,</li>
<li>a solution (for a fixed <span class="math inline">\(x\)</span>) having an explicit expression and given by <code>lm()</code>.</li>
</ul>
<p>Notice from the Taylor expansion above that <span class="math inline">\(\widehat{\beta}_j(x)\)</span> estimates <span class="math inline">\(\frac{m^{(j)}(x)}{j!}\)</span>. Thus, if we are interested only in <span class="math inline">\(m(x)\)</span>, which is typically the case, we only retain <span class="math inline">\(\widehat{\beta}_0(x)\)</span>. However, this estimator will generally be different than the one from the local constant regression above. At the same time, we immediately obtain also estimates for the derivatives of <span class="math inline">\(m\)</span> (depending on the order <span class="math inline">\(p\)</span> we choose).</p>
</section>
<section id="local-linear-regression" class="level1">
<h1>3. Local Linear Regression</h1>
<p>Choosing the order <span class="math inline">\(p=1\)</span> leads to the local linear estimator <span class="math display">\[(\widehat{\beta}_0(x), \widehat{\beta}_1(x)) = \mathrm{arg\,min}_{\mathbf b \in \mathbb{R}^{2}} \sum_{i=1}^n [Y_i - \beta_0 - \beta_1(X_i-x)]^2 K\left(\frac{X_i-x}{h}\right),\]</span> which provides an estimate for the regression function <span class="math inline">\(\widehat{m}(x) = \widehat{\beta}_0(x)\)</span> (and also for its derivative <span class="math inline">\(\widehat{m}'(x) = \widehat{\beta}_1(x)\)</span>).</p>
<p>It can be shown (quite easily, but quite tediously) that <span class="math inline">\(\widehat{\beta}_0(x) = \sum_{i=1}^n w_{ni}(x) Y_i\)</span>, where <span class="math display">\[w_{ni}(x) = \frac{\frac{1}{nh}K\left(\frac{X_i-x}{h}\right)\Big[ S_{n,2}(x) - \left(X_i-x\right) S_{n,1}(x) \Big]}{S_{n,0}(x) S_{n,2}(x) - S_{n,1}^2(x)}
\quad \mathrm{with} \quad
S_{n,k}(x) = \frac{1}{nh}\sum_{i=1}^n \left(X_i-x\right)^k K\left(\frac{X_i-x}{h}\right)
\]</span> so <span class="math inline">\(\sum_{i=1}^n w_{ni}(x) = 1\)</span> (but not necessarily <span class="math inline">\(w_{ni}(x) \geq 0\)</span>, if <span class="math inline">\(x\)</span> happens to be close to the minimum or maximum of <span class="math inline">\(X\)</span>).</p>
<p>Now, similarly (but much more tediously) to KDE, asymptotic expressions (containing stochastic little-o and big-O terms) can be found for <span class="math inline">\(S_{n,k}(x)\)</span> (under certain assumptions on the limiting behaviour of <span class="math inline">\(h_n\)</span> as well as the density of <span class="math inline">\(X\)</span>), and those can be collected into the following bias and variance formulas: <span class="math display">\[
\begin{split}
\mathrm{bias}\{\widehat{m}(x)\} &amp;= \frac{1}{2} m''(x) h_n^2 \int z^2 K(z) dz + \mathcal{o}_P(h^2_n)\\
\mathrm{var}\{\widehat{m}(x)\} &amp;= \frac{\sigma^2(x) \int [K(z)]^2 dz}{f_X(x) n h_n} + \mathcal{o}_P\left( \frac{1}{n h_n} \right)
\end{split}
\]</span> where <span class="math inline">\(\sigma^2(x) = \mathrm{var}(Y_1|X_1=x)\)</span> is the conditional/local variance.</p>
<p>These formulas are pertinent only to the local linear estimator. For other orders <span class="math inline">\(p\)</span>, they look differently (but are obtained by the same means). The bias and variance formulas here share many similarities to those developed for KDEs last week. In particular, we still need <span class="math inline">\(h = h_n \to 0\)</span> in order for bias to disappear asymptotically and <span class="math inline">\(n h_n \to \infty\)</span> in order for the variance to disappear asymptotically. But we also need <span class="math inline">\(n h_n^3 \to \infty\)</span> here for the calculations above (one of the “certain assumptions”), so basically we need to be even more careful when choosing the bandwidth <span class="math inline">\(h\)</span> for local regression compared to KDE.</p>
<section id="bandwidth-selection" class="level2">
<h2 class="anchored" data-anchor-id="bandwidth-selection">3.1 Bandwidth Selection</h2>
<p>Similarly to what we did last week with KDEs, we consider <span class="math display">\[MSE\{\widehat{m}(x)\} = \mathrm{var}\{\widehat{m}(x)\} + \big[\mathrm{bias}\{\widehat{m}(x)\}\big]^2\]</span> and, dropping the little-o terms, we obtain <span class="math display">\[AMSE\{\widehat{m}(x)\} = \frac{\sigma^2(x) \int [K(z)]^2 dz}{f_X(x) n h_n} + \frac{1}{4} \big\{m''(x)\big\}^2 h_n^4 \bigg\lbrace \int z^2 K(z) dz\bigg\rbrace^2.\]</span></p>
<p>Now, a local bandwidth choice can be obtained by optimizing AMSE. Taking derivatives and setting them to zero, we obtain <span class="math display">\[h_{opt}(x) = n^{-1/5} \left[ \frac{\sigma^2(x) \int \lbrace K(z)\rbrace^2 dz}{\big\lbrace m''(x) \int z^2 K(z) dz\big\rbrace^2 f_X(x)} \right]^{1/5}.\]</span></p>
<p>This is somewhat more complicated compared to the KDE case, because we have to estimate</p>
<ul>
<li>the marginal density <span class="math inline">\(f_X(x)\)</span>,
<ul>
<li>let’s say that we already know how to do this, e.g.&nbsp;by KDE even though that requires choice of yet another bandwidth</li>
</ul></li>
<li>the local variance function <span class="math inline">\(\sigma^2(x)\)</span>, and</li>
<li>the second derivative of the regression functions <span class="math inline">\(m''(x)\)</span>.</li>
</ul>
<p>Rule of thumb algorithms exist to actually obtain some <span class="math inline">\(h_{opt}(x)\)</span> in practice starting from some initial bandwidth:</p>
<ul>
<li>use a higher order <span class="math inline">\(p\)</span> (e.g., <span class="math inline">\(p=3\)</span>) to estimate <span class="math inline">\(m(x)\)</span> with an inflated value of the bandwidth, and obtain an estimate of <span class="math inline">\(m''(x)\)</span></li>
<li>assume that <span class="math inline">\(\sigma^2(x) = \sigma^2\)</span> is constant and estimate it from the residuals as <span class="math inline">\(\frac{1}{n-d} \sum_{i=1}^n \{Y_i - \widetilde{m}(X_i)\}^2\)</span>, where <span class="math inline">\(\widetilde{m}\)</span> is the estimator from the previous step and <span class="math inline">\(d\)</span> is the degrees of freedom of <span class="math inline">\(\widetilde{m}\)</span></li>
<li>update the current bandwidth by the <span class="math inline">\(h_{opt}(x)\)</span> formula above</li>
</ul>
<p>and iterate these steps a couple of times.</p>
<p>Again, like in the case of KDEs, the global bandwidth choice can be obtained by integration:</p>
<ul>
<li>calculate <span class="math inline">\(AMISE(\widehat{m}) = \int AMSE(\widehat{m}(x)) f_X(x) dx\)</span>, and</li>
<li>set <span class="math inline">\(h_{opt} = \mathrm{arg\,min}_{h&gt;0} AMISE(\widehat{m})\)</span>,</li>
</ul>
<p>but the situation is not simplified very much.</p>
<p><em>Note</em>: Next week we are going to study cross-validation, which is a much simpler, data-driven (i.e., without asymptotic arguments), and popular bandwidth (or more generally any tuning parameter) selection strategy.</p>
</section>
<section id="why-local-linear-is-the-order-of-choice" class="level2">
<h2 class="anchored" data-anchor-id="why-local-linear-is-the-order-of-choice">3.2 Why Local Linear is the Order of Choice?</h2>
<p>Bias and variance can be calculated similarly also for higher order local polynomial regression estimators. In general:</p>
<ul>
<li>bias decreases with an increasing order</li>
<li>variance increases with increasing order, but only for <span class="math inline">\(p=2k+1 \to p+1\)</span>, i.e., when increasing an odd order to an even one</li>
</ul>
<p>For this reason, odd orders are preferred to even ones, and <span class="math inline">\(p=1\)</span> is easy to grasp as it corresponds to locally fitted simple regression line. Contrarily, it is hard to argue why <span class="math inline">\(p=3\)</span> or higher should be used. In terms of degrees of freedom (and in fact also the convergence rates, which we are not showing) increasing the order has a similar effect as increasing the bandwidth. So it is reasonable to assume with a higher <span class="math inline">\(p\)</span>, lower <span class="math inline">\(h\)</span> will be optimal, and vice versa. In practice, one simply fixes <span class="math inline">\(p=1\)</span> and only tunes the bandwidth <span class="math inline">\(h\)</span>.</p>
</section>
</section>
<section id="loess-and-lowess" class="level1">
<h1>4. <code>loess()</code> and <code>lowess()</code></h1>
<p><code>loess()</code> (LOcal regrESSion) should be local linear regression with tricube kernel</p>
<p><code>lowess()</code> (LOcally WEighted Scatterplot Smoothing) is an iterative algorithm:</p>
<ul>
<li>fit <code>loess()</code></li>
<li>repeat
<ul>
<li>calculate residuals of the current fit<br>
</li>
<li>calculate weights based on the (MAD of the) residuals</li>
<li>fit weighted (product of the kernel and residuals-based weights) <code>loess()</code></li>
</ul></li>
<li>until convergence (or just 3-times)</li>
</ul>
<p>The goal of <code>lowess()</code> is to improve <code>loess()</code> by making it <em>robust</em> to outliers.</p>
</section>
<section id="smoothing-splines" class="level1">
<h1>5. Smoothing Splines</h1>
<p>Let’s go back to least squares (there is a natural connection to likelihood, if we assume Gaussianity) and penalize for roughness (2nd derivative is the most convenient one): <span class="math display">\[\mathrm{arg\,min}_{g \in C^2} \sum_{i=1}^n \big\lbrace Y_i - g(X_i) \big\rbrace^2 + \lambda \int \big\lbrace g''(x) \big\rbrace^2 dx,\]</span> where <span class="math inline">\(\lambda &gt; 0\)</span> is a tuning parameter controlling smoothness.</p>
<p>Remarkably, the solution to this optimization problem is known to be the natural cubic spline. While the optimization problem is over an infinite-dimensional space (of functions with square-integrable second derivatives, which is a Sobolev space – of all Sobolev spaces, only the one with order two is RKHS and hence allows for a finite-dimensional solution), knowing that the solution is the natural cubic spline, it can be re-cast as a finite-dimensional problem, not too dissimilar to ridge regression.</p>
<p>While in the case of natural cubic splines there is some beautiful math justifying why we look for functions that can be written as linear combinations of predefined functions (which is a parametric problem, but the beautiful math describes how this is related to the non-parametric, infinite-dimensional problem), we can also adopt a more brute viewpoint: let us simply assume that <span class="math inline">\(m\)</span> can be expressed as a linear combination of some basis functions. In statistics, <em>B-spline</em> basis is extremely popular. If we restrict the number of basis functions enough, no penalization may be needed anymore. In practice, we can meet both penalized and non-penalized approaches.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p><a href="https://stat.ethz.ch/education/semesters/ss2010/CompStat/">M. Maechler’s and P. Buhlmann’s course notes</a></p>
<p><a href="https://www2.karlin.mff.cuni.cz/~omelka/Vyuka_nmst434_2122.php">M. Omelka’s course notes</a></p>
<p><a href="http://users.stat.umn.edu/~helwig/notes/smooth-notes.html">N. Helwig’s notes</a></p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>