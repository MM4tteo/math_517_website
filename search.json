[
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "This is the homepage for MATH-517 Statistical computation and visualisation in Fall 2023 at EPFL. All course materials will be posted on this site.\nYou can find the course syllabus here and the course schedule here. All the additional ressources can be found either on this page or by using the search bar on the top left corner of the website. These ressources include supplementary material, tutorials, tips and tricks, etc…"
  },
  {
    "objectID": "course-overview.html#assignments",
    "href": "course-overview.html#assignments",
    "title": "Course overview",
    "section": "3.1 Assignments",
    "text": "3.1 Assignments\nThere will be 8 assignments during the semester, graded as described above. Assignments can be in the Assignments tab on this website (or equivalently in the schedule). Exercises are not mandatory, only recommended.\n\nTo accept the assignment, you need to first acess the google document with all the invite links, then click on the link corresponding to the assignment you want to accept. This will create a repository for you on GitHub. You can then clone this repository on your computer and start working on the assignment."
  },
  {
    "objectID": "course-overview.html#projects",
    "href": "course-overview.html#projects",
    "title": "Course overview",
    "section": "3.2 Projects",
    "text": "3.2 Projects\nThe small project can start after the second lecture, deadline on Week 5. See here for details.\nThe main project can start following the \\(7\\)-th lecture, deadline on December 24, at 23:59. This is a soft deadline. I would suggest you finish the project before Christmas, however, if all members of the team agree to this, the project can be submitted by the end of the calendar year. This is recommended in order to prevent the holiday season ruined by a lazy member(s) of the team. Note that if a single member of your team wishes to submit on December 23, you are required to do so. See here for details.\n Groups can be of size of either 2 or 3 people. The size will not matter w.r.t. to grading. However, a group of size 3 will have one additional task to do: as part of their submission, every team member will individually include a short paragraph describing contributions of every individual member of the team. This is not to be discussed among the team members, as it serves as a safeguard. Regardless of their individual contributions, each member of the team will receive the same grade, apart from where this would be extremely unfair. Such cases will be discussed personally. In case of any team-work problems, the students are encouraged to seek advice (mostly as a group) from the teachers (mostly during the exercise classes)."
  },
  {
    "objectID": "course-overview.html#license",
    "href": "course-overview.html#license",
    "title": "Course overview",
    "section": "6.1 License",
    "text": "6.1 License\n\nThis online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\nThis website and part of the course materials where adapted from different sources:\n\nDr. Mine Çetinkaya-Rundel STA 101 website\nDr. Tomas Masák MATH-517 Fall 2022 course\nfredhutch.io and R for data analysis and visualization of Ecological Data"
  },
  {
    "objectID": "notes/week_03.html",
    "href": "notes/week_03.html",
    "title": "Kernel Density Estimation",
    "section": "",
    "text": "Let \\(X_1,\\ldots,X_n\\) be a random sample from a density \\(f(x)\\). Our goal is to estimate the density \\(f\\) non-parametrically (as flexibly as possible). We have already used a histogram to give us some intuition on an underlying distribution of a variable in a data set.\nWe will be using the faithful data which provides the time between eruptions (variable waiting) and the duration time of the eruptions (variable eruptions) for the Old Faithful geyser in Yellowstone. By specifying probability=T, we scaled the y-axis so each histogram integrates to one (it is easy to see why this is not the default – the numbers do not tell us much, but here we want to look at histograms as density estimators).\nCode\ndata(faithful)\npar(mfrow=c(1,2))\nhist(faithful$eruptions, probability=T, main = \"Eruption duration\", xlab=\"time [min]\")\nhist(faithful$waiting, probability=T, main = \"Waiting time\", xlab=\"time [min]\")\nWe assume that there exist underlying densities from which the eruption duration and waiting times are drawn. While histograms are useful in providing some quick information (supports, bimodality, etc.) they are quite useless in capturing finer properties, such as the shape, since they heavily depend on the binwidth as well as the origin – the two quantities that determine equally-spaced breaks for the bins.\nCode\npar(mfrow=c(1,4), mar = c(3.2, 3, 1.6, 0.2)) # reduce the white space around individual plots\nhist(faithful$eruptions, probability=T, main=\"Origin at 1.5\");\nhist(faithful$eruptions, breaks=seq(1.4,5.4,by=0.5), probability=T, main=\"Origin at 1.4\", xlab=\"time [min]\")  \nhist(faithful$eruptions, breaks=seq(1.3,5.3,by=0.5), probability=T, main=\"Origin at 1.3\", xlab=\"time [min]\") \nhist(faithful$eruptions, breaks=seq(1.2,5.2,by=0.5), probability=T, main=\"Origin at 1.2\", xlab=\"time [min]\")\nCode\npar(mfrow=c(1,4), mar = c(3.2, 3, 1.6, 0.2))\nhist(faithful$eruptions, probability=T, main=\"Binwidth=0.5\")\nhist(faithful$eruptions, breaks=seq(1.5,5.5,by=0.25), probability=T, main=\"Binwidth=0.25\") \nhist(faithful$eruptions, breaks=seq(1.5,5.5,by=0.1), probability=T, main=\"Binwidth=0.1\") \nhist(faithful$eruptions, breaks=seq(1.5,5.5,by=0.01), probability=T, main=\"Binwidth=0.01\")\nWhile binwidth is a reasonable tuning parameter (it’s role is quite intuitive and the histogram for different binwidths provides the same qualitative information), the choice of origin is completely arbitrary, and can significantly distort the histogram."
  },
  {
    "objectID": "notes/week_03.html#circulant-matrices-and-discrete-fourier-transform",
    "href": "notes/week_03.html#circulant-matrices-and-discrete-fourier-transform",
    "title": "Kernel Density Estimation",
    "section": "5.1 Circulant Matrices and Discrete Fourier Transform",
    "text": "5.1 Circulant Matrices and Discrete Fourier Transform\nIn this sub-section only, we will index \\(p\\)-dimensional vectors from \\(0\\) to \\(p-1\\) and similarly for matrices. I will write it down explicitly as a reminder from time to time.\nDefinition: A matrix \\(\\mathbf{C} = (c_{jk})_{j,k=0}^{p-1} \\in \\mathbb{R}^{p \\times p}\\) is called circulant if \\(c_{jk} = c_{|j-k|}\\), where \\(\\mathbf{c} = (c_j) \\in \\mathbb{R}^{p}\\) is the symbol of \\(\\mathbf{C}\\) (the first column of \\(\\mathbf{C}\\)).\n\\[C=\\left[\\begin{array}{ccccc}c_0 & c_{n-1} & \\cdots & c_2 & c_1 \\\\ c_1 & c_0 & c_{n-1} & & c_2 \\\\ \\vdots & c_1 & c_0 & \\ddots & \\vdots \\\\ c_{n-2} & & \\ddots & \\ddots & c_{n-1} \\\\ c_{n-1} & c_{n-2} & \\cdots & c_1 & c_0\\end{array}\\right]\\]\nDefinition: The discrete Fourier basis in \\(\\mathbf{R}^p\\) is (in the columns of) the matrix \\(\\mathbf{E} = (e_{jk})_{j,k=0}^{p-1} \\in \\mathbb{R}^{p \\times p}\\) with entries given by \\[ e_{jk} = \\frac{1}{\\sqrt{p}} e^{2\\pi i j k / p} , \\qquad j,k=0,\\ldots,p-1\\] It is straightforward to check that \\(\\mathbf{E}\\) is unitary and hence the discrete Fourier basis is really a basis.\nDefinition: The discrete Fourier transform (DFT) of \\(\\mathbf{x} \\in \\mathbb{R}^p\\) is \\(\\mathbf{E}^* \\mathbf{x}\\) with \\(\\mathbf{E}\\) being the discrete Fourier basis from the previous definition.\nIn the previous definition, the superscript \\(*\\) denotes the conjugate transpose. Since \\(\\mathbf{E}\\) is symmetric, it only corresponds to adding minuses to all the complex exponents. The inverse DFT (IDFT) is just the application \\(\\mathbf{E} \\mathbf{x}\\) (without the complex conjugate).\nA fast Fourier transform (FFT) is an algorithm for evaluating the DFT \\(\\mathbf{E}^* \\mathbf{x}\\) efficiently. While the naive matrix-vector multiplication would require \\(\\mathcal{O}(p^2)\\) operations, this can be reduced to \\(\\mathcal{O}(p \\log p)\\) by utilizing the special (cyclic) structure of \\(\\mathbf{E}\\). Any specific algorithm achieving this reduction is referred to as a FTT.\nThe FFT is indispensable (mainly in the engineering areas), and is widely considered to be one of the most important algorithms out there. While the FFT idea goes back to Gauss and has little to do with statistics, the FFT is attributed to the great statistician John W. Tukey.\nThe important connection between circulant matrices is in the following claim.\nClaim: Circulant matrices are diagonalizable by the DFT. Specifically, the eigendecomposition of a circulant matrix \\(\\mathbf{C}\\) (with a symbol \\(\\mathbf{c}\\)) is given by \\(\\mathbf{C} = \\mathbf{E} \\mathrm{diag}(\\mathbf{q}) \\mathbf{E}^*\\), where \\(\\mathbf{q} = \\mathbf{E}^* \\mathbf{c}\\).\nProof: As usual, let \\(\\mathbf{e}_j\\) denote the \\(j\\)-th column of \\(\\mathbf{E}\\). Then we have \\[\n\\mathbf{C} \\mathbf{e}_j = \\begin{pmatrix}\n\\sum_{k=0}^{p-1} c_{0k} e_{jk} \\\\\n\\sum_{k=0}^{p-1} c_{1k} e_{jk} \\\\\n\\vdots \\\\\n\\sum_{k=0}^{p-1} c_{(p-1),k} e_{jk}\n\\end{pmatrix} =\n\\frac{1}{\\sqrt{p}}\\begin{pmatrix}\n\\sum_{k} c_{|0-k|} e_{jk} \\\\\n\\sum_{k} c_{|1-k|} e_{jk} \\\\\n\\vdots \\\\\n\\sum_{k} c_{|(p-1)-k|} e_{jk}\n\\end{pmatrix} =\n\\frac{1}{\\sqrt{p}}\\begin{pmatrix}\ne^{2 \\pi i j/p} \\sum_k c_{|0-k|} e^{2 \\pi i j (k-0)/p} \\\\\ne^{2 \\pi i j 2/p} \\sum_k c_{|1-k|} e^{2 \\pi i j (k-1)/p} \\\\\n\\vdots \\\\\ne^{2 \\pi i j p/p} \\sum_k c_{|(p-1)-k|} e^{2 \\pi i j (k-p+1)/p}\n\\end{pmatrix}\n\\] Since both the symbol \\(\\mathbf{c}\\) and the Fourier basis are cyclic, all the sums on the RHS of the previous formula are the same, let us denote them by \\(\\widetilde{q}_j\\). Then we have \\(\\mathbf{C} \\mathbf{e}_j = \\frac{1}{\\sqrt{p}} \\widetilde{q}_j \\mathbf{e}_j\\) for all \\(j\\). Equivalently, in the full matrix format: \\[ \\mathbf{C} \\mathbf{E} = \\mathbf{E} \\frac{1}{\\sqrt{p}} \\mathrm{diag}\\big(\\widetilde{q}\\big) \\] and hence we have the eigendecomposition \\(\\mathbf{C} = \\mathbf{E} \\frac{1}{\\sqrt{p}} \\mathrm{diag}\\big(\\widetilde{q}\\big) \\mathbf{E}^*\\). Defining \\(\\mathbf{q} := \\frac{1}{\\sqrt{p}} \\widetilde{q}\\), this is exactly what we wanted. It remains to show that \\(\\mathbf{q} = \\mathbf{E}^* \\mathbf{c}\\), but this is clear since: \\[\n\\mathbf{q} =\n\\frac{1}{\\sqrt{p}}\\begin{pmatrix}\n\\sum_k c_{|0-k|} e^{2 \\pi i j (k-0)/p} \\\\\n\\sum_k c_{|1-k|} e^{2 \\pi i j (k-1)/p} \\\\\n\\vdots \\\\\n\\sum_k c_{|(p-1)-k|} e^{2 \\pi i j (k-p+1)/p}\n\\end{pmatrix} = \\begin{pmatrix}\n\\sum_k c_{|0-k|} e^{- 2 \\pi i j (0-k)/p} \\\\\n\\sum_k c_{|1-k|} e^{- 2 \\pi i j (1-k)/p} \\\\\n\\vdots \\\\\n\\sum_k c_{|(p-1)-k|} e^{- 2 \\pi i j (p-1+k)/p}\n\\end{pmatrix} = \\begin{pmatrix}\n\\sum_k c_{k} e^{- 2 \\pi i j k/p} \\\\\n\\sum_k c_{k} e^{- 2 \\pi i j k/p} \\\\\n\\vdots \\\\\n\\sum_k c_{k} e^{- 2 \\pi i j k/p}\n\\end{pmatrix} = \\mathbf{E}^* \\mathbf{c}\n\\]\n\nQ.E.D.\n\nNow we know that every circulant matrix can be applied efficiently thanks to the FFT: \\[\\mathbf{C} \\mathbf{x} = \\underbrace{\\mathbf{E} \\underbrace{\\mathrm{diag}(\\mathbf{\\underbrace{\\mathbf q}_{= \\mathrm{FFT}(\\mathbf{c})}}) \\underbrace{\\mathbf{E}^* \\mathbf x}_{ = \\mathrm{FFT}(\\mathbf x)}}_{\\text{entry-wise prod of those 2 vectors}}}_{\\text{inverse FFT of that product}}\\] Hence instead of the \\(\\mathcal{O}(p^2)\\) operations needed to calculate \\(\\mathbf{C} \\mathbf{x}\\) naively, we can calculate it using FFT (twice FFT and once inverse FFT) in just \\(\\mathcal{O}(p \\log(p))\\) operations."
  },
  {
    "objectID": "notes/week_03.html#kde-viewed-as-a-linear-smoother",
    "href": "notes/week_03.html#kde-viewed-as-a-linear-smoother",
    "title": "Kernel Density Estimation",
    "section": "5.2 KDE viewed as a Linear Smoother",
    "text": "5.2 KDE viewed as a Linear Smoother\nWhy are circulant matrices and FFT interesting to us here? Instead of calculating \\(\\widehat{f}(x)\\) by the formula above, i.e., by applying a smoother directly to data \\(X_1,\\ldots,X_n\\), let’s create a histogram first (with a small binwidth) and then smooth the histogram instead of the original values. This is equivalent to rounding the observations \\(X_1,\\ldots,X_n\\) to a common grid \\(t_1,\\ldots, t_p \\in \\mathbb{R}\\), obtaining approximate values \\(\\widetilde{X}_1,\\ldots,\\widetilde{X}_n\\), and it will allow us to use FFT to reduce the KDE complexity from \\(\\mathcal{O}(n^2)\\) to \\(\\mathcal{O}(n \\log n)\\). It could actually be even less if we used a larger binwidth for the initial histogram (i.e., rounded to a coarser grid), but that is not what we want.\nLet us denote the vector of counts in the initial histogram by \\(\\mathbf{y} \\in \\mathbb{R}^p\\), where \\(p\\) is the grid size (the number of bins of the initial histogram). For simplicity, we will assume \\(p=n\\). It is easy to see that applying KDE to the rounded data \\(\\widetilde{X}_1,\\ldots,\\widetilde{X}_n\\) is equivalent to calculating a linear smoother of the initial histogram: \\[\n\\widehat{f}(x) = \\frac{1}{n h_n} \\sum_{i=1}^n K\\left(\\frac{\\widetilde{X}_i - x}{h_n} \\right) =\n\\frac{1}{n h_n} \\sum_{j=1}^p K\\left(\\frac{t_j - x}{h_n} \\right) y_j\n\\]\n\n\nCode\npar(mfrow=c(1,2), mar = c(3.2, 3, 1.6, 0.2))\ninit_hist &lt;- hist(faithful$eruptions, breaks = 256, plot=F)\nx_tilde &lt;- rep(init_hist$mids, times=init_hist$counts) # read \\tilde{X} from the initial histogram\nhist(faithful$eruptions, breaks = 256, freq=F, plot=T,xlab=\"\",main=\"\") # plot init hist on the density scale\npoints(density(x_tilde,bw=0.25),col=4,type=\"l\",lwd=3)\n# compare with the default KDE\nplot(density(x_tilde,bw=0.25),col=4,lwd=3,xlab=\"\",main=\"\")\npoints(density(faithful$eruptions,bw=0.25),type=\"l\",main=\"KDE\",lwd=3,lty=2)\nlegend(\"topleft\",legend=c(\"original\",\"rounded\"),lty=c(2,1),col=c(1,4))\n\n\n\n\n\nAnd as we can see from the right-hand plot above, calculating KDE using the rounded data \\(\\widetilde{X}_1,\\ldots,\\widetilde{X}_n\\) is very similar to calculating it using the original data \\(X_1,\\ldots,X_n\\).\nHowever, the rounding essentially enables us to utilize the algebra above. Say we only want to evaluate \\(\\widehat{f}(x)\\) on a grid – for simplicity let us say the same grid, given by \\(t_1, \\ldots, t_p\\) – we can write the whole resulting vector as \\[\n\\big(\\widehat{f}(t_1),\\ldots,\\widehat{f}(t_p)\\big)^\\top = \\mathbf{S} y\n\\] where the entries of \\(\\mathbf{S}\\) are given by \\(s_{ij} = \\frac{1}{n h_n} K\\left(\\frac{t_j - t_i}{h_n} \\right)\\).\nMethods that calculate the output as a matrix transformation of the input (like in the previous formula or in linear regression) are called linear smoothers, and their simplicity leads to nice properties. This will be seen in the next section as well as later in the course. For example, questions such as “how would the fit look like if we dropped a single observations” (which is needed e.g. in linear regression for calculating the Cook’s distances) can be answered without refitting for linear smoothers. Adopting linear models terminology, \\(\\mathbf{S}\\) is called the hat matrix.\nYou can notice that KDE is a linear smoother even without rounding the observations on a grid (simply take \\(\\mathbf{y} \\equiv \\mathbf{1}\\)), but only on an equidistant grid is the hat matrix \\(\\mathbf{S}\\) Toeplitz (in this case, it is symmetric, see below).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe kernel on the left-hand plot above just shifts (from left to right, visualized in the middle) and its values form the hat matrix on the right-hand side (from bottom to top).\nAny Toeplitz matrix \\(\\mathbf{S}\\) of dimensions \\(n \\times n\\) can be embedded into a circulant matrix \\(\\mathbf{C}\\) of dimensions at most \\((2n -1) \\times (2n-1)\\). The easiest way is to wrap the first row of \\(\\mathbf{S}\\), denoted \\(\\mathbf{s}\\), to form the first row of \\(\\mathbf{C}\\) as \\[\n\\mathbf{c} = (s_1, s_2, \\ldots, s_{n-1}, s_n, s_{n-1},\\ldots,s_2)^\\top\n\\]\n\n\n\n\n\n\n\n\n\nThen, when we want to apply the Toeplitz matrix \\(\\mathbf{S}\\), i.e., calculate the KDE from the initial histogram as \\(\\mathbf{S} y\\), we can instead apply the wrapped circulant matrix \\(\\mathbf{C}\\) on a vector suitably extended with zeros: \\[\n\\mathbf{C} \\begin{pmatrix}\n\\mathbf{y} \\\\\n\\mathbf 0\n\\end{pmatrix}\n= \\left( \\begin{array}{c|c}\n\\mathbf{S} & \\cdot \\\\\n\\hline\n\\cdot & \\cdot\n\\end{array}\\right)\n\\left( \\begin{array}{c}\n\\mathbf{y} \\\\\n\\hline\n\\mathbf 0\n\\end{array}\\right) = \\left( \\begin{array}{c}\n\\mathbf S \\mathbf{y} \\\\\n\\hline\n\\cdot\n\\end{array}\\right)\n\\] Since this section is a bit longer and we might have lost the thread, let’s summarize what we have done to make KDE calculations efficient:\n\nRound up the original data to a common equidistant grid – equivalently: calculate the initial histogram\nFollowing point 1, KDE reduces to a linear smoother with a Toeplitz hat matrix. Embed this into a circulant matrix, and add appropriate number of zeros to the observation vector.\nUse FFT to calculate the KDE."
  },
  {
    "objectID": "notes/week_03.html#kde-in-r",
    "href": "notes/week_03.html#kde-in-r",
    "title": "Kernel Density Estimation",
    "section": "5.3 KDE in R",
    "text": "5.3 KDE in R\nWhile I have been using solely the function density() from the base R distributions, there are in fact dozens of packages in R performing univariate kernel density estimation. An overview is given in Wickham (2011), including a simulation study concerning the speed and accuracy of some of the available packages. The takeaway message is clear: one should be aware of what software they use. To this point, the density() function uses an FFT algorithm as described above and, while still not being the fastest, it is a well documented and reliable option.\nSecondly, now that we understand that KDE (which is really the standard for density estimation) is basically just a histogram smoother, it is clear why “densigrams” (histograms overlaid with KDEs) are so popular for visualization purposes (like the first plot in Section 5.2 above)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 517: Statistical computation and visualisation",
    "section": "",
    "text": "This page contains an outline of the topics, exercises (ex.), and assignments (ae.) for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester. See the overview page for more information about the course.\n\n\n\n\n\n\nTip\n\n\n\nIt can be useful to check the available tutorials to get ready for the assignments quickly. You can find them under the resources section on your left. Please check the FAQ page for common questions and answers.\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n    \n      WEEK\n      DATE\n      TOPIC\n      RESOURCES\n      AE\n      EX\n      DUE\n    \n  \n  \n    1\nFri, Sep 22\nSoftware and Data Considerations\n\n📖 linear regression\n\n📖 ae-01\n\n⌨️ ex-01\n\n\n    2\nFri, Sep 29\nGraphics and Visualization\n\n📖 exploring data\n\n💻 mini project\n\n\n\n    \nSun, Oct 1\n\n\n\n\n📖 ae-01\n\n    3\nFri, Oct 6\nKernel Density Estimation\n\n\n🖥️ kde interactive  from notes\n📖 KDE notes\n\n\n📖 ae-02\n\n⌨️ ex-02\n\n\n    \nSun, Oct 8\n\n\n\n\n\n    4\nFri, Oct 13\nNonparametric Regression (Smoothing)\n\n\n📖 ae-03\n\n\n\n    \nSun, Oct 15\n\n\n\n\n📖 ae-02\n\n    5\nFri, Oct 20\nCross-validation\n\n\n📖 ae-04\n\n\n\n    \nSun, Oct 22\n\n\n\n\n💻 mini project\n📖 ae-03\n\n    6\nFri, Oct 27\nEM algorithm\n\n\n📖 ae-05\n\n\n\n    \nSun, Oct 29\n\n\n\n\n📖 ae-04\n\n    7\nFri, Nov 3\nEM algorithm\n\n\n💻 project\n\n\n\n    \nSun, Nov 5\n\n\n\n\n📖 ae-05\n\n    8\nFri, Nov 10\nMonte Carlo\n\n\n📖 ae-06\n\n\n\n    \nSun, Nov 12\n\n\n\n\n\n    9\nFri, Nov 17\nBootstrap\n\n\n\n\n\n    \nSun, Nov 19\n\n\n\n\n📖 ae-06\n\n    10\nFri, Nov 24\nBootstrap\n\n\n📖 ae-07\n\n\n\n    \nSun, Nov 26\n\n\n\n\n\n    11\nFri, Dec 1\nBayesian Computations\n\n\n📖 ae-08\n\n\n\n    \nSun, Dec 3\n\n\n\n\n📖 ae-07\n\n    12\nFri, Dec 8\nBayesian Computations\n\n\n\n\n\n    \nSun, Dec 10\n\n\n\n\n📖 ae-08\n\n    13\nFri, Dec 15\nTree-based Methods for Classification\n\n\n\n\n\n    \nSun, Dec 17\n\n\n\n\n\n    14\nFri, Dec 22\nno lecture\n\n\n\n\n\n    \nSun, Dec 24\n\n\n\n\n💻 project"
  },
  {
    "objectID": "resources/tips/collaboration.html",
    "href": "resources/tips/collaboration.html",
    "title": "Best practices for collaborative work",
    "section": "",
    "text": "In the world of statistics and data analysis, collaboration plays a vital role in advancing scientific knowledge and solving complex problems. When working in a team or contributing to open-source projects, it is crucial to follow best practices for collaborative work. In this section, we will explore key strategies to enhance collaboration, maintain code quality, and ensure seamless cooperation with peers."
  },
  {
    "objectID": "resources/tips/collaboration.html#code-style-guidelines",
    "href": "resources/tips/collaboration.html#code-style-guidelines",
    "title": "Best practices for collaborative work",
    "section": "Code Style Guidelines",
    "text": "Code Style Guidelines\nConsistency in coding style is essential for making the code more readable and understandable by others. Adopting a standard coding style across your team helps avoid confusion and reduces the time spent on code reviews. For each language (Julia, R, and Python), there are widely accepted coding style guidelines:\n\n\nJulia\n\nJulia official style guide\nmore opinionated SciML Style Guide\n\n\n\n\nPython\n\nPEP 8\n\n\n\n\nR\n\ntidyverse style guide\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere exists tools to help you check your code style and correct the basic mistakes. They are called linters. For example, in Julia, you can use JuliaFormatter.jl, in Python black and in R styler."
  },
  {
    "objectID": "resources/tips/collaboration.html#documentation",
    "href": "resources/tips/collaboration.html#documentation",
    "title": "Best practices for collaborative work",
    "section": "Documentation",
    "text": "Documentation\nWriting clear and comprehensive documentation for your code is crucial for effective collaboration. Documenting your functions, classes, and important code blocks with comments ensures that others can understand the purpose and functionality of each component. Additionally, provide explanations for any complex algorithms or statistical methods used in your analysis."
  },
  {
    "objectID": "resources/tips/collaboration.html#using-version-control-effectively",
    "href": "resources/tips/collaboration.html#using-version-control-effectively",
    "title": "Best practices for collaborative work",
    "section": "Using Version Control Effectively",
    "text": "Using Version Control Effectively\nWhen collaborating with others, version control becomes even more critical. Follow these best practices:\n\nCommit often\nMake small, logical commits with descriptive commit messages. Good commit messages are clear, concise, and informative. They provide context and explain the purpose of the commit in a way that anyone reading them, whether it’s your collaborators or your future self, can understand. A well-written commit message describes why the change was made, how it affects the codebase, and any relevant issues it addresses. By adhering to this practice, you make it easier to track changes, collaborate effectively, and maintain a clean and understandable version history for your projects.\nBad Commit Message:\nMade changes\nThis commit message is too generic and doesn’t provide any insight into what changes were made or why.\nCorrected Commit Message:\nAdd median calculation function for dataset variance\n\nIn response to user feedback and to enhance the statistical capabilities of our library, this commit introduces a new function, `calculate_median()`, which accurately calculates the median value for datasets. The function has been rigorously tested against various data sets to ensure its reliability and accuracy in statistical computations.\nThe corrected commit message provides specific information about the change made, mentions its purpose in improving the library’s statistical capabilities, and briefly explains the testing process to ensure the quality of the new feature. This level of detail is essential for collaboration and helps others understand the changes made to the codebase. If you are working on your own, you might not need to be so verbose in the body of the commit, but it is still a good practice to write good commit messages. See this article for some tips on how to write good commit messages, and why it could be useful.\n\n\nBranching\nUse branches for different features or tasks to keep the main development branch clean and stable.\n\n\nPull Requests (PRs)\nWhen contributing to shared repositories, submit pull requests for review before merging changes into the main branch."
  },
  {
    "objectID": "resources/tips/collaboration.html#code-reviews",
    "href": "resources/tips/collaboration.html#code-reviews",
    "title": "Best practices for collaborative work",
    "section": "Code Reviews",
    "text": "Code Reviews\nCode reviews are an integral part of the collaborative process. Reviewing each other’s code helps identify potential issues, provides valuable feedback, and improves the overall quality of the project. During code reviews, be respectful, specific, and constructive in your comments."
  },
  {
    "objectID": "resources/tips/collaboration.html#issue-tracking",
    "href": "resources/tips/collaboration.html#issue-tracking",
    "title": "Best practices for collaborative work",
    "section": "Issue Tracking",
    "text": "Issue Tracking\nUtilize issue tracking systems like GitHub Issues to keep track of tasks, bugs, and enhancements. When collaborating on larger projects, this helps organize and prioritize work effectively."
  },
  {
    "objectID": "resources/tips/collaboration.html#continuous-integration-ci",
    "href": "resources/tips/collaboration.html#continuous-integration-ci",
    "title": "Best practices for collaborative work",
    "section": "Continuous Integration (CI)",
    "text": "Continuous Integration (CI)\nConsider integrating continuous integration into your workflow. CI systems automatically build and test your code whenever changes are pushed to the repository. This ensures that the project remains in a working state and prevents introducing new bugs unintentionally."
  },
  {
    "objectID": "resources/tips/collaboration.html#collaborative-decision-making",
    "href": "resources/tips/collaboration.html#collaborative-decision-making",
    "title": "Best practices for collaborative work",
    "section": "Collaborative Decision-Making",
    "text": "Collaborative Decision-Making\nWhen making significant decisions about the project, involve all relevant team members. Encourage open discussions and consider different perspectives to arrive at the best solutions collectively."
  },
  {
    "objectID": "resources/tips/collaboration.html#communication-and-feedback",
    "href": "resources/tips/collaboration.html#communication-and-feedback",
    "title": "Best practices for collaborative work",
    "section": "Communication and Feedback",
    "text": "Communication and Feedback\nMaintain effective communication channels with your team. Use tools like Slack or Discord to discuss ideas, share progress, and address any challenges. Offer and receive feedback graciously to create a positive and productive working environment."
  },
  {
    "objectID": "resources/tips/collaboration.html#licensing-and-copyright",
    "href": "resources/tips/collaboration.html#licensing-and-copyright",
    "title": "Best practices for collaborative work",
    "section": "Licensing and Copyright",
    "text": "Licensing and Copyright\nEnsure that all code and resources used in the project comply with appropriate licenses and copyright laws. Respect intellectual property rights and provide proper attribution when using external libraries or resources.\nIn conclusion, effective collaboration is crucial for success in statistical research and data analysis projects. By adhering to code style guidelines, providing thorough documentation, using version control effectively, conducting code reviews, and maintaining open communication, you and your team can work together seamlessly, produce high-quality code, and contribute to the advancement of statistical knowledge. Collaborative work is not only about sharing ideas but also about learning from others and growing as a team. By adopting these best practices, you will become an effective collaborator and contribute to the success of your statistical projects."
  },
  {
    "objectID": "resources/tips/reproducible_research.html",
    "href": "resources/tips/reproducible_research.html",
    "title": "Reproducible scientific computing",
    "section": "",
    "text": "Reproducible research is a critical aspect of modern data analysis and statistical investigations. It ensures that the results and findings obtained through data analysis can be independently verified, replicated, and built upon by others. In this section, we will explore the importance of reproducibility in statistical research and cover essential tools and practices to organize your projects for reproducibility.\n\n\nReproducible research is the practice of making your data analysis, code, and results transparent and accessible to others. By adopting reproducible research practices, you contribute to the integrity and reliability of scientific findings. It allows others to validate your results, identify potential errors, and extend your analyses. The benefits of reproducible research include increased trustworthiness of research, better collaboration, and more efficient scientific progress.\n\n\n\nA well-structured project organization is the foundation of reproducibility. When starting a new project, create a dedicated directory for it. Within this directory, consider the following structure:\n\nData: Store all relevant datasets and raw data files in this folder.\nCode: Keep your analysis scripts and code files in this directory. Use clear and meaningful filenames.\nOutput: Store the results, processed data, and visualizations generated by your code here.\nReports: Save any reports or documentation related to your analysis in this folder.\nReferences: Store any literature, papers, or external references used in your research.\nREADME.md: Create a readme file to provide an overview of the project, its purpose, and the steps to reproduce the analysis.\n\n\n\n\nVersion control systems like Git are essential tools for tracking changes in your code and project files. They allow you to maintain a history of your work, collaborate with others, and revert to previous versions if needed.\nGitHub is a popular platform that hosts Git repositories in the cloud, enabling seamless collaboration among team members. By pushing your project to a GitHub repository, you can share your work with others and receive feedback and contributions.\n\n\n\nTo ensure reproducibility, strive to create a clear and well-documented workflow. Document each step of your analysis, from data preprocessing to statistical computations and visualization. Use comments in your code to explain the rationale behind your decisions and highlight any assumptions made.\n\n\n\nIn any statistical computation, you may rely on various libraries, packages, or external data files. Make sure to specify the versions of the software and packages used in your analysis to ensure that others can reproduce your results precisely.\n\n\n\nIn this section, we have introduced the concept of reproducible research and its significance in statistical investigations. We explored how to organize your projects for reproducibility and utilize version control systems like Git and GitHub for collaboration and version tracking. By adopting these practices, you lay the groundwork for conducting reliable and transparent data analyses and creating a solid foundation for your statistical research. In the subsequent sections, we will delve into data manipulation, statistical computations, visualization, and reporting using Julia, R, and Python to support your journey towards becoming proficient in statistical computation and visualization.\n\n\n\n\n\nName\n\n\nTopic\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease see this page for good practices in programming."
  },
  {
    "objectID": "resources/tips/reproducible_research.html#introduction-to-reproducible-research",
    "href": "resources/tips/reproducible_research.html#introduction-to-reproducible-research",
    "title": "Reproducible scientific computing",
    "section": "",
    "text": "Reproducible research is a critical aspect of modern data analysis and statistical investigations. It ensures that the results and findings obtained through data analysis can be independently verified, replicated, and built upon by others. In this section, we will explore the importance of reproducibility in statistical research and cover essential tools and practices to organize your projects for reproducibility.\n\n\nReproducible research is the practice of making your data analysis, code, and results transparent and accessible to others. By adopting reproducible research practices, you contribute to the integrity and reliability of scientific findings. It allows others to validate your results, identify potential errors, and extend your analyses. The benefits of reproducible research include increased trustworthiness of research, better collaboration, and more efficient scientific progress.\n\n\n\nA well-structured project organization is the foundation of reproducibility. When starting a new project, create a dedicated directory for it. Within this directory, consider the following structure:\n\nData: Store all relevant datasets and raw data files in this folder.\nCode: Keep your analysis scripts and code files in this directory. Use clear and meaningful filenames.\nOutput: Store the results, processed data, and visualizations generated by your code here.\nReports: Save any reports or documentation related to your analysis in this folder.\nReferences: Store any literature, papers, or external references used in your research.\nREADME.md: Create a readme file to provide an overview of the project, its purpose, and the steps to reproduce the analysis.\n\n\n\n\nVersion control systems like Git are essential tools for tracking changes in your code and project files. They allow you to maintain a history of your work, collaborate with others, and revert to previous versions if needed.\nGitHub is a popular platform that hosts Git repositories in the cloud, enabling seamless collaboration among team members. By pushing your project to a GitHub repository, you can share your work with others and receive feedback and contributions.\n\n\n\nTo ensure reproducibility, strive to create a clear and well-documented workflow. Document each step of your analysis, from data preprocessing to statistical computations and visualization. Use comments in your code to explain the rationale behind your decisions and highlight any assumptions made.\n\n\n\nIn any statistical computation, you may rely on various libraries, packages, or external data files. Make sure to specify the versions of the software and packages used in your analysis to ensure that others can reproduce your results precisely.\n\n\n\nIn this section, we have introduced the concept of reproducible research and its significance in statistical investigations. We explored how to organize your projects for reproducibility and utilize version control systems like Git and GitHub for collaboration and version tracking. By adopting these practices, you lay the groundwork for conducting reliable and transparent data analyses and creating a solid foundation for your statistical research. In the subsequent sections, we will delve into data manipulation, statistical computations, visualization, and reporting using Julia, R, and Python to support your journey towards becoming proficient in statistical computation and visualization.\n\n\n\n\n\nName\n\n\nTopic\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease see this page for good practices in programming."
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html",
    "title": "Good practices in Julia",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction"
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html#writing-clean-and-readable-code",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html#writing-clean-and-readable-code",
    "title": "Good practices in Julia",
    "section": "Writing Clean and Readable Code",
    "text": "Writing Clean and Readable Code\n# Good: Use meaningful variable names and comments\npopulation_size = 1000  # Number of individuals in the population\nsample_size = 100       # Number of individuals in the sample\n\n# Avoid: Using unclear or abbreviated variable names\nps = 1000\nss = 100"
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html#using-comments-and-documentation",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html#using-comments-and-documentation",
    "title": "Good practices in Julia",
    "section": "Using Comments and Documentation",
    "text": "Using Comments and Documentation\n# Good: Add comments to explain complex operations or logic\nfunction calculate_mean(data)\n    # Calculate the sum of elements in the data\n    total = sum(data)\n    \n    # Calculate the mean by dividing the total by the number of elements\n    mean_value = total / length(data)\n    \n    return mean_value\nend\n\n# Avoid: Lack of comments and explanation\nfunction calc_mean(d)\n    t = sum(d)\n    m = t / length(d)\n    return m\nend"
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html#avoiding-global-variables-and-code-dependencies",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html#avoiding-global-variables-and-code-dependencies",
    "title": "Good practices in Julia",
    "section": "Avoiding Global Variables and Code Dependencies",
    "text": "Avoiding Global Variables and Code Dependencies\n# Good: Use local variables within functions to avoid global scope pollution\nfunction calculate_variance(data)\n    n = length(data)\n    mean_value = sum(data) / n\n    \n    # Calculate the variance using local variables\n    variance = sum((data .- mean_value).^2) / (n - 1)\n    \n    return variance\nend\n\n# Avoid: Using global variables in functions\nmean_value = 0\nfunction calc_variance(data)\n    n = length(data)\n    global mean_value = sum(data) / n\n    \n    # Calculate the variance using global variables\n    variance = sum((data .- mean_value).^2) / (n - 1)\n    \n    return variance\nend"
  },
  {
    "objectID": "resources/computing/intro_to_python/index.html",
    "href": "resources/computing/intro_to_python/index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "For a refresher on Python basics, you can check out one of the following resources:\n\nPython For Beginners\nIntroductory books from the official documentation\nThe Python Tutorial\nThe Hitchhiker’s Guide to Python\nDuchesnay, Lofstedt, and Younes (2020)\nand many others\n\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nGood practices in Python\n\n\nTips and tricks for writing better code\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nReferences\n\nDuchesnay, Edouard, Tommy Lofstedt, and Feki Younes. 2020. “Statistics and Machine Learning in Python.”"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html",
    "title": "Data visualization in R",
    "section": "",
    "text": "So far in this course, we have:\n\nlearned basic R syntax, including working with objects and functions\nimported data into R for manipulation with base R methods\nloaded tidyverse and used its data science tools to manipulate and filter data\n\nThis last material continues our explorations of tidyverse with a specific focus on data visualization. After completing this material, you should be able to use ggplot2 in R to:\n\ncreate and modify scatterplots and boxplots\nrepresent time series data as line plots\nsplit figures into multiple panels\ncustomize your plots"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#objectives",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#objectives",
    "title": "Data visualization in R",
    "section": "",
    "text": "So far in this course, we have:\n\nlearned basic R syntax, including working with objects and functions\nimported data into R for manipulation with base R methods\nloaded tidyverse and used its data science tools to manipulate and filter data\n\nThis last material continues our explorations of tidyverse with a specific focus on data visualization. After completing this material, you should be able to use ggplot2 in R to:\n\ncreate and modify scatterplots and boxplots\nrepresent time series data as line plots\nsplit figures into multiple panels\ncustomize your plots"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#getting-set-up",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#getting-set-up",
    "title": "Data visualization in R",
    "section": "Getting set up",
    "text": "Getting set up\nSince we are continuing to work with data in tidyverse, we need to make sure all of our data and packages are available for use.\nOpen your project in RStudio. Create a new script called class4.R, add a title, and enter the following code with comments:\n\n# load library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# read in first filtered data from last class\nbirth_reduced &lt;- read_csv(\"data/birth_reduced.csv\")\n\nRows: 4169 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): primary_diagnosis, tumor_stage, vital_status, morphology, state, t...\ndbl  (8): age_at_diagnosis, days_to_death, days_to_birth, days_to_last_follo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read in second filtered data from last class\nsmoke_complete &lt;- read_csv(\"data/smoke_complete.csv\")\n\nRows: 1152 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): primary_diagnosis, tumor_stage, vital_status, morphology, state, t...\ndbl  (8): age_at_diagnosis, days_to_death, days_to_birth, days_to_last_follo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you have trouble accessing your data and see an error indicating the file is not found, it is likely one of the following problems:\n\nCheck to make sure your project is open in RStudio. You should see the path to your project directory (e.g., ~/Desktop/introR) appear at the top of the console (above the window showing output). If this doesn’t appear, you should save your script in your project directory, then go to File -&gt; Open Project. Navigate to the location of your project directory and open the folder, then try to reexecute your code.\nMake sure you have the two datasets (birth_reduced.csv and smoke_complete.csv) in your data directory. Please reference the materials from class 3 to filter the original clinical dataset and export these data.\n\nOnce your data are imported appropriately, we can create a quick plot:\n\n# simple plot from base R from the smoke_complete dataset\nplot(x=smoke_complete$age_at_diagnosis, y=smoke_complete$cigarettes_per_day)\n\n\n\n\nThis plot is from base R. It gives you a general idea about the data, but isn’t very aesthetically pleasing. Our work today will focus on developing more refined plots using ggplot2, which is part of the tidyverse."
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#intro-to-ggplot2-and-scatterplots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#intro-to-ggplot2-and-scatterplots",
    "title": "Data visualization in R",
    "section": "Intro to ggplot2 and scatterplots",
    "text": "Intro to ggplot2 and scatterplots\nThere are three steps to creating a ggplot. We’ll start with a scatterplot, which is used to compare quantitative (continuous) variables.\n\nbind data: create a new plot with a designated dataset\n\n\n# basic ggplot\nggplot(data = smoke_complete) # bind data to plot\n\n\n\n\nThe last line of code creates an empty plot, since we didn’t include any instructions for how to present the data.\n\nspecify the aesthetic: maps the data to axes on a plot\n\n\n# basic ggplot\nggplot(data = smoke_complete, aes(x = age_at_diagnosis, \n                           y = cigarettes_per_day)) # specify aesthetics (axes)\n\n\n\n\nThis adds labels to the axis, but no data appear because we haven’t specified how they should be represented\n\nadd layers: visual representation of plot, including ways through which data are represented (geometries or shapes) and themes (anything not the data, like fonts)\n\n\nggplot(data = smoke_complete,\n       mapping = aes(x = age_at_diagnosis, y = cigarettes_per_day)) + \n  geom_point() # add a layer of geometry\n\n\n\n\nThe plus sign (+) is used here to connect parts of ggplot code together. The line breaks and indentation used here represents the convention for ggplot, which makes the code more readible and easy to modify.\nIn the code above, note that we don’t need to include the labels for data = and mapping =. It’s also common to include the mapping (aes) in the geom, which allows for more flexibility in customizing (we’ll get to this later!).\n\nggplot(smoke_complete) + \n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day)) \n\n\n\n\nThis plot is identical to the previous plot, despite the differences in code."
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#customizing-plots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#customizing-plots",
    "title": "Data visualization in R",
    "section": "Customizing plots",
    "text": "Customizing plots\nNow that we have the data generally displayed the way we’d like, we can start to customize a plot.\n\n# add transparency with alpha\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day), alpha = 0.1)\n\n\n\n\nTransparency is useful to help see the distribution of data, especially when points are overlapping.\n\n# change color of points\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day), \n             alpha = 0.1, color = \"green\")\n\n\n\n\nFor more information on colors available, look here.\nWe can also color points based on another (usually categorical) variable:\n\n# plot disease by color\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, \n                 color = disease), \n             alpha = 0.1)\n\n\n\n\nNote the location of color= with the other aesthetics, as well as the lack of quotation marks around disease.\nColoring by a variable automatically adds a legend as well.\nWe can also change the general appearance of the plot (background colors and fonts):\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease), alpha = 0.1) +\n  theme_bw() # change background theme\n\n\n\n\nThis adds another layer to our plot representing a black and white theme. A complete list of pre-set themes is available here, and we’ll cover ways to customize our own themes later in this lesson.\nWhile the axes are currently sufficient, they aren’t particularly attractive. We can add a title and replace the axis labels using labs:\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease), alpha = 0.1) +\n  labs(title = \"Age at diagnosis vs cigarettes per day\", # title\n       x=\"age (days)\", # x axis label\n       y=\"cigarettes per day\") +# y axis label\n  theme_bw()\n\n\n\n\nAnother common feature to customize involves the orientation and appearance of fonts. While this can be controlled by default themes like theme_bw), you can also control different parts independently. For example, we can make a dramatic modification to all text in the plot:\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  theme(text = element_text(size = 16)) # increase all font size\n\n\n\n\nAlternatively, you can alter only one specific type of text:\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5)) # rotate and adjust x axis text\n\n\n\n\nThis rotates and adjusts the horizontal and vertical arrangement of the labels on only the x axis. Of course, you can also modify other text (y axis, axis labels, legend).\nAfter you’re satisfied with a plot, it’s likely you’d want to share it with other people or include in a manuscript or report.\n\n# create directory for output\ndir.create(\"figures\")\n\n\n# save plot to file\nggsave(\"figures/awesomePlot.jpg\", width = 10, height = 10, dpi = 300)\n\nThis automatically saves the last plot for which code was executed. You can view your figures/ directory to see the exported jpeg file. This command interprets the file format for export using the file suffix you specify. The other arguments dictate the size (width and height) and resolution (dpi).\n\nChallenge-scatterplot\nCreate a scatterplot showing age at diagnosis vs years smoked with points colored by gender and appropriate axis labels."
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#box-and-whisker-plots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#box-and-whisker-plots",
    "title": "Data visualization in R",
    "section": "Box and whisker plots",
    "text": "Box and whisker plots\nBox and whisker plots compare the distribution of a quantitative variable among categories.\n\n# creating a box and whisker plot\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day))\n\n\n\n\nThe main differences from the scatterplots we created earlier are the geom type and the variables plotted.\nWe can change the color similarly to scatterplots:\n\n# adding color\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day), color = \"tomato\")\n\n\n\n\nIt seems weird to change the color of the entire box, though. A better option would be to add colored points to a black box and whisker plot:\n\n# adding colored points to black box and whisker plot\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day)) +\n  geom_jitter(aes(x = vital_status, y = cigarettes_per_day), alpha = 0.3, color = \"blue\")\n\n\n\n\nJitter references a method of randomly offsetting points slightly to allow them to be seen and interpreted more easily.\nThis method, however, effectively duplicates some data points, since all points are shown with jitter and the boxplot shows outliers. You can use an option in geom_boxplot to suppress plotting of outliers:\n\n# boxplot with both boxes and points\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day), outlier.shape = NA) +\n  geom_jitter(aes(x = vital_status, y = cigarettes_per_day), alpha = 0.3, color = \"blue\")\n\n\n\n\n\nChallenge-comments\nWrite code comments for each of the following lines of code. What is the advantage of writing code like this?\n\n\nmy_plot &lt;- ggplot(smoke_complete, aes(x = vital_status, y = cigarettes_per_day)) \nmy_plot +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(alpha = 0.2, color = \"purple\")\n\n\n\n\n\nChallenge-order\nDoes the order of layers in the last plot matter? What happens if jitter is coded before boxplot?"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#time-series-data-as-line-plots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#time-series-data-as-line-plots",
    "title": "Data visualization in R",
    "section": "Time series data as line plots",
    "text": "Time series data as line plots\nSo far we’ve been able to work with the data as it appears in our filtered dataset. Now that we’re moving on to time series plots (changes in variables over time), we need to manipulate the data. We’ll also be working with the birth_reduced dataset, which we created last class (primarily by removing all missing data for year of birth). We’d like to plot the number of individuals in the dataset born by year, so we need to first count our observations based on both disease and year of birth:\n\n# count number of observations for each disease by year of birth\nyearly_counts &lt;- birth_reduced %&gt;%\n  count(year_of_birth, disease) \n\nWe can plot these data as a single line:\n\n# plot all counts by year\nggplot(yearly_counts) +\n  geom_line(aes(x = year_of_birth, y = n))\n\n\n\n\nHere, n represents the number of patients born in each year, from the count table created above. The result isn’t very satisfying, because we also grouped by disease. We can improve this by plotting each disease on a separate line, which is more appropriate when there are multiple data points per year:\n\n# plot one line per cancer type\nggplot(yearly_counts) +\n  geom_line(aes(x = year_of_birth, y = n, \n                group = disease))\n\n\n\n\nMoreover, we can color each line individually:\n\n# color each line per cancer type\nggplot(yearly_counts) +\n  geom_line(aes(x = year_of_birth, y = n, color = disease))\n\n\n\n\nNote that you don’t have to include a separate argument for group = disease because grouping is assumed by color = disease.\n\nChallenge-line\nCreate a line plot for year of birth and number of patients with lines representing each gender. Hint: you’ll need to manipulate the birth_reduced dataset first.\n\n\nChallenge-dash\nHow do you show differences in lines using dashes/dots instead of color?"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#faceting",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#faceting",
    "title": "Data visualization in R",
    "section": "Faceting",
    "text": "Faceting\nSo far we’ve been working on building single plots, which can show us two main variables (for the x and y axes) and additional variables using color (and potentially size/shape/etc). Scientific visualizations often need to compare among categories (e.g., control vs various treatments), which is generally clearer if those categories are presented in separate panels. ggplot provides this capacity through faceting.\nLet’s revisit the scatterplot we initially created, plotting age at diagnosis by cigarettes per day, with points colored by disease. We add an additional layer to create facets, or separate panels, for a given variable (in this case, the same variable being used to color points):\n\n# use previous scatterplot, but separate panels by disease\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_wrap(vars(disease)) # wraps panels to make a square/rectangular plot\n\n\n\n\nvars is used for faceting in the same way that aes() is used for mapping: it is used to specify the variable to form facet groups.\nfacet_wrap determines how many rows and columns of panels are needed to create the most square-shaped final plot possible. This becomes useful when there are many more categories:\n\n# add a variable by leaving color but changing panels to other categorical data\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_wrap(vars(tumor_stage))\n\n\n\n\nIn this case, we’re now visualizing an additional variable (tumor stage), in addition to the original three (age at diagnosis, cigarettes per day, and disease).\nIf you want to control the specific layout of panels, you can use facet_grid instead of facet_wrap:\n\n# scatterplots with panels for vital status in one row\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_grid(rows = vars(vital_status)) \n\n\n\n\nThis method can also plot panels in columns.\nWe may want to show interactions between two categorical variables, by arranging panels into rows according to one variable and columns according to another:\n\n# add another variable using faceting\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_grid(rows = vars(vital_status), cols = vars(disease)) # arrange plots via variables in rows, columns\n\n\n\n\nDon’t forget to look at the help documentation (e.g., ?facet_grid) to learn more about additional ways to customize your plots!\n\nChallenge-panels\nAlter your last challenge plot of (birth year by number of patients) to show each gender in separate panels.\n\n\nChallenge-axis\nHow do you change axis formatting, like tick marks and lines? Hint: You may want to use Google!"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#wrapping-up",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#wrapping-up",
    "title": "Data visualization in R",
    "section": "Wrapping up",
    "text": "Wrapping up\nThis material introduced you to ggplot as a tool for data visualization, allowing you to now create publication-quality images using R code. Combined with our previous explorations of the basic principles of R syntax, importing and extracting data with base R, and manipulating data using tidyverse, you should be equipped to continue learning about R on your own and developing code to meet your research needs.\nIf you are interested in learning more about ggplot: - Documentation for all ggplot features is available here. - RStudio also publishes a ggplot cheat sheet that is really handy!"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#extra-exercises",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#extra-exercises",
    "title": "Data visualization in R",
    "section": "Extra exercises",
    "text": "Extra exercises\n\nChallenge-improve\nImprove one of the plots previously created today, by changing thickness of lines, name of legend, or color palette http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html",
    "title": "Functions and objects",
    "section": "",
    "text": "Welcome to Introduction to R from fredhutch.io! This course introduces you R by working through common tasks in data science: importing, manipulating, and visualizing data.\nR is a statistical and programming computer language widely used for a variety of applications.\nBefore proceeding with these training materials, please ensure you have installed both R and RStudio as described here.\nBy the end of this session, you should be able to:\n\nwork within the RStudio interface to run and save R code in a project\nunderstand basic R syntax to use functions and assign objects\ncreate and manipulate vectors and understand how R deals with missing data"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#objectives",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#objectives",
    "title": "Functions and objects",
    "section": "",
    "text": "Welcome to Introduction to R from fredhutch.io! This course introduces you R by working through common tasks in data science: importing, manipulating, and visualizing data.\nR is a statistical and programming computer language widely used for a variety of applications.\nBefore proceeding with these training materials, please ensure you have installed both R and RStudio as described here.\nBy the end of this session, you should be able to:\n\nwork within the RStudio interface to run and save R code in a project\nunderstand basic R syntax to use functions and assign objects\ncreate and manipulate vectors and understand how R deals with missing data"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#a-brief-orientation-to-rstudio",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#a-brief-orientation-to-rstudio",
    "title": "Functions and objects",
    "section": "A brief orientation to RStudio",
    "text": "A brief orientation to RStudio\nR is a statistical programming language, while RStudio is an integrated development environment (IDE) that allows you to code in R more easily. RStudio possesses many features that you may find useful in your work. We’ll highlight a few of the most common and useful parts for our introductory course.\nThe first time you open RStudio, you’ll see three panels, or windows.\n\nThe panel on the left is the console, where you can run R code. The text printed in this panel is basic information about R and the version you’re running. You can test how the console can be used to run code by entering 3 + 4 and then pressing enter. This instructs your computer to read, interpret, and execute the command, then print the result (7) to the Console, and show a right facing arrow (&gt;), indicating it is ready to accept additional code.\nThe panel on the top right is the environment. It’s empty right now, but we’ll learn more about this later in this lesson.\nThe panel on the lower right shows the files present in your working directory. Currently, that’s probably your Home directory, which includes folders like Documents and Downloads.\n\nYou may notice that some of the panels possess additional tabs. We’ll explore some of these features in this class, but for more information:\nHelp -&gt; Cheetsheets -&gt; RStudio IDE cheat sheet\nThis PDF includes an overview of each of the things you see in RStudio, as well as explanations of how you can use them. It may be intimidating right now, but will come in handy as you gain experience with R.\nOne of the ways that RStudio makes working in R easier is by allowing you to create R projects. You can think of a project as a discrete unit of work, such as a chapter of a thesis/dissertation, analysis for a manuscript, or a monthly report. We recommend organizing your code, data, and other associated files as projects, which allows you to keep all parts of an analysis together for easier access.\nWe’ll be creating a project to use for the duration of this course. Create a new project in RStudio:\n\nFile -&gt; New Project\nChoose New Directory, then New Project\nname your project intro_r and save it somewhere on your computer you’ll be able to find easily later (we recommend your Desktop or Documents)\nClick Create project\n\nAfter your RStudio screen reloads, note two things:\n\nThe file browser in the lower right panel will now show the contents of a new folder, intro_r, that was created as a part of your RStudio project.\nThe console window will show the path, or location in your computer, for your project directory. This is important later in class, when this path will be required to locate data for analysis.\n\nNow we’re ready to create a new R script:\n\nFile -&gt; New File -&gt; R Script\nSave the new file as class1.R. By default, RStudio will save this in your project directory.\n\nThis R script is a text file that we’ll use to save code we learn in this class. We’ll refer to this window as the script or source window. Remember to save this file periodically to retain the record of the work you’re doing, so you can re-execute the code later if necessary.\nBy convention, a script should include a title at the top, so type the following on the first line:\n# Introduction to R: Class 1"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#using-functions",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#using-functions",
    "title": "Functions and objects",
    "section": "Using functions",
    "text": "Using functions\nNow that we have a project and new script set up, we’re ready to begin adding code. Skipping a line after the title, type the following on the next two lines:\n\n# basic math\n4 + 5 \n\n[1] 9\n\n\nThe first of the two boxes above represents the code you execute. The second box (prefaced with ##) shows the output you should expect. The [1] in the second box means there is one item (in this case, 9) present in the output.\nThe first line in that example is a code comment. It is not interpreted by R, but is a human-readable explanation of the code that follows. This is also how we included a title in our script. In R, anything to the right of one or more # symbols represents a comment.\nThe code above is the same mathematical operation we executed earlier. If we wanted to re-run this command, we have two options:\n\nCopy and paste the code into the Console\nUse the Run button at the top of the script window\nUse the keyboard shortcut: Ctrl + Enter\n\nThe third option is the most efficient, especially as your coding skills progress. With your cursor on the line with 4 + 5, hold down the Control key and press Enter. You’ll see the code and answer both appear in the Console. A few things to note about this keyboard shortcut:\n\nIt doesn’t matter where your cursor is on the line of code; the entire line will be executed with the keyboard shortcut.\nIf there isn’t code on the line where your cursor is located, RStudio will attempt to execute following lines.\n\nIn practice, a script should represent code you are developing in R, and you should only save the code that you know functions. For this class, we’ll be including notes about things we learn as comments.\n\nCtrl + Enter is the only keyboard shortcut we emphasize in this course, but there are many others available. You can view them on the second page of the cheat sheet linked above, or by going to Help -&gt; Keyboard Shortcuts Help.\n\nIf you were looking carefully, you may have noticed that the + in the previous code example had spaces on either side, separating it from the numbers. You may wonder whether spaces matter in how the code is interpreted. As with many questions in coding, the easiest way to assess whether removing the spaces matters is to simply try it:\n\n# same code as above, without spaces\n4+5\n\n[1] 9\n\n\nGiven the output, we can conclude that spaces do not matter in how the code functions. In this case, however, spaces represent a common convention in formatting R code, as it makes it easier for human eyes to read. In general, you should attempt to replicate the code presented here as closely as possible, and we’ll do our best to note when something is required as opposed to convention.\n\nCode convention and style doesn’t make or break the ability of your code to run, but it does affect whether other people can easily understand your code. A brief overview of common code style is available here, and more information is available in the tidyverse style guide.\n\nSo far, we’ve used R with mathematical symbols representing operations. R possesses the ability to perform much more complex tasks using functions, which is a pre-defined set of code that allows you to repeat particular actions.\nR includes functions for other types of math:\n\n# using a function: rounding numbers\nround(3.14)\n\n[1] 3\n\n\nIn this case, round is the function, and 3.14 is the number (data) being manipulated by the funcion. A word followed by parentheses is a common format for functions in R.\n\nSyntax refers to the rules that dictate how combinations of words and symbols are interpreted in a language (either programming or human).\n\nAdditional options for modifying functions are called arguments, and are included with the data between parentheses. For the round function, a common modification would be the number of decimal points output. You can change this detail by adding a comma and then additional argument:\n\n# using a function with more arguments\nround(3.14, digits = 1)\n\n[1] 3.1\n\n\nIf you would like to learn more about how this function works, you can go to the bottom righthand panel and click on the Help tab. Enter the name of a function into the search box and hit Enter. Alternatively, execute the following in your console:\n?round\nThis is a shortcut for performing the same task in the panel described above.\nR help documentation tends to be formatted very consistently. At the very top, you’ll see the name of the function. Below that, a short title indicates the purpose of the function, along with a more verbose “Description”. “Usage” tells you how to use the function in code, and “Arguments” details each of the optiond in “Usage”. The rest of the subheadings should be self-explanatory.\nIn the example above, there is no label associated with 3.14. In reality, 3.14 represents x, so the command can actually be written as round(x = 3.14, digits = 1). Even if not explicitly stated, the computer assumes that 3.14 represents x if the number is the first thing that appears after the opening parenthesis.\nIf you define both arguments explicitly, you can switch the order in which they appear:\n\n# can switch order of arguments\nround(digits = 1, x = 3.14)\n\n[1] 3.1\n\n\nIf you remove the labels (round(1, 3.14)), the answer is different, because R is assuming you mean round(x = 1, digits = 3.14).\n\nYou may notice that boxes pop up as you type. These represent RStudio’s attempts to guess what you’re typing and share additional options.\n\n\nChallenge-hist\nWhat does the function hist do? What are its main arguments? How did you determine this?"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#assigning-objects",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#assigning-objects",
    "title": "Functions and objects",
    "section": "Assigning objects",
    "text": "Assigning objects\nSo far, we’ve been performing tasks with R that require us to input the data manually. One of the strengths of using a programming language is the ability to assign data to objects, or variables.\n\nObjects in R are referred to as variables in other programming languages. We’ll use these terms synonymously for this course, though in other contexts there may be differences between them. Please see the R documentation on objects for more information.\n\nLike in math, a variable is a word used to represent a value (in this case, a number):\n\n# assigning value to an object\nweight_kg &lt;- 55\n\nIn the code above, &lt;- is the assignment operator: it instructs R to recognize weight_kg as representing the value 55. You can think of this code as referencing “55 goes into weight_kg.”\nAfter executing the code above, you’ll see the object appear in the Environment panel on the upper right hand side of the RStudio screen. The name of the object will appear on the left, with the value assigned to it on the right.\nThe name you assign to objects can be arbitrary, but we recommend using names that are relatively short and meaningful in the context of the values they represent. It’s useful to also know other general limitations on object names:\n\ncase sensitive\ncannot start with numbers\navoid other common words in R (e.g., function names, like mean)\navoid dots (underscores are a good alternative, such as the example above)\n\nExtra information on object names is available in the tidyverse style guide.\nNow that the object has been assigned, we can reference that object by executing its name:\n\n# recall object\nweight_kg\n\n[1] 55\n\n\nThus, the value weight_kg represents is printed to the Console.\nWe can also perform operations on an object:\n\n# multiple an object (convert kg to lb)\n2.2 * weight_kg\n\n[1] 121\n\n\nIn that case, the answer is printed to the Console. You can also assign the output to a new object:\n\n# assign weight conversion to object\nweight_lb &lt;- 2.2 * weight_kg\n\nAfter executing that line of code, you’ll see weight_lb appear in the Environment panel, too.\nNow let’s explore what happens if we assign a value to an existing object name:\n\n# reassign new value to an object\nweight_kg &lt;- 100\n\nNote that the value assigned to weight_kg as it appears in the Environment panel changes after executing the code above.\nHas the value assigned to weight_lb also changed? You might expect this would be the case, since this value is derived from weight_kg. However, weight_kg remains the same as previously assigned. If you want the value for weight_kg to reflect the new value for weight_kg, you will need to again execute weight_lb &lt;- 2.2 * weight_kg. This should help you understand an important concept in writing code: the order in which you execute lines of code matters! In the context of the material we cover in this class, we’ll continue saving code in scripts so we have a record of both the relevant commands and the appropriate order for execution.\n\nYou can think of the names of objects like sticky notes. You have the option to place the sticky note (name) on any value you choose. You can pick up the sticky note and place it on another value, but you need to explicitly tell R when you want values assigned to certain objects.\n\nAt this point in the lesson, it’s common to have accidentally created an object with a typo in the name. If this has happened to you, it’s useful to know how to remove the object to keep your environment up to date. Here, we’ll practice removing an object with something everyone has available:\n\n# remove object\nremove(weight_lb) \n\nThis removes the specified object from the environment, which you can confirm by its absence in the Environment panel. You can also abbreviate this command to rm(weight_lb).\n\nYou can clear the entire environment using the button at the top of the Environment panel with a picture of a broom. This may seem extreme, but don’t worry! We can re-create all the work we’ve already done by executing each line of code again.\n\n\nChallenge-values\nFor the code chunk below, what is the value of each item at each step?\n\n\nmass &lt;- 47.5            # mass?\nwidth  &lt;- 122             # width?\nmass &lt;- mass * 2.0      # mass?\nwidth  &lt;- width - 20        # width?\nmass_index &lt;- mass/width  # mass_index?"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#vectors",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#vectors",
    "title": "Functions and objects",
    "section": "Vectors",
    "text": "Vectors\nSo far, we’ve worked with objects containing a single value. For most research purposes, however, it’s more realistic to work with a collection of values. We can do that in R by creating a vector with multiple values:\n\n# assign vector\nages &lt;- c(50, 55, 60, 65) \n# recall vector\nages\n\n[1] 50 55 60 65\n\n\nThe c function used above stands for “combine,” meaning all of the values in parentheses after it are included in the object. This is reflected in the Console, where recalling the value shows all four values, and the Environment window, where multiple values are shown on the right side.\nWe can use functions to ask basic questions about our vector, including:\n\n# how many things are in object?\nlength(ages)\n\n[1] 4\n\n# what type of object?\nclass(ages)\n\n[1] \"numeric\"\n\n# get overview of object\nstr(ages)\n\n num [1:4] 50 55 60 65\n\n\nIn the code above, we learn that there are four items (values) in our vector, and that the vector is composed of numeric data. str stands for “structure”, and shows us a general overview of the data, including a preview of the first few values (or all the values, as is the case in our small vector).\nEven more useful is the ability to use functions to perform more complex tasks for us, such as statistical summaries:\n\n# performing functions with vectors\nmean(ages)\n\n[1] 57.5\n\nrange(ages)\n\n[1] 50 65\n\n\nAlthough we’ve focused on numbers as data so far, it’s also possible for data to be words instead:\n\n# vector of body parts\norgans &lt;- c(\"lung\", \"prostate\", \"breast\")\n\nIn this case, each word is encased in quotation marks, indicating these are character data, rather than object names.\n\nChallenge-organs\nPlease answer the following questions about organs: - How many values are in organs? - What type of data is organs? - How can you see an overview of organs?\n\nWe’ve seen data as numbers and letters so far. In fact, R has all of the following basic data types:\n\ncharacter: sometimes referred to as string data, tend to be surrounded by quotes\nnumeric: real or decimal numbers, sometimes referred to as “double”\ninteger: a subset of numeric in which numbers are stored as integers\nlogical: Boolean data (TRUE and FALSE)\ncomplex: complex numbers with real and imaginary parts (e.g., 1 + 4i)\nraw: bytes of data (machine readable, but not human readable)\n\nThe three data types listed in bold above are the focus of this class. R automatically interprets the type as you enter data. Most data analysis activities will not require you to understand specific details of the other data types.\n\nChallenge-dtypes\nR tends to handle interpreting data types in the background of most operations. The following code is designed to cause some unexpected results in R. What is unusual about each of the following objects?\n\n\nnum_char &lt;- c(1, 2, 3, \"a\")\nnum_logical &lt;- c(1, 2, 3, TRUE)\nchar_logical &lt;- c(\"a\", \"b\", \"c\", TRUE)\ntricky &lt;- c(1, 2, 3, \"4\")"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#manipulating-vectors",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#manipulating-vectors",
    "title": "Functions and objects",
    "section": "Manipulating vectors",
    "text": "Manipulating vectors\nIn the section above, we learned to create and assess vectors, and use functions to calculate statistics across the values. We can also modify a vector after it’s been created:\n\n# add a value to end of vector\nages &lt;- c(ages, 90) \n\nThe example above uses the same combine (c) function as when we initially created the vector. We can also use it to add values to the beginning of the vector:\n\n# add value at the beginning\nages &lt;- c(30, ages)\n\nIf we wanted to extract, or subset, a portion of a vector:\n\n# extracting second value\norgans[2] \n\n[1] \"prostate\"\n\n\nIn general, square brackets ([ ]) in R refer to a part of an object. The number 2 indicates the second value in the vector.\n\nThe index position of a value is the number associated with its location in a collection. In the example above, note that R indexes (or counts) starting with 1. This is different from many other programming languages, like Python, which use 0-based indexing.\n\nIn R, a minus sign (-) can be used to negate a value’s position, which excludes that value from the output:\n\n# excluding second value\norgans[-2] \n\n[1] \"lung\"   \"breast\"\n\n\nYou may be tempted to try extracting multiple values at a time by separating the numbers with commas (e.g., organs[2,3]). This will result in a rather cryptic error, which we’ll talk more about next time. For now, remember that you can use the combine function to indicate multiple values for subsetting:\n\n# extracting first and third values\norgans[c(1, 3)] \n\n[1] \"lung\"   \"breast\"\n\n\nWe’ll switch back to our numerical ages object to explore another common need when subsetting: extracting values based on a condition (or criteria). For numerical data, we’re often interested in extracting data that are in a certain range of values. It is tempting to try something like:\n\nages &gt; 60 \n\n[1] FALSE FALSE FALSE FALSE  TRUE  TRUE\n\n\nThe result, however, is less than satisfying: you receive either TRUE or FALSE for each data point, depending on whether it meets the condition or not.\nWhile that information isn’t quite what we expected, we can combine it with the subsetting syntax we learned earlier:\n\n# extracts values which meet condition\nages[ages &gt; 60] \n\n[1] 65 90\n\n\nIf we read the code above from the inside out (a common strategy for R), the code above identifies which values meet the criteria, and the square brackets are used to extract this from the original vector.\nIf you want to extract items exactly equal to a specific value, you need to use two equal signs:\n\n# extracts values numerically equivalent values\nages[ages == 60]\n\n[1] 60\n\n\nYou can think of this as a way to differentiate mathematical equivalency from specification of parameters for arguments (such as digits = 1 for round(), as we learned earlier). R also allows you to use &lt;= and &gt;=.\nFinally, it’s common to need to combine conditions while subsetting. For example, you may be interested in only values between 50 and 60:\n\n# ages less than 50 OR greater than 60\nages[ages &lt; 50 | ages &gt; 60]\n\n[1] 30 65 90\n\n\nIn the code above, the vertical pipe | is interpreted to mean “or,” so each data point can belong to either the category on the left of the pipe, the category on the right, or both. In other words, the vertical pipe means any single value being evaluated must meet one or both conditions.\nYou can also combine conditions with &, but this means any single value must meet both conditions:\n\n# ages greater than 50 OR less than 60\nages[ages &gt; 50 & ages &lt; 60]\n\n[1] 55\n\n\n\nBe careful when thinking about human language as opposed to programming languges. When speaking, we is reasonable to say “extract all values below 50 and above 60.” While this makes sense in context, it is mathematically impossible for a value to be both less than 50 AND greater than 60.\n\n\nChallenge-compare\nWhy does the following code return the answer it\n\n\n\"four\" &gt; \"five\"\n\n[1] TRUE"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#missing-data",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#missing-data",
    "title": "Functions and objects",
    "section": "Missing data",
    "text": "Missing data\nMost of the data we encounter has missing data. Programming languages interpret and handle missing data in different ways, so it’s worth taking time to dig into how R approaches this issue.\nFirst, we’ll create a new vector some values indicated as missing data:\n\n# create a vector with missing data\nheights &lt;- c(2, 4, 4, NA, 6)\n\nIn the vector above, NA represents a value where data are missing. You may notice NA is not encased in quotation marks. This is because R interprets that set of characters specifically as missing data.\nNext, let’s investigate how this vector responds to use in functions:\n\n# calculate mean and max on vector with missing data\nmean(heights)\n\n[1] NA\n\nmax(heights)\n\n[1] NA\n\n\nThe answer isn’t very satisfying; we’re told the answer is missing data because of the presence of a single missing value in the vector. This is a slightly frustrating default behavior for some common statistical functions in R, but we can add an argument to ignore missing data and calculate across the remaining values:\n\n# add argument to remove NA\nmean(heights, na.rm = TRUE)\n\n[1] 4\n\nmax(heights, na.rm = TRUE)\n\n[1] 6\n\n\nIn the code above, the na.rm parameter controls whether missing data are removed. The default (which you can also reference in the help documentation) is for missing values to be included (na.rm = FALSE). By switching to na.rm = TRUE, we’re instructing R to remove missing data.\nThe example above retains missing values in the dataset while performing calculations. There are certainly cases in which you may want to specifically filter out the missing data from your dataset.\nThe function is.na allows you to ask whether elements in a dataset are missing:\n\n# identify elements which are missing data\nis.na(heights)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\nIf a resulting value is TRUE, the value is missing. If FALSE, the data point is present. We can invert the resulting logical data using an exclamation point:\n\n# reverse the TRUE/FALSE\n!is.na(heights)\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE\n\n\nThis means missing data are now listed as FALSE, with data present as TRUE.\nAs with the conditional statements we learned earlier, we can combine these results with our square bracket subsetting syntax to extract only values that are present in the dataset:\n\n# extract elements which are not missing values\nheights[!is.na(heights)]\n\n[1] 2 4 4 6\n\n\nAlternatively, you can use a function specifically designed for excluding (omitting) missing data:\n\n# remove incomplete cases\nna.omit(heights) \n\n[1] 2 4 4 6\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n\nYou may notice that this output looks slightly different than the previous example. This is because na.omit includes output about attributes, or information about the data. The output vectors are the same for the last two code examples, even though the way they appear in the Console seems different.\n\nIf you aren’t sure how to interpret the output in your console, sometimes it helps to assign the output to an object. You can then inspect the data type, structure, etc to ensure you’re getting the answer you expected.\n\n\nChallenge-analyze\nComplete the following tasks after executing the code chunk below. (Note: there are multiple solutions): - Remove NAs - Calculate the median - Identify how many elements in the vector are greater than 67 inches - Visualize the data as a histogram (hint: function hist)\n\n\n# create vector\nmore_heights &lt;- c(63, 69, 60, 65, NA, 68, 61, 70, 61, 59, 64, 69, 63, 63, NA, 72, 65, 64, 70, 63, 65)"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#wrapping-up",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#wrapping-up",
    "title": "Functions and objects",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this session, we spent some time getting to know the RStudio interface for writing and running R code, explored the basic principles of R syntax for functions and object assignment, and worked with vectors to understand how R handles missing data.\nIn the next session, we’ll learn to import spreadsheet-style data that are more similar to what you’d like handle for a research project, and practice accessing different portions of the data.\nWhen you are done working in RStudio, you should save any changes to your R script. When you close RStudio, you will see a pop-up box asking if you want to save your workspace image. We do not recommend saving your project in this way, as it creates extra (hidden) files on your computer that can be unwieldy in size and inadvertently retain sensitive data (if you’re working with PHI or other private data). If you’ve saved your R script, you can recreate all the work you’ve accomplished. For more information on this topic, please review this explanation. If you would like to prevent this box from popping up in the future, we recommend:\n\nGo to Tools -&gt; Global Options (Global means for all projects; you can also change this for each project using Project Options)\nIn the drop-down menu next to Save workspace to ~/.Rdata on exit select Never.\n\nIf you need to reopen your project after closing RStudio, you should use the File -&gt; Open Project and navigate to the location of your project directory. Alternatively, using your operating system’s file browser, double click on the r_intro.Rrpoj file."
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#extra-exercises",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#extra-exercises",
    "title": "Functions and objects",
    "section": "Extra exercises",
    "text": "Extra exercises\n\nChallenge-objects\n\nCreate an object called agge that contains your age in years\nReassign the object to a new object called age (e.g., correct the typo)\nRemove the previous object from your environment\nCalculate your age in days\n\n\n\nChallenge-char\nThe following vector represents the number of vacation days possessed by various employees:\n\nvacation_days &lt;- c(5, 7, 20, 1, 0, 0, 12, 4, 2, 2, 2, 4, 5, 6, 7, 10, 4)\n\n\nHow many employees are represented in the vector?\nHow many employees have at least one work week’s worth of vacation available to them?"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html",
    "href": "resources/computing/intro_to_r/2_working_with_data.html",
    "title": "Working with data",
    "section": "",
    "text": "In the last section, we learned some fundamental principles of R and how to work in RStudio.\nIn this session, we’ll continue our introduction to R by working with a large dataset that more closely resembles that which you may encounter while analyzing data for research. By the end of this session, you should be able to:\n\nimport spreadsheet-style data into R as a data frame\nextract portions of data from a data frame\nmanipulate factors (categorical data)"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#objectives",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#objectives",
    "title": "Working with data",
    "section": "",
    "text": "In the last section, we learned some fundamental principles of R and how to work in RStudio.\nIn this session, we’ll continue our introduction to R by working with a large dataset that more closely resembles that which you may encounter while analyzing data for research. By the end of this session, you should be able to:\n\nimport spreadsheet-style data into R as a data frame\nextract portions of data from a data frame\nmanipulate factors (categorical data)"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#importing-spreadsheet-style-data-into-r",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#importing-spreadsheet-style-data-into-r",
    "title": "Working with data",
    "section": "Importing spreadsheet-style data into R",
    "text": "Importing spreadsheet-style data into R\nOpen RStudio, and we’ll check to make sure you’re ready to start work again. You can check to see if you’re working in your project directory by looking at the top of the Console. You should see the path (location in your computer) for the project directory you created last time (e.g., ~/Desktop/intro_r).\nIf you do not see the path to your project directory, go to File -&gt; Open Project and navigate to the location of your project directory. Alternatively, using your operating system’s file browser, double click on the r_intro.Rrpoj file.\nCreate a new R script (File -&gt; New File -&gt; R Script) and save it in your project directory with the name class2.R. Place the following comment on the top line as a title:\n# Introduction to R: Class 2\nIn the last session, we recommended organizing your work in directories (folders) according to projects. While a thorough discussion of project organization is beyond the scope of this class, we will continue to model best practices by creating a directory to hold our data:\n\n# make a directory\ndir.create(\"data\")\n\nYou should see the new directory appear in your project directory, in the lower right panel in RStudio. There is also a button in that panel you can use to create a new folder, but including the code to perform this task makes other people (and yourself) able to reproduce your work more easily.\nNow that we have a place to store our data, we can go ahead and download the dataset:\n\n# download data from url\ndownload.file(\"https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.csv\", \"data/clinical.csv\")\n\nThe code above has two arguments, both encompassed in quotation marks: first, you indicate where the data can be found online. Second, you indicate where R should store a copy of the file on your own computer.\nThe output from that command may look alarming, but it represents information confirming it worked. You can click on the data folder to ensure the file is now present.\nNotice that the URL above ends in clinical.csv, which is also the name we used to save the file on our computers. If you click on the URL and view it in a web browser, the format isn’t particularly easy for us to understand. You can also view the file by clicking on it in the lower right hand panel, then selecting “View File.”\n\nThe option to “Import Dataset” you see after clicking on the file references some additional tools present in RStudio that can assist with various kinds of data import. Because this requires installing additional software, complete exploration of these options is outside the scop of this class. For more information, check out this article.\n\nThe data we’ve downloaded are in csv format, which stands for “comma separated values.” This means the data are organized into rows and columns, with columns separated by commas.\nThese data are arranged in a tidy format, meaning each row represents an observation, and each column represents a variable (piece of data for each observation). Moreover, only one piece of data is entered in each cell.\nNow that the data are downloaded, we can import the data and assign to an object:\n\n# import data and assign to object\nclinical &lt;- read.csv(\"data/clinical.csv\")\n\nYou should see clinical appear in the Environment window on the upper right panel in RStudio. If you click on clinical there, a new tab will appear next to your R script in the Source window.\n\nClicking on the name of an object in the Environment window is a shortcut for running View(clinical); you’ll see this code appear in the Console after clicking.\n\nNow that we have the data imported and assigned to an object, we can take some time to explore the data we’ll be using for the rest of this course:\n\nThese data are clinical cancer data from the National Cancer Institute’s Genomic Data Commons, specifically from The Cancer Genome Atlas, or TCGA.\nEach row represents a patient, and each column represents information about demographics (race, age at diagnosis, etc) and disease (e.g., cancer type).\nThe data were downloaded and aggregated using an R script, which you can view in the GitHub repository for this course.\n\nThe function we used to import the data is one of a family of commands used to import the data. Check out the help documentation for read.csv for more options for importing data.\n\nYou can also import data directly into R using read.csv, using clinical &lt;- read.csv(\"https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.csv\"). For these lessons, we model downloading and importing in two steps, so you retain a copy of the data on your computer. This reflects how you’re likely to import your own data, as well as recommended practice for retaining data used in an analysis (since data online may be updated).\n\n\nChallenge-data\nDownload, inspect, and import the following data files. The URL for each sample dataset is included along with a name to assign to the object. (Hint: you can use the same function as above, but may need to update the sep = parameter) - URL: https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.tsv, object name: example1 - URL: https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.txt, object name: example2\n\nImporting data can be tricky and frustrating, However, if you can’t get your data into R, you can’t do anything to analyze or visualize it. It’s worth understanding how to do it effectively to save you time and energy later."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#data-frames",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#data-frames",
    "title": "Working with data",
    "section": "Data frames",
    "text": "Data frames\nNow that we have data imported and available, we can start to inspect the data more closely.\nThese data have been interpreted by R to be a data frame, which is a data structure (way of organizing data) that is analogous to tabular or spreadsheet style data. By definition, a data frame is a table made of vectors (columns) of all the same length. As we learned in our last session, a vector needs to include all of the same type of data (e.g., character, numeric). A data frame, however, can include vectors (columns) of different data types.\nTo learn more about this data frame, we’ll first explore its dimensions:\n\n# assess size of data frame\ndim(clinical)\n\n[1] 6832   20\n\n\nThe output reflects the number of rows first (6832), then the number of columns (20).\nWe can also preview the content by showing the first few rows:\n\n# preview first few rows\nhead(clinical) \n\n  primary_diagnosis tumor_stage age_at_diagnosis vital_status morphology\n1             C34.1    stage ia            24477         dead     8070/3\n2             C34.1    stage ib            26615         dead     8070/3\n3             C34.3    stage ib            28171         dead     8070/3\n4             C34.1    stage ia            27154        alive     8083/3\n5             C34.3   stage iib            29827         dead     8070/3\n6             C34.1  stage iiia            23370        alive     8070/3\n  days_to_death state tissue_or_organ_of_origin days_to_birth\n1           371  live                     C34.1        -24477\n2           136  live                     C34.1        -26615\n3          2304  live                     C34.3        -28171\n4            NA  live                     C34.1        -27154\n5           146  live                     C34.3        -29827\n6            NA  live                     C34.1        -23370\n  site_of_resection_or_biopsy days_to_last_follow_up cigarettes_per_day\n1                       C34.1                     NA          10.958904\n2                       C34.1                     NA           2.191781\n3                       C34.3                   2099           1.643836\n4                       C34.1                   3747           1.095890\n5                       C34.3                     NA                 NA\n6                       C34.1                   3576           2.739726\n  years_smoked gender year_of_birth         race              ethnicity\n1           NA   male          1936        white not hispanic or latino\n2           NA   male          1931        asian not hispanic or latino\n3           NA female          1927        white not hispanic or latino\n4           NA   male          1930        white not hispanic or latino\n5           NA   male          1923 not reported           not reported\n6           NA female          1942 not reported           not reported\n  year_of_death bcr_patient_barcode disease\n1          2004        TCGA-18-3406    LUSC\n2          2003        TCGA-18-3407    LUSC\n3            NA        TCGA-18-3408    LUSC\n4            NA        TCGA-18-3409    LUSC\n5          2004        TCGA-18-3410    LUSC\n6            NA        TCGA-18-3411    LUSC\n\n\nThe default number of rows shown is six. You can specify a different number using the n = parameter, demonstrated below using tail, which shows the last few rows\n\n# show last three rows\ntail(clinical, n = 3) \n\n     primary_diagnosis  tumor_stage age_at_diagnosis vital_status morphology\n6830             C54.1 not reported            27326         dead     8950/3\n6831             C54.1 not reported            24781        alive     8950/3\n6832             C54.1 not reported            20318        alive     8950/3\n     days_to_death state tissue_or_organ_of_origin days_to_birth\n6830           949  live                     C54.1        -27326\n6831            NA  live                     C54.1        -24781\n6832            NA  live                     C54.1        -20318\n     site_of_resection_or_biopsy days_to_last_follow_up cigarettes_per_day\n6830                       C54.1                     NA                 NA\n6831                       C54.1                    587                 NA\n6832                       C54.1                      0                 NA\n     years_smoked gender year_of_birth  race              ethnicity\n6830           NA female          1932 white not hispanic or latino\n6831           NA female          1945 white not hispanic or latino\n6832           NA female          1957 asian not hispanic or latino\n     year_of_death bcr_patient_barcode disease\n6830          2008        TCGA-NG-A4VW     UCS\n6831            NA        TCGA-QM-A5NM     UCS\n6832            NA        TCGA-QN-A5NN     UCS\n\n\nWe often need to reference the names of columns, so it’s useful to print only those to the screen:\n\n# view column names\nnames(clinical) \n\n [1] \"primary_diagnosis\"           \"tumor_stage\"                \n [3] \"age_at_diagnosis\"            \"vital_status\"               \n [5] \"morphology\"                  \"days_to_death\"              \n [7] \"state\"                       \"tissue_or_organ_of_origin\"  \n [9] \"days_to_birth\"               \"site_of_resection_or_biopsy\"\n[11] \"days_to_last_follow_up\"      \"cigarettes_per_day\"         \n[13] \"years_smoked\"                \"gender\"                     \n[15] \"year_of_birth\"               \"race\"                       \n[17] \"ethnicity\"                   \"year_of_death\"              \n[19] \"bcr_patient_barcode\"         \"disease\"                    \n\n\nIt’s also possible to view row names usingrownames(clinical), but our data only possess numbers for row names so it’s not very informative.\nAs we learned last time, we can use str to provide a general overview of the object:\n\n# show overview of object\nstr(clinical) \n\n'data.frame':   6832 obs. of  20 variables:\n $ primary_diagnosis          : chr  \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ tumor_stage                : chr  \"stage ia\" \"stage ib\" \"stage ib\" \"stage ia\" ...\n $ age_at_diagnosis           : int  24477 26615 28171 27154 29827 23370 19025 26938 28430 30435 ...\n $ vital_status               : chr  \"dead\" \"dead\" \"dead\" \"alive\" ...\n $ morphology                 : chr  \"8070/3\" \"8070/3\" \"8070/3\" \"8083/3\" ...\n $ days_to_death              : int  371 136 2304 NA 146 NA 345 716 2803 973 ...\n $ state                      : chr  \"live\" \"live\" \"live\" \"live\" ...\n $ tissue_or_organ_of_origin  : chr  \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_birth              : int  -24477 -26615 -28171 -27154 -29827 -23370 -19025 -26938 -28430 -30435 ...\n $ site_of_resection_or_biopsy: chr  \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_last_follow_up     : int  NA NA 2099 3747 NA 3576 NA NA 1810 956 ...\n $ cigarettes_per_day         : num  10.96 2.19 1.64 1.1 NA ...\n $ years_smoked               : int  NA NA NA NA NA NA NA NA NA NA ...\n $ gender                     : chr  \"male\" \"male\" \"female\" \"male\" ...\n $ year_of_birth              : int  1936 1931 1927 1930 1923 1942 1953 1932 1929 1923 ...\n $ race                       : chr  \"white\" \"asian\" \"white\" \"white\" ...\n $ ethnicity                  : chr  \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" ...\n $ year_of_death              : int  2004 2003 NA NA 2004 NA 2005 2006 NA NA ...\n $ bcr_patient_barcode        : chr  \"TCGA-18-3406\" \"TCGA-18-3407\" \"TCGA-18-3408\" \"TCGA-18-3409\" ...\n $ disease                    : chr  \"LUSC\" \"LUSC\" \"LUSC\" \"LUSC\" ...\n\n\nThe output provided includes:\n\ndata structure: data frame\ndimensions: 6832 rows and 20 columns\ncolumn-by-column information: each prefaced with a $, and includes the column name, data type (num, int, Factor)\n\n\nFactors are how character data are interpreted by R in data frames. We’ll talk more about working with factors at the end of this lesson.\n\nFinally, we can also examine basic summary statistics for each column:\n\n# provide summary statistics for each column\nsummary(clinical) \n\n primary_diagnosis  tumor_stage        age_at_diagnosis vital_status      \n Length:6832        Length:6832        Min.   : 3982    Length:6832       \n Class :character   Class :character   1st Qu.:19191    Class :character  \n Mode  :character   Mode  :character   Median :22842    Mode  :character  \n                                       Mean   :22320                      \n                                       3rd Qu.:26002                      \n                                       Max.   :32872                      \n                                       NA's   :114                        \n  morphology        days_to_death        state          \n Length:6832        Min.   :    0.0   Length:6832       \n Class :character   1st Qu.:  274.0   Class :character  \n Mode  :character   Median :  524.0   Mode  :character  \n                    Mean   :  878.2                     \n                    3rd Qu.: 1044.5                     \n                    Max.   :10870.0                     \n                    NA's   :4645                        \n tissue_or_organ_of_origin days_to_birth    site_of_resection_or_biopsy\n Length:6832               Min.   :-32872   Length:6832                \n Class :character          1st Qu.:-26002   Class :character           \n Mode  :character          Median :-22842   Mode  :character           \n                           Mean   :-22320                              \n                           3rd Qu.:-19191                              \n                           Max.   : -3982                              \n                           NA's   :114                                 \n days_to_last_follow_up cigarettes_per_day  years_smoked      gender         \n Min.   :  -64.0        Min.   : 0.008     Min.   : 8.00   Length:6832       \n 1st Qu.:  345.0        1st Qu.: 1.370     1st Qu.:30.75   Class :character  \n Median :  650.0        Median : 2.192     Median :40.00   Mode  :character  \n Mean   :  976.8        Mean   : 2.599     Mean   :39.96                     \n 3rd Qu.: 1259.0        3rd Qu.: 3.288     3rd Qu.:50.00                     \n Max.   :11252.0        Max.   :40.000     Max.   :63.00                     \n NA's   :1118           NA's   :5661       NA's   :6384                      \n year_of_birth      race            ethnicity         year_of_death \n Min.   :1902   Length:6832        Length:6832        Min.   :1990  \n 1st Qu.:1937   Class :character   Class :character   1st Qu.:2004  \n Median :1947   Mode  :character   Mode  :character   Median :2007  \n Mean   :1948                                         Mean   :2006  \n 3rd Qu.:1957                                         3rd Qu.:2010  \n Max.   :1993                                         Max.   :2014  \n NA's   :170                                          NA's   :5266  \n bcr_patient_barcode   disease         \n Length:6832         Length:6832       \n Class :character    Class :character  \n Mode  :character    Mode  :character  \n                                       \n                                       \n                                       \n                                       \n\n\nFor numeric data (such as year_of_death), this output includes common statistics like median and mean, as well as the number of rows (patients) with missing data (as NA). For factors (character data, such as disease), you’re given a count of the number of times the top six most frequent factors (categories) occur in the data frame."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#subsetting-data-frames",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#subsetting-data-frames",
    "title": "Working with data",
    "section": "Subsetting data frames",
    "text": "Subsetting data frames\nNow that our data are available for use, we can begin extracting relevant information from them.\n\n# extract first column and assign to a variable\nfirst_column &lt;- clinical[1]\n\nAs discussed last time with vectors, the square brackets ([ ]) are used to subset, or reference part of, a data frame. You can inspect the output object by clicking on it in the environment. It contains all of the rows for only the first column.\nWhen a single number is included in the square brackets, R assumes you are referencing a column. When you include two numbers in square brackets separated by a comma, R assumes the first number references the row and the second number references the column you desire.\nThis means you can also reference the first column as follows:\n\n# extract first column\nfirst_column_again &lt;- clinical[ , 1]\n\nLeaving one field blank means you want the entire set in the output (in this case, all rows).\n\nChallenge-extract\nWhat is the difference in results between the last two lines of code?\n\nSimilarly, we can also extract only the first row across all columns:\n\n# extract first row \nfirst_row &lt;- clinical[1, ]\n\nWe can also extract slices, or sections of rows and columns, such as a single cell:\n\n# extract cell from first row of first column\nsingle_cell &lt;- clinical[1,1]\n\nTo extract a range of cells, we use the same colon (:) syntax from last time:\n\n# extract a range of cells, rows 1 to 3, second column\nrange_cells &lt;- clinical[1:3, 2]\n\nThis works for ranges of columns as well.\nWe can also exclude particular parts of the dataset using a minus sign:\n\n# exclude first column\nexclude_col &lt;- clinical[ , -1] \n\nCombining what we know about R syntax, we can also exclude a range of cells using the c function:\n\n# exclude first 100 rows\nexclude_range &lt;- clinical[-c(1:100), ] \n\nSo far, we’ve been referencing parts of the dataset based on index position, or the number of row/column. Because we have included column names in our dataset, we can also reference columns using those names:\n\n# extract column by name\nname_col1 &lt;- clinical[\"tumor_stage\"]\nname_col2 &lt;- clinical[ , \"tumor_stage\"]\n\nNote the example above features quotation marks around the column name. Without the quotation marks, R will assume we’re attempting to reference an object.\nAs we discussed with subsetting based on index above, the two objects created above differ in the data structure. name_col1 is a data frame (with one column), while name_col2 is a vector. Although this difference in the type of object may not matter for your analysis, it’s useful to understand that there are multiple ways to accomplish a task, each of which may make particular code work more easily.\nThere are additional ways to extract columns, which use R specific for complex data objects, and may be useful to recognize as your R skills progress.\nThe first is to use double square brackets:\n\n# double square brackets syntax\nname_col3 &lt;- clinical[[\"tumor_stage\"]]\n\nYou can think of this approach as digging deeply into a complex object to retrieve data.\nThe final approach is equivalent to the last example, but can be considered a shortcut since it requires fewer keystrokes (no quotation marks, and only one symbol):\n\n# dollar sign syntax\nname_col4 &lt;- clinical$tumor_stage\n\nBoth of the last two approaches above return vectors. For more information about these different ways of accessing parts of a data frame, see this article.\nThe following challenges all use the clinical object:\n\nChallenge-days\nCode as many different ways possible to extract the column days_to_death.\n\n\nChallenge-rows\nExtract the first 6 rows for only age_at_diagnosis and days_to_death.\n\n\nChallenge-calculate\nCalculate the range and mean for cigarettes_per_day."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#factors",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#factors",
    "title": "Working with data",
    "section": "Factors",
    "text": "Factors\nNote: This section was written with a previous version of R that automatically interprets all character data as factors (this is not true of more recent versions of R). To execute the code in this section, please first import your data again, using the following modified command:\n\nclinical &lt;- read.csv(\"data/clinical.csv\", stringsAsFactors = TRUE)\n\nThis section explores one of the trickier types of data you’re likely to encounter: factors, which are how R interprets categorical data.\nWhen we imported our dataset into R, the read.csv function assumed that all the character data in our dataset are factors, or categories. Factors have predefined sets of values, called levels. We can explore what this means by first creating a factor vector:\n\n# create vector with factor data\ntest_data &lt;- factor(c(\"placebo\", \"test_drug\", \"placebo\", \"known_drug\"))\n# show factor\ntest_data\n\n[1] placebo    test_drug  placebo    known_drug\nLevels: known_drug placebo test_drug\n\n\nThis vector includes four pieces of data (often referred to as items or elements), which are printed as output above. The second line of the output shows information about the levels, or categories, of our vector. We can also access this information separately, which is useful if the data (vector) has a large number of elements:\n\n# show levels of factor\nlevels(test_data) \n\n[1] \"known_drug\" \"placebo\"    \"test_drug\" \n\n\nThe levels in this test dataset are currently listed in alphabetical order, which is the default presentation in R. The order of factors dictates how they are presented in subsequent analyses, so there are definitely cases in which you may want the levels in a specific order. In the case of test_data, we may want to keep the two drug treatments together, with placebo at the end:\n\n# reorder factors to put placebo at end\ntest_data &lt;- factor(test_data, levels = c(\"known_drug\", \"test_drug\", \"placebo\"))\n# show reordered\ntest_data\n\n[1] placebo    test_drug  placebo    known_drug\nLevels: known_drug test_drug placebo\n\n\nThis doesn’t change the data itself, but does make it easier to manage the data later.\nAnother useful aspect of factors is that they are stored as integers with labels. This means that you can easily convert them to numeric data:\n\n# converting factors to numeric\nas.numeric(test_data)\n\n[1] 3 2 3 1\n\n\nThis can be handy for some types of statistical analyses, and also illustrates the importance of ordering your levels appropriately.\nWe can apply this knowledge to our clinical dataset, by first observing how the data are presented when creating a basic plot:\n\n# quick and dirty plot\nplot(clinical$race)\n\n\n\n\nThe labels as presented by default are not particularly readable, and also lack appropriate capitalization and formatting. While it is possible to modify only the plot labels, we would have to do that for all of our subsequent analyses. It is more efficient to modify the levels once:\n\n# assign race data to new object \nrace &lt;- clinical$race \nlevels(race)\n\n[1] \"american indian or alaska native\"         \n[2] \"asian\"                                    \n[3] \"black or african american\"                \n[4] \"native hawaiian or other pacific islander\"\n[5] \"not reported\"                             \n[6] \"white\"                                    \n\n\nBy assigning the data to a new object, we can more easily perform manipulations without altering the original dataset.\nThe output above shows the current levels for race. We can access each level using their position in this order, combined with our knowledge of square brackets for subsetting:\n\nlevels(race)[1]\n\n[1] \"american indian or alaska native\"\n\n\nWe can modify them to improve their formatting by assigning a new level (name) of our choosing:\n\n# correct factor levels\nlevels(race)[1] &lt;- \"Am Indian\"\nlevels(race)[2] &lt;- \"Asian\" # capitalize asian\nlevels(race)[3] &lt;- \"black\"\nlevels(race)[4] &lt;- \"Pac Isl\"\nlevels(race)[5] &lt;- \"unknown\"\n# show revised levels\nlevels(race) \n\n[1] \"Am Indian\" \"Asian\"     \"black\"     \"Pac Isl\"   \"unknown\"   \"white\"    \n\n\nAlthough we’re not doing so here, we could also reorder the levels (as we did for test_data).\nOnce we are satisfied with the resulting levels, we assign the modified factor back to the original dataset:\n\n# replace race in data frame\nclinical$race &lt;- race\n# replot with corrected names\nplot(clinical$race)\n\n\n\n\nThis section was a very brief introduction to factors, and it’s likely you’ll need more information when working with categorical data of your own. A good place to start would be this article, and exploring some of the tools in the tidyverse (which we’ll discuss in the next lesson).\n\nChallenge-not-reported\nIn your clinical dataset, replace “not reported” in ethnicity with NA\n\n\nChallenge-remove\nWhat Google search helps you identify additional strategies for renaming missing data?"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#optional-creating-a-data-frame-by-hand",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#optional-creating-a-data-frame-by-hand",
    "title": "Working with data",
    "section": "Optional: Creating a data frame by hand",
    "text": "Optional: Creating a data frame by hand\nThis last section shows two different approaches to creating a data frame by hand (in other words, without importing the data from a spreadsheet). It isn’t particularly useful for most of your day-to-day work, and also not a method you want to use often, as this type of data entry can introduce errors. However, it’s frequently used in online tutorials, which can be confusing, and also helps illustrate how data frames are composed.\nThe first approach is to create separate vectors (columns), and then join them together in a second step:\n\n# create individual vectors\ncancer &lt;- c(\"lung\", \"prostate\", \"breast\")\nmetastasis &lt;- c(\"yes\", \"no\", \"yes\")\ncases &lt;- c(30, 50, 100)\n# combine vectors\nexample_df1 &lt;- data.frame(cancer, metastasis, cases)\nstr(example_df1)\n\n'data.frame':   3 obs. of  3 variables:\n $ cancer    : chr  \"lung\" \"prostate\" \"breast\"\n $ metastasis: chr  \"yes\" \"no\" \"yes\"\n $ cases     : num  30 50 100\n\n\nThe resulting data frame has column headers, identified from the names of the vectors combined together.\nThe next way seems more complex, but represents the code above combined into one step:\n\n# create vectors and combine into data frame simultaneously\nexample_df2 &lt;- data.frame(cancer = c(\"lung\", \"prostate\", \"breast\"),\n                          metastasis = c(\"yes\", \"no\", \"yes\"),\n                          cases = c(30, 50, 100), stringsAsFactors = FALSE)\nstr(example_df2)\n\n'data.frame':   3 obs. of  3 variables:\n $ cancer    : chr  \"lung\" \"prostate\" \"breast\"\n $ metastasis: chr  \"yes\" \"no\" \"yes\"\n $ cases     : num  30 50 100\n\n\nAs we learned above, factors can be particularly difficult, so it’s useful to know that you can use stringsAsFactors = FALSE to import such data as character instead."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#wrapping-up",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#wrapping-up",
    "title": "Working with data",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this session, we learned to import data into R from a csv file, learned multiple ways to access parts of data frames, and manipulated factors.\nIn the next session, we’ll begin to explore a set of powerful, elegant data manipulation tools for data cleaning, transforming, and summarizing, and we’ll prepare some data to visualize in our final session."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#extra-exercises",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#extra-exercises",
    "title": "Working with data",
    "section": "Extra exercises",
    "text": "Extra exercises\nThe following exercises all use the same clinical data from this class.\n\nChallenge-disease-race\nExtract the last 100 rows for only disease and race and save to an object called disease_race.\n\n\nChallenge-min-max\nCalculate the minimum and maximum for days_to_death.\n\n\nChallenge-factors\nChange all of the factors of race to shorter names for each category, and appropriately indicate missing data."
  },
  {
    "objectID": "resources/computing/computing-cheatsheets.html",
    "href": "resources/computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references."
  },
  {
    "objectID": "resources/tutorials/installing_software.html",
    "href": "resources/tutorials/installing_software.html",
    "title": "Software installation",
    "section": "",
    "text": "Mac\n\nbrew install r\n\nLinux & Windows\n\nGo to https://cran.r-project.org/ and select the appropriate link for your operating system. Install R from the downloaded executable.\n\n\n\nGo to https://www.python.org/downloads/ and follow the instructions to install Python.\n\n\n\nGo to https://github.com/JuliaLang/juliaup and follow the instructions to install Julia."
  },
  {
    "objectID": "resources/tutorials/installing_software.html#r",
    "href": "resources/tutorials/installing_software.html#r",
    "title": "Software installation",
    "section": "",
    "text": "Mac\n\nbrew install r\n\nLinux & Windows\n\nGo to https://cran.r-project.org/ and select the appropriate link for your operating system. Install R from the downloaded executable."
  },
  {
    "objectID": "resources/tutorials/installing_software.html#python",
    "href": "resources/tutorials/installing_software.html#python",
    "title": "Software installation",
    "section": "",
    "text": "Go to https://www.python.org/downloads/ and follow the instructions to install Python."
  },
  {
    "objectID": "resources/tutorials/installing_software.html#julia",
    "href": "resources/tutorials/installing_software.html#julia",
    "title": "Software installation",
    "section": "",
    "text": "Go to https://github.com/JuliaLang/juliaup and follow the instructions to install Julia."
  },
  {
    "objectID": "resources/tutorials/installing_software.html#visual-studio-code",
    "href": "resources/tutorials/installing_software.html#visual-studio-code",
    "title": "Software installation",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\nGo to https://code.visualstudio.com/ and follow the instruction. You can choose the nightly or stable build. I am personally using the nightly build, but would not recommend it if you are not comfortable with debugging random issues that might arise.\nIt is a good idea to install extension from the visual studio marketplace. You can do that by clicking on the extension icon on the left side of the screen. I would recommend the following extensions:\n\nfor Julia: https://www.julia-vscode.org/\nfor Python: follow this https://code.visualstudio.com/docs/python/editing\nfor R : https://code.visualstudio.com/docs/languages/r"
  },
  {
    "objectID": "resources/tutorials/installing_software.html#rstudio",
    "href": "resources/tutorials/installing_software.html#rstudio",
    "title": "Software installation",
    "section": "RStudio",
    "text": "RStudio\nGo to posit.co and download the RStudio IDE. Choose RStudio Desktop free version and make sure the correct operating system was selected. Install RStudio from the downloaded executable. Open RStudio."
  },
  {
    "objectID": "resources/week-01/recap-linear-regression.html",
    "href": "resources/week-01/recap-linear-regression.html",
    "title": "OLS estimator",
    "section": "",
    "text": "This document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "resources/week-01/recap-linear-regression.html#introduction",
    "href": "resources/week-01/recap-linear-regression.html#introduction",
    "title": "OLS estimator",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "resources/week-01/recap-linear-regression.html#matrix-representation-for-the-regression-model",
    "href": "resources/week-01/recap-linear-regression.html#matrix-representation-for-the-regression-model",
    "title": "OLS estimator",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "resources/week-01/recap-linear-regression.html#estimating-the-coefficients",
    "href": "resources/week-01/recap-linear-regression.html#estimating-the-coefficients",
    "title": "OLS estimator",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation 7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "resources/week-01/recap-linear-regression.html#variance-covariance-matrix-of-the-coefficients",
    "href": "resources/week-01/recap-linear-regression.html#variance-covariance-matrix-of-the-coefficients",
    "title": "OLS estimator",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation 9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]\n\n\n\n\n\n\nAttribution\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210."
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 22 October 2023."
  },
  {
    "objectID": "assignments/assignment-03.html#recap-on-global-optimal-bandwidth",
    "href": "assignments/assignment-03.html#recap-on-global-optimal-bandwidth",
    "title": "Assignment 3",
    "section": "Recap on global optimal bandwidth",
    "text": "Recap on global optimal bandwidth\nThe Nadaraya–Watson kernel estimator of \\(m(x) = \\mathbb{E}(Y|X=x)\\)\n\\[ \\widehat{m}(x) = \\frac{\\sum_{i=1}^n K\\left( \\frac{X_i-x}{h} \\right) Y_i}{ \\sum_{i=1}^n K\\left( \\frac{X_i-x}{h} \\right)} \\]\nis a weighted mean of the \\(Y_i\\).\nSimilarly to what we did with KDEs in week 3, we consider \\[MSE\\lbrace \\widehat{m}(x) \\rbrace = \\mathrm{var}\\lbrace \\widehat{m}(x) \\rbrace + \\big[\\mathrm{bias}\\lbrace \\widehat{m}(x) \\rbrace\\big]^2\\] and, dropping the little-o terms, we obtain \\[AMSE\\lbrace \\widehat{m}(x) \\rbrace = \\frac{\\sigma^2(x) \\int \\lbrace K(z) \\rbrace ^2 dz}{f_X(x) n h_n} + \\frac{1}{4} \\lbrace m''(x)\\rbrace ^2 h_n^4 \\left(\\int z^2 K(z) dz\\right)^2.\\]\nNow, a local bandwidth choice can be obtained by optimizing AMSE. Taking derivatives and setting them to zero, we obtain \\[h_{opt}(x) = n^{-1/5} \\left[ \\frac{\\sigma^2(x) \\int [K(z)]^2 dz}{m''(x) f_X(x) \\int z^2 K(z) dz} \\right]^{1/5}.\\]\nThis is somewhat more complicated compared to the KDE case, because we have to estimate\n\nthe marginal density \\(f_X(x)\\),\n\nlet’s say that we already know how to do this, e.g. by KDE even though that requires a choice of yet another bandwidth\n\nthe local variance function \\(\\sigma^2(x)\\), and\nthe second derivative of the regression functions \\(m''(x)\\).\n\nAgain, like in the case of KDEs, the global bandwidth choice can be obtained by integration:\n\ncalculate \\(AMISE(\\widehat{m}) = \\int AMSE \\lbrace \\widehat{m}(x) \\rbrace f_X(x) dx\\), and\nset \\(h_{AMISE} = \\mathrm{arg\\,min}_{h&gt;0} AMISE(\\widehat{m})\\).\n\n\nRule of Thumb Plug-in Algorithm\nReplace the unknown quantities in \\[h_{AMISE} = n^{-1/5} \\bigg[ \\frac{\\int K^2(z) dz \\int \\sigma^2(x) dx}{\\int z^2 K(z) dz \\int \\lbrace m''(x) \\rbrace^2 f_{X}(x) dx} \\bigg]^{1/5}\\]\nby parametric OLS estimators\n\nAssume homoscedasticity and a quartic kernel, then \\[h_{AMISE} = n^{-1/5} \\bigg( \\frac{35 \\sigma^2 \\vert supp(X) \\vert}{\\theta_{22}} \\bigg)^{1/5}, \\quad \\theta_{22}= \\int \\lbrace m''(x) \\rbrace^2 f_{X}(x) dx\\]\nBlock the sample in \\(N\\) blocks and fit, in each block \\(j\\), the model \\[y_i = \\beta_{0j} + \\beta_{1j} x_i + \\beta_{2j} x_i^2 + \\beta_{3j} x_i^3 + \\beta_{4j} x_i^4 + \\epsilon_i\\] to obtain estimate \\(\\hat{m}_j = \\hat{\\beta}_{0j} + \\hat{\\beta}_{1j} x_i + \\hat{\\beta}_{2j} x_i^2 + \\hat{\\beta}_{3j} x_i^3 + \\hat{\\beta}_{4j} x_i^4\\)\nEstimate the unknown quantities by \\[\n\\begin{split}\n\\hat{\\theta}_{22}(N) &= \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^N \\hat{m}_j''(X_i) \\hat{m}_j''(X_i)  \\mathbb{1}_{X_i \\in \\mathcal{X}_j} \\\\\n\\hat{\\sigma}^2(N) &= \\frac{1}{n-5N} \\sum_{i=1}^n \\sum_{j=1}^N \\{Y_i - \\hat{m}_j(X_i) \\}^2 \\mathbb{1}_{X_i \\in \\mathcal{X}_j}\n\\end{split}\n\\]"
  },
  {
    "objectID": "assignments/assignment-03.html#task",
    "href": "assignments/assignment-03.html#task",
    "title": "Assignment 3",
    "section": "Task",
    "text": "Task\nThe goal is to study the (global, asymptotically optimal) bandwidth choice \\(h_{AMISE}\\) on a simulated data example:\n\ncovariate \\(X\\) from beta distribution Beta\\((\\alpha,\\beta)\\)\nresponse values \\(Y = m(X) + \\epsilon\\) where\n\nthe regression function \\(m\\) is given by \\(\\sin\\left(\\left(\\frac{x}{3+0.1}\\right)^{-1}\\right)\\)\n\\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\)\n\nfix \\(\\sigma^2\\) at some visually appealing value (e.g. \\(\\sigma^2=1\\) should be fine)\nexplore how \\(h_{AMISE}\\) behaves depending on the following parameters (in case of a Shiny App, these will be sliders for the user to tweak):\n\nsample size n\nblock size N (see (Ruppert, Sheather, and Wand 1995) for choosing the optimal block size)\ndensity of the covariate: beta parameters \\(\\alpha\\) and \\(\\beta\\)\n\nvisualize (global, asymptotically optimal) bandwidth\n\nalso find a way to visually incorporate the beta density"
  },
  {
    "objectID": "assignments/assignment-03.html#submission",
    "href": "assignments/assignment-03.html#submission",
    "title": "Assignment 3",
    "section": "Submission",
    "text": "Submission\nWrite a Markdown report explaining your findings. Use the report.qmd file to write your report 1. (Alternatively, any other format that produces an html and a pdf file is fine (e.g. Rmarkdown)).\nThe report should be self-contained and include all code necessary to reproduce your results (for help with virtual environments, see this page).\nYou can create a Shiny App (or an interactive Jupyter notebook, Pluto notebook) as it might be easier to play with the data. An interactive report (shiny app, notebook with interactive sliders, …) is not required, but it is a good way to explore the data interactively. The .qmd report is required."
  },
  {
    "objectID": "assignments/assignment-08.html",
    "href": "assignments/assignment-08.html",
    "title": "Assignment 8",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 10 December 2023.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-6-YOUR_GITHUB_USERNAME to get started.\n\n\nTo be posted soon."
  },
  {
    "objectID": "assignments/assignment-07.html",
    "href": "assignments/assignment-07.html",
    "title": "Assignment 7",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 3 December 2023.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-7-YOUR_GITHUB_USERNAME to get started.\n\n\nTo be posted soon."
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 1 October 2023."
  },
  {
    "objectID": "assignments/assignment-01.html#today",
    "href": "assignments/assignment-01.html#today",
    "title": "Assignment 1",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ncreate a github account and familiarise yourself with git and GitHub\naccept the first assignment and submit your work\nmake sure GitHub Classroom works as expected and you can submit your work."
  },
  {
    "objectID": "assignments/assignment-01.html#assignment",
    "href": "assignments/assignment-01.html#assignment",
    "title": "Assignment 1",
    "section": "Assignment",
    "text": "Assignment\nThe goal of this assignment is for you to fit a linear model. For a refresher on linear models, see the ressource for this week. Your task will be to code a function that fits a linear model to a given dataset. You will need to fill in the blacks in the code located in your personal repository ae-1-YOUR_GITHUB_USERNAME.\nHere is how to proceed:\n\nWe expect you to know the basis of progamming in either R, Julia or Python. Please read the introduction to the language of your choice (R, Julia, or Python) if it is not the case.\nRead the README.md file in your assignment repository and follow the instructions there. (duplicated there for your convenience):\nYou have to code a function that performs linear regression on a given dataset. The function should take as input a dataset (a matrix of size n x d where n is the number of observations and d is the number of features) and a vector of size n containing the labels. The function should return the vector of coefficients of the linear regression model.\nYou are not allowed to use any library for linear regression. You can use libraries for basic operations (e.g. matrix multiplication, matrix inversion, etc.). You will have to check the correctness of your code by comparing the results with the ones obtained by traditional libraries (e.g. sklearn in Python, lm in R, GLM.jl in Julia).\nYou will have to save a figure showing the data and add the line \\(X_{BMI}*\\hat{\\beta}_{BMI}\\) as result.png, and modify the README.md file to add a table containing the coefficients of the linear regression model."
  },
  {
    "objectID": "projects/project-02.html",
    "href": "projects/project-02.html",
    "title": "Main project",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 24 December 2023.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the project, and either join an existing team or create a new one. Once this is done, go to the course GitHub organization and locate the repo titled main-project-TEAM-NAME to get started.\n\n\nThe goal of this project is quite broad, students are free to come up with their own ideas. While simulation studies are the designated topic, groups that found interesting data during the small project and would like to carry on analyzing it, or groups interested in studying a bit deeper one of the methodological concepts from this course are encouraged to approach the teachers during the exercises and discuss their ideas. Prospective topics for the final project will be gradually revealed during the lectures.\nA part of the grade for the final project (10 % of the total grade, i.e. one quarter of the final project) will be awarded for value added (original data analysis, simulation study answering a previously unclear question, etc.). All of the prospective topics that will be introduced during the lecture will have this element, and by half-way through the semester (when the final project will start) it should be clear through the examples what the project should aspire to. We will also discuss this in person at some point, likely on Week 7. The remaining three quarters of the project (i.e. 30 % of the total grade) will be awarded for\n\nquality of the Markdown report\n\nclarity, readability, structure, referencing, etc.\n\ngraphical considerations\n\nwell chosen (as discussed during the course) graphics with captions, referenced from the main text\n\nconcepts explored beyond the scope of the course\n\nin the soft sense that they were not fully covered during classes\n\noverall quality\n\ncorrectness, demonstration of understanding, etc.\n\n\nA project seriously lacking in any of the criteria above will be penalized."
  },
  {
    "objectID": "projects/project-tips-resources.html",
    "href": "projects/project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "Consistency is the most important thing for a statistician.\n\n\njumping between different citation styles is bad\nhaving some captions centered above the figure produced by R and others flushed right below using Markdown is annoying\n\n\nSave space for better readability.\n\n\nplots that convey little information don’t have to be large, several (related) plots can be put next to each other on the same line, etc.\neven in an html file, unnecessary scrolling back and forth when reading a report is annoying\na plot frame with one or two boxplots is a waste of space (e.g. histograms would be better)\nbarplots can often be replaced by tables to both save space and improve readability\nin general, if a plot only shows like 3 numbers and is not important for any argument made, it should not be a plot\n\n\nStory-telling matters.\n\n\nit is important to grasp attention with an introduction (and describe at the same time what to expect from a report)\nre-iterate the most important ideas/results in several places\ncomment on plots even if they are self-explanatory\n\n\nMore on plots.\n\n\ntext in figures (labels, etc.) should be of similar size as the main text\nlabels have to be readable, e.g. no overlaying etc.\ncaptions are necessary and should make the plot self-contained (without looking at the paragraphs around it)\n\n\nLess is sometimes more.\n\n\nshowing a scatterplot only to colors to it based on a group of two on the next plot, then adding one regression line, and then two regression lines on the fourth plot… just dilutes the important parts\nBarplots colored by an additional variable should probably be replaced by a table (potentially colored by the cell values – coloring for better readability only).\n\n\nList of references should be itemized or enumerated (in order to be readable).\nAvoid using local paths.\n\n\nreproducibility of the report itself!\n\n\nTransforming variables.\n\n\nif your plots look bad because of a clear skewing in one of the variables, transform the varible (typically plot it on a log-scale)\nif plotting on a log-scale, consider log10 or log2 to have better interpretability"
  },
  {
    "objectID": "projects/project-tips-resources.html#tips-and-tricks-for-visualisation",
    "href": "projects/project-tips-resources.html#tips-and-tricks-for-visualisation",
    "title": "Project tips + resources",
    "section": "",
    "text": "Consistency is the most important thing for a statistician.\n\n\njumping between different citation styles is bad\nhaving some captions centered above the figure produced by R and others flushed right below using Markdown is annoying\n\n\nSave space for better readability.\n\n\nplots that convey little information don’t have to be large, several (related) plots can be put next to each other on the same line, etc.\neven in an html file, unnecessary scrolling back and forth when reading a report is annoying\na plot frame with one or two boxplots is a waste of space (e.g. histograms would be better)\nbarplots can often be replaced by tables to both save space and improve readability\nin general, if a plot only shows like 3 numbers and is not important for any argument made, it should not be a plot\n\n\nStory-telling matters.\n\n\nit is important to grasp attention with an introduction (and describe at the same time what to expect from a report)\nre-iterate the most important ideas/results in several places\ncomment on plots even if they are self-explanatory\n\n\nMore on plots.\n\n\ntext in figures (labels, etc.) should be of similar size as the main text\nlabels have to be readable, e.g. no overlaying etc.\ncaptions are necessary and should make the plot self-contained (without looking at the paragraphs around it)\n\n\nLess is sometimes more.\n\n\nshowing a scatterplot only to colors to it based on a group of two on the next plot, then adding one regression line, and then two regression lines on the fourth plot… just dilutes the important parts\nBarplots colored by an additional variable should probably be replaced by a table (potentially colored by the cell values – coloring for better readability only).\n\n\nList of references should be itemized or enumerated (in order to be readable).\nAvoid using local paths.\n\n\nreproducibility of the report itself!\n\n\nTransforming variables.\n\n\nif your plots look bad because of a clear skewing in one of the variables, transform the varible (typically plot it on a log-scale)\nif plotting on a log-scale, consider log10 or log2 to have better interpretability"
  },
  {
    "objectID": "projects/project-tips-resources.html#some-links-to-open-data",
    "href": "projects/project-tips-resources.html#some-links-to-open-data",
    "title": "Project tips + resources",
    "section": "Some Links to Open Data",
    "text": "Some Links to Open Data\nfivethirtyeight: article data of Nate Silver’s data journalism platform freely available (see also R package fivethirtyeight)\ndata-is-plural: weekly newsletter of datasets by Jeremy Singer-Vine\nre3data: Registry of research data repositories\nopenml datasets: many uniformly formatted datasets for training machine learning models – however, not always good descriptions available\nWorldbank Datacatalog: the World Bank data catalogue\nUK Data Service: UK’s largest collection of social, economic and population data resources (filter for open data) or also data.gov.uk\nICPSR: unit within the Institute for Social Research at the University of Michigan, social and behavioral research. In particular including replication datasets for published studies.\ngovdata: Open Government - German administrative data freely accessible\ngapminder: “an independent educational non-proﬁt ﬁghting global misconceptions”; collection and vizualisation of datasets concerning gobal developement\nnature.com: peer-reviewed, open-access journal for descriptions of datasets (broad range of natural science disciplines)\nNIH (National Institute of Health) Data Sharing Repositories: overview on different thematically sorted medical databases\nUCI Machine Learning Repository or the new beta version: containing various datasets – however, sometimes with a little few description\ndata.bris Research Data Repository: Data repository of the University of Bristol\n… no systematic selection. Much more out there"
  },
  {
    "objectID": "exercises/exercise-02.html",
    "href": "exercises/exercise-02.html",
    "title": "Exercise 2",
    "section": "",
    "text": "Consider bivariate kernel density estimation. Simulate data from the bivariate normal distribution \\(\\mathcal{N}((0,0), (1, 0.3; 0.3, 1))\\).\n\nTry different bandwidths and different kernels (use the bivariate normal kernel as well as the product kernel with the univariate Epanechnikov kernel)\nFind ways to visually compare your estimates with the real density (3d plots of the density or density contour plots)\nR functions such as kde2d of package MASS, contour and persp might be helpful"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "The course will provide the opportunity to tackle real world problems requiring advanced computational skills and visualisation techniques to complement statistical thinking. Students will practice proposing efficient solutions, and effectively communicating the results with stakeholders."
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "The course will provide the opportunity to tackle real world problems requiring advanced computational skills and visualisation techniques to complement statistical thinking. Students will practice proposing efficient solutions, and effectively communicating the results with stakeholders."
  },
  {
    "objectID": "course-syllabus.html#content",
    "href": "course-syllabus.html#content",
    "title": "Syllabus",
    "section": "Content",
    "text": "Content\n\nModern statistical computing environments (e.g., R, Julia, and Python)\nAids to efficiency and reproducibility (e.g., GitHub, Markdown)\nData management, wrangling, and ethics\nStatistical graphics (grammar, good practices, applications, and examples)\nKernel density estimation and smoothing\nCross-validation\nEM algorithm and applications\nResampling methods for uncertainty assessment (bootstrap, jackknife, cross-validation), with applications to regression, time series and dependent data\nMarkov chain Monte Carlo techniques (Gibbs sampler, Metropolis-Hastings algorithm, Hamiltonian Monte Carlo, convergence diagnostics) and software (e.g., Stan)\nOther methods for Bayesian inference (e.g., importance sampling)\nDecision trees for classification"
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nRequired courses: Probability and statistics, Linear models"
  },
  {
    "objectID": "course-syllabus.html#learning-outcomes",
    "href": "course-syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the course, the student must be able to:\n\nPlan complex visualisation and computational tasks\nPerform complex visualisation and computational tasks\nImplement reproducible computational solutions to statistical problems in modern environments and platforms"
  },
  {
    "objectID": "course-syllabus.html#transversal-skills",
    "href": "course-syllabus.html#transversal-skills",
    "title": "Syllabus",
    "section": "Transversal skills",
    "text": "Transversal skills\n\nTake feedback (critique) and respond in an appropriate manner.\nCommunicate effectively with professionals from other disciplines.\nDemonstrate the capacity for critical thinking\nIdentify the different roles that are involved in well-functioning teams and assume different roles, including leadership roles."
  },
  {
    "objectID": "exercises/exercise-01.html",
    "href": "exercises/exercise-01.html",
    "title": "Exercise 1",
    "section": "",
    "text": "The severity depends on the number of peeks the researcher takes at the data and on the number of observations added between peeks.\nExercise: The “Example of Peeking” below is an example of a small simulation study, checking whether a designed test strategy respects the nominal level \\(\\alpha = 0.05\\) or not. Incorporate further levels of peeking and see how it affects the nominal significance level.\n\npeeking &lt;- function(a,b=10){\n  x &lt;- rnorm(25)\n  Tstat &lt;- mean(x)/sd(x)*sqrt(length(x))\n  if(abs(Tstat) &gt; qt(0.975,length(x)-1)){\n    return(Tstat)\n  }else{\n    x &lt;- append(x, rnorm(b))\n    Tstat &lt;- mean(x)/sd(x)*sqrt(length(x))\n    return(Tstat)\n  }\n}\nset.seed(517)\nTstats &lt;- sapply(1:10000,peeking)\nmean(I(abs(Tstats) &gt; qnorm(0.975)))\n\n[1] 0.0851"
  },
  {
    "objectID": "projects/project-01.html",
    "href": "projects/project-01.html",
    "title": "Small project",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 22 October 2023.\nThe goal of this project is data exploration. Find an interesting (in the sense it interests you!) data set and\nNote that the purpose of this project is to play around, demonstrating your data exploration, wrangling and visualization skills. Hopefully, you will also find scientifically interesting questions or questions of personal interest (e.g. does ball possession matter in a game of football?), but even if not, you can still be awarded full points, as long as you avoid Kaggle data sets that have been analyzed zillion times before.\nThe first step should be done individually. Here is an example of what your report could look like. See the this page for some tips and resources (e.g. example datasets)."
  },
  {
    "objectID": "projects/project-01.html#task",
    "href": "projects/project-01.html#task",
    "title": "Small project",
    "section": "Task",
    "text": "Task\nWrite your report in the report.qmd file, and compile it. See the Assignment.md file in your github repository for details."
  },
  {
    "objectID": "assignments/assignment-06.html",
    "href": "assignments/assignment-06.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 19 November 2023.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-6-YOUR_GITHUB_USERNAME to get started.\n\n\nTo be posted soon."
  },
  {
    "objectID": "assignments/assignment-05.html",
    "href": "assignments/assignment-05.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 5 November 2023.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-5-YOUR_GITHUB_USERNAME to get started.\n\n\n\n1 Task\nSimulate data from the mixture of two Gaussian distributions (see Section 3 in Assignment 2) and implement the EM algorithm from Example 2 in the course (also repeated in Section 2) . Use absolute change in \\(\\ell_{obs}\\) of observed data as a convergence criterion.\nThe following should naturally be done:\n\nVisualize the resulting parametric density estimators\nTry running your algorithm from different starting points:\n\nHow sensitive is your algorithm to your choice of starting point?\nCan you find a bad starting point where your algorithm fails?\n\n\n\n\n2 Mixture distributions\nMixture of two Gaussian distributions:\nLet \\(X^{(1)}, \\dots, X^{(N)}\\) be i.i.d. random variables each with pdf \\[ f_{\\boldsymbol{\\theta}}(x) = (1-\\tau) \\ \\varphi_{\\mu_1, \\sigma_1}\\left(x \\right) + \\tau \\ \\varphi_{\\mu_2, \\sigma_2}\\left(x \\right) \\] where \\(\\boldsymbol{\\theta} = (\\tau, \\mu_1,\\mu_2,\\sigma_1^2, \\sigma_2^2)^\\top\\), with\n\n\\(\\varphi_{\\mu, \\sigma}\\) is the pdf of a Gaussian with mean \\(\\mu\\) and standard deviation \\(\\sigma\\),\n\\(\\mu_1, \\mu_2\\) and \\(\\sigma_1^2, \\sigma_2^2\\) are the means and variances of the mixture components, and\n\\(\\tau \\in (0,1)\\) is the mixing proportion\n\n\\[\n  \\ell_{obs}(\\boldsymbol{\\theta}) = \\sum_{n=1}^N \\log \\left\\lbrace (1-\\tau)\\, \\varphi_{\\mu_1, \\sigma_1}\\left(X_n\\right) {\\boldsymbol{+}} \\tau\\, \\varphi_{\\mu_2, \\sigma_2}\\left(X_n\\right) \\right\\rbrace\n\\] Trick: add latent i.i.d. indicators \\(Z^{(n)} \\sim \\operatorname{Bernoulli}(\\tau)\\) such that \\(X^{(n)} \\mid Z^{(n)} = 0 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X^{(n)} \\mid Z^{(n)} = 1 \\sim N(\\mu_2, \\sigma_2^2)\\).\nGiven \\(Z^{(n)} = z^{(n)}\\), \\(n=1, \\dots, N\\), the joint likelihood can be written as \\[\n  L_{comp}(\\boldsymbol{\\theta}) = (1-\\tau)^{N_1} \\tau^{N_2} \\prod_{n=1}^{N} \\varphi_{\\mu_1, \\sigma_1}\\left( X^{(n)}\\right)^{(1-Z^{(n)})}  \\varphi_{\\mu_2, \\sigma_2}\\left( X^{(n)}\\right)^{Z^{(n)}}\n\\] with \\(N_2 = \\sum_{n=1}^{N} Z^{(n)}\\) and \\(N_1 = N - N_2\\).\n\nE-step: calculate \\(\\mathbb{E}_{\\widehat{\\theta}^{(l-1)}}\\big[\\ell_{comp}(\\theta) \\big| \\mathbf{X} = \\mathbf{x} \\big] =: Q\\big(\\theta,\\widehat{\\theta}^{(l-1)}\\big)\\)\n\\[\\begin{align*}\n\\ell_{comp}(\\boldsymbol{\\theta}) &= \\ln L_{comp}(\\boldsymbol{\\theta}) =\nN_1 \\ln(1 - \\tau) + N_2 \\ln(\\tau) +\\\\\n&+ \\sum_{n=1}^{N} (1-Z^{(n)}) \\ln\\varphi_{\\mu_1, \\sigma_1}\\left( X^{(n)} \\right) + \\sum_{n=1}^{N} Z^{(n)} \\ln\\varphi_{\\mu_2, \\sigma_2}\\left( X^{(n)} \\right)\n\\end{align*}\\] such that, we obtain \\[\\begin{align*}\n\\mathbb{E}_{\\widehat{\\boldsymbol{\\theta}}^{(l-1)}}&\\big[\\ell_{comp}(\\boldsymbol{\\theta}) \\big| \\mathbf{X} = \\mathbf{x} \\big] =\n    \\log(1-\\tau) (N - \\sum_{n=1}^{N} p^{(l-1)}_n) +  \n    \\log(\\tau) \\sum_{n=1}^{N} p^{(l-1)}_n +\\\\\n    &+ \\sum_{n=1}^{N} (1-p^{(l-1)}_n) \\log\\varphi_{\\mu_1, \\sigma_1}\\left( x^{(n)} \\right) +\n    \\sum_{n=1}^{N} p^{(l-1)}_n \\log\\varphi_{\\mu_2, \\sigma_2}\\left(x^{(n)}\\right)\n\\end{align*}\\] with \\(p^{(l-1)}_n = \\mathbb{E}_{\\widehat{\\theta}^{(l-1)}}\\big[ Z^{(n)} \\big| X^{(n)} = x^{(n)} \\big] \\overset{Bayes}{=} \\frac{\\varphi_{\\hat{\\mu}_2^{(l-1)}, \\hat{\\sigma}_2^{(l-1)}}\\left( x^{(n)}\\right) \\hat{\\tau}^{(l-1)}}{f_{\\hat{\\boldsymbol{\\theta}}^{(l-1)}}(x^{(n)})}.\\)\nM-step: optimize \\(\\mathrm{arg\\,max}_{\\theta}\\; Q\\big(\\theta,\\widehat{\\theta}^{(l-1)}\\big)\\)\nHence, \\(Q\\big(\\boldsymbol{\\theta},\\widehat{\\boldsymbol{\\theta}}^{(l-1)}\\big)\\) nicely splits into three parts\n\\[\\begin{align*}\nQ\\big(&\\boldsymbol{\\theta},\\widehat{\\boldsymbol{\\theta}}^{(l-1)}\\big) =\\\\\n    &\\mathbf{A:}\\quad \\log(1-\\tau) (N - \\sum_{n=1}^{N} p^{(l-1)}_n) +  \n    \\log(\\tau) \\sum_{n=1}^{N} p^{(l-1)}_n +\\\\\n    &\\mathbf{B:}\\quad + \\sum_{n=1}^{N} (1-p^{(l-1)}_n) \\log\\varphi_{\\mu_1, \\sigma_1}\\left( x^{(n)} \\right) +\\\\\n    &\\mathbf{C:}\\quad + \\sum_{n=1}^{N} p^{(l-1)}_n \\log\\varphi_{\\mu_2, \\sigma_2}\\left( x^{(n)} \\right)\n\\end{align*}\\] which can be optimized separately."
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 15 October 2023."
  },
  {
    "objectID": "assignments/assignment-02.html#recap-kernel-density-estimation",
    "href": "assignments/assignment-02.html#recap-kernel-density-estimation",
    "title": "Assignment 2",
    "section": "1 Recap: Kernel Density Estimation",
    "text": "1 Recap: Kernel Density Estimation\nRemember that the Kernel Density Estimation (KDE) of \\(f\\) based on \\(X_1,\\ldots,X_N\\) is \\[\\widehat{f}(x) = \\frac{1}{n h_n} \\sum_{i=1}^n K\\left(\\frac{X_i - x}{h_n} \\right),\\] where the \\(K(\\cdot)\\) satisfies:\n\n\n\n\\(K(x) \\geq 0\\) for all \\(x \\in \\mathbb{R}\\)\n\\(K(- x) = K(x)\\) for all \\(x \\in \\mathbb{R}\\)\n\\(\\int_\\mathbb{R} K(x) d x = 1\\)\n\n\n\n\n\n\\(\\lim_{|x| \\to \\infty} |x| K(x) = 0\\)\n\\(\\sup_x |K(x)| &lt; \\infty.\\)\n\n\n\n\n\nCode\nplot_kdes &lt;- function(bw){\n  plot(density(faithful$eruptions, kernel=\"gaussian\", bw=bw),\n       main=paste(\"bandwidth = \",bw,sep=\"\"), xlab=\"time [min]\")\n  lines(density(faithful$eruptions, kernel=\"epanechnikov\", bw=bw), col=4)\n  lines(density(faithful$eruptions, kernel=\"rectangular\", bw=bw), col=2)\n}\npar(mfrow=c(1,4), mar = c(3.2, 3, 1.6, 0.2))\nplot_kdes(0.75); plot_kdes(0.5); plot_kdes(0.25); plot_kdes(0.1)\n\n\n\n\n\nImpact of the bandwidth on KDE, where the Gaussian kernel is in black, the Epanechnikov kernel in blue and the rectangular kernel in red.\n\n\n\n\nwhere the kernels are defined as:\n\n\n\n\n\n\n\nKernel Name\nFormula\n\n\n\n\nEpanechnikov\n\\(K(x) \\propto (1-x^2) \\mathbb{1}_{[|x| \\leq 1]}\\)\n\n\nGaussian\n\\(K(x) \\propto \\exp(-x^2/2)\\)\n\n\nRectangular\n\\(K(x)=\\frac{1}{2} 1_{\\{-1&lt;x&lt;1\\}}\\)"
  },
  {
    "objectID": "assignments/assignment-02.html#task",
    "href": "assignments/assignment-02.html#task",
    "title": "Assignment 2",
    "section": "2 Task",
    "text": "2 Task\nWe saw in the lecture that the choice of the bandwidth \\(h\\) is crucial for the performance of the KDE. In this assignment we will explore this in more detail with a simulation study. Specifically:\n\nrepeat the following \\(200\\) times:\n\ngenerate \\(N=100\\) samples from the Gaussian mixture (see Section 3) 1\nperform density estimation, i.e., obtain \\(\\widehat{f}\\), for\n\nGaussian, Epanechnikov, and rectangular kernels\nbandwidth values \\(h = 0.1,0.15,0.2,0.25,\\ldots,0.9\\)\n\ncalculate the error measure \\(\\| f - \\widehat{f} \\|_2\\)\n\nreport your findings as a single (well commented) figure 2"
  },
  {
    "objectID": "assignments/assignment-02.html#sec-GaussianMixture",
    "href": "assignments/assignment-02.html#sec-GaussianMixture",
    "title": "Assignment 2",
    "section": "3 Gaussian mixture",
    "text": "3 Gaussian mixture\nLet \\(X^{(1)}, \\dots, X^{(N)}\\) be i.i.d. random variables each with pdf \\[ f_{\\boldsymbol{\\theta}}(x) = (1-\\tau) \\ \\varphi_{\\mu_1, \\sigma_1}\\left(x \\right) + \\tau \\ \\varphi_{\\mu_2, \\sigma_2}\\left(x \\right) \\] where \\(\\boldsymbol{\\theta} = (\\tau, \\mu_1,\\mu_2,\\sigma_1^2, \\sigma_2^2)^\\top\\), with\n\n\\(\\varphi_{\\mu, \\sigma}\\) is the pdf of a Gaussian with mean \\(\\mu\\) and standard deviation \\(\\sigma\\),\n\\(\\mu_1, \\mu_2\\) and \\(\\sigma_1^2, \\sigma_2^2\\) are the means and variances of the mixture components, and\n\\(\\tau \\in (0,1)\\) is the mixing proportion\n\n\\[\n  \\ell_{obs}(\\boldsymbol{\\theta}) = \\sum_{n=1}^N \\log \\left\\lbrace (1-\\tau)\\, \\varphi_{\\mu_1, \\sigma_1}\\left(X_n\\right) {\\boldsymbol{+}} \\tau\\, \\varphi_{\\mu_2, \\sigma_2}\\left(X_n\\right) \\right\\rbrace\n\\] Trick: add latent i.i.d. indicators \\(Z^{(n)} \\sim \\operatorname{Bernoulli}(\\tau)\\) such that \\(X^{(n)} \\mid Z^{(n)} = 0 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X^{(n)} \\mid Z^{(n)} = 1 \\sim N(\\mu_2, \\sigma_2^2)\\).\nGiven \\(Z^{(n)} = z^{(n)}\\), \\(n=1, \\dots, N\\), the joint likelihood can be written as \\[\n  L_{comp}(\\boldsymbol{\\theta}) = (1-\\tau)^{N_1} \\tau^{N_2} \\prod_{n=1}^{N} \\varphi_{\\mu_1, \\sigma_1}\\left( X^{(n)}\\right)^{(1-Z^{(n)})}  \\varphi_{\\mu_2, \\sigma_2}\\left( X^{(n)}\\right)^{Z^{(n)}}\n\\] with \\(N_2 = \\sum_{n=1}^{N} Z^{(n)}\\) and \\(N_1 = N - N_2\\).\nThe two functions below allow for random number generation and density evaluation for the Gaussian mixture distribution\n\n\nCode\n# random generation for a mixture of two normal distributions\nrmixnorm &lt;- function(N, mu1, mu2, sigma1, sigma2, tau){\n  ind &lt;- I(runif(N) &gt; tau)\n  X &lt;- rep(0,N)\n  X[ind] &lt;- rnorm(sum(ind), mu1, sigma1)\n  X[!ind] &lt;- rnorm(sum(!ind), mu2, sigma2)\n  return(X)\n}\n\n# density evaluation for a mixture of two normal distributions\ndmixnorm &lt;- function(x, mu1, mu2, sigma1, sigma2, tau){\n  y &lt;- (1-tau)*dnorm(x,mu1,sigma1) + tau*dnorm(x,mu2,sigma2)\n  return(y)\n}\n\n\nA sample call is below.\n\n\nCode\nlibrary(ggplot2)\n\nN &lt;- 300\nmu1 &lt;- 3\nmu2 &lt;- 0\nsigma1 &lt;- 0.5\nsigma2 &lt;- 1\ntau &lt;- 0.6\n\nX &lt;- rmixnorm(N, mu1, mu2, sigma1, sigma2, tau)\nx &lt;- seq(-3, 6, by = 0.01)\nfx &lt;- dmixnorm(x, mu1, mu2, sigma1, sigma2, tau)\n\nggplot() +\n    theme_bw() +\n    aes(X, after_stat(density)) +\n    geom_histogram(colour = \"#999999\", fill = \"#999999\", binwidth = 0.1) + \n    geom_line(aes(x, fx), colour = \"#E69F00\", linewidth = 1) + \n    labs(\n    title = \"Gaussian mixture density and histogram of 300 samples\",\n    subtitle = \"generated from a Gaussian mixture with tau = 0.6\")    \n\n\n\n\n\nSimilar functions will be provided for Python and Julia in the assignment repository."
  },
  {
    "objectID": "assignments/assignment-04.html",
    "href": "assignments/assignment-04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 29 October 2023.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse the corresponding invite link in this google doc (accessible with your EPFL account) to accept the assignment. Then go to the course GitHub organization and locate the repo titled ae-4-YOUR_GITHUB_USERNAME to get started.\n\n\nConsider a subset of mcycle data (of the MASS package)1 for times \\(\\leq 40\\) and use cross-validation to select\n\nthe polynomial degree of \\(\\textcolor{red}{p}\\) from candidate values \\(\\{1,2,3\\}\\)\nthe bandwidth \\(\\textcolor{red}{h}\\) from candidate values \\(\\{3,4,\\ldots,15\\}\\)\n\nfor a local polynomial smoother2 of the form\n\\[ \\underset{\\boldsymbol{\\beta} \\in \\mathbb{R}^{\\textcolor{red}{p}+1}}{\\operatorname{argmin}} \\sum_{i=1}^n [Y_i - \\sum_{j=0}^\\textcolor{red}{p} \\beta_j(x)\\left(x_i-x\\right)^j]^2 K\\left(\\frac{X_i-x}{\\textcolor{red}{h}}\\right),\\]\nwhere \\(K(x)=\\frac{3}{4}(1 - x^2)\\) if \\(|x|\\leq 1\\) and \\(0\\) otherwise is the Epanechnikov kernel.\n\n\n\n\n\n\n\n\n\nNotes:\n\nUse your own visualizations to verify your progress.\nYou may run into issues for large \\(p\\) and small \\(h\\) if you use a small number of folds.\nBeware of the time-dependent aspect of your data.\n\n\n\n\n\nFootnotes\n\n\navailable on Kaggle, through the R library MASS or through RDataset.jl↩︎\nfor R you can use locpol, for Python localreg, and for Julia you can try KernSmooth or just call locpol via RCall.↩︎"
  },
  {
    "objectID": "resources/week-02/mini-project-example.html",
    "href": "resources/week-02/mini-project-example.html",
    "title": "EDA example",
    "section": "",
    "text": "Data and Problem Description\nInsurance redlining refers to the practice of refusing to issue insurance to certain types of people or within some geographic area. The name comes from the act of drawing a red line around an area on a map. Now few would quibble with an insurance company refusing to sell a car insurance to a frequent drunk driver, but other forms of discrimination would be unacceptable.\nIn the late 1970s, the US Commission on Civil Rights examined charges by several Chicago community organizations that insurance companies were redlining their neighborhoods. Because comprehensive information about individuals homeowners was not available, the number of FAIR plan policies written and renewed in Chicago by ZIP code for the months of December 1977 through May 1978 was recorded. The FAIR plan was offered by the city of Chicago as a default policy to homeowners who had been rejected by the voluntary market. Information on other variables that might affect insurance writing such as fire and theft rates was also collected at the ZIP code level. The variables are:\n\ninvolact new FAIR plan policies and renewals per 100 housing units,\nfire fires per 100 housing units,\ntheft thefts per 1000 population,\nrace racial composition in percentage of minority,\nage (of housing) percentage of housing units built before 1939,\nincome median family income in thousands of dollars,\nside north or south side of Chicago.\n\nThe variable involact acts as a measure of insurance availability in the voluntary market, since most FAIR plan policyholders secure such coverage only after they have been rejected by the voluntary market. Insurance companies claim to reject insurances based on their pass losses (captured in variables theft and fire). The U.S. Commission on Civil Rights in 1979 was interested in how much income, age of housing, and in particular race affect insurance availability.\n\nlibrary(faraway)\ndata(chredlin, package=\"faraway\") # attaches the data from the faraway package\nstr(chredlin) # the name \"chredlin\" refers to CHicago REDLINing\n\n'data.frame':   47 obs. of  7 variables:\n $ race    : num  10 22.2 19.6 17.3 24.5 54 4.9 7.1 5.3 21.5 ...\n $ fire    : num  6.2 9.5 10.5 7.7 8.6 34.1 11 6.9 7.3 15.1 ...\n $ theft   : num  29 44 36 37 53 68 75 18 31 25 ...\n $ age     : num  60.4 76.5 73.5 66.9 81.4 52.6 42.6 78.5 90.1 89.8 ...\n $ involact: num  0 0.1 1.2 0.5 0.7 0.3 0 0 0.4 1.1 ...\n $ income  : num  11.74 9.32 9.95 10.66 9.73 ...\n $ side    : Factor w/ 2 levels \"n\",\"s\": 1 1 1 1 1 1 1 1 1 1 ...\n\nhead(chredlin)\n\n      race fire theft  age involact income side\n60626 10.0  6.2    29 60.4      0.0 11.744    n\n60640 22.2  9.5    44 76.5      0.1  9.323    n\n60613 19.6 10.5    36 73.5      1.2  9.948    n\n60657 17.3  7.7    37 66.9      0.5 10.656    n\n60614 24.5  8.6    53 81.4      0.7  9.730    n\n60610 54.0 34.1    68 52.6      0.3  8.231    n\n\n\nThe first column is ZIP codes, since we only aggregated data for counties (not for individuals). This is not unreasonable here, due to the nature of redlining.\n\n\nData Exploration\nLet’s take a look at the individual variables, e.g., using histograms\n\nlibrary(tidyverse)\nchredlin %&gt;% mutate(side = as.numeric(side)) %&gt;% pivot_longer(everything()) %&gt;%\n  ggplot(aes(value)) + facet_wrap(~ name, scales = \"free\") + geom_histogram()\n\n\n\n\nThe univariate distributions look quite nice for most of the variables. For example, we have a good spread in race, so we should be capable to asses its effect quite accurately. There are more features to be noticed:\n\ninvolact has some concentration at 0, casting doubts on the usage of a linear model,\nmany variables seem to be skewed and transformations should be explored,\n\ne.g. using log-transform for income and fire seems like a no-brainer, also due to interpretation\n\nthere is an outlier visible at the right tail of theft, and other potential outliers.\n\nWe will not act on these observations at this point, with the exception of taking log(income).\nNow let us explore relationships between the individual predictors and the response. The following plots show the simple regression lines with 95 % confidence bands (for the regression line). Jittering has been added to the variable side to avoid overplotting.\n\np1 &lt;- ggplot(chredlin,aes(race,involact)) + geom_point() +stat_smooth(method=\"lm\")\np2 &lt;- ggplot(chredlin,aes(fire,involact)) + geom_point() +stat_smooth(method=\"lm\")\np3 &lt;- ggplot(chredlin,aes(theft,involact)) + geom_point() +stat_smooth(method=\"lm\")\np4 &lt;- ggplot(chredlin,aes(age,involact)) + geom_point() +stat_smooth(method=\"lm\")\np5 &lt;- ggplot(chredlin,aes(income,involact)) + geom_point() +stat_smooth(method=\"lm\")\np6 &lt;- ggplot(chredlin,aes(side,involact)) + geom_point(position = position_jitter(width = .2,height=0))\n\nlibrary(ggpubr)\nggarrange(p1,p2,p3,p4,p5,p6)\n\n\n\n\nWithout a doubt, we see that race has a strong correlation with the response variable, potentially suggesting the bad practice of insurance redlining is based on race. However, we also may be observing confounding: can insurance companies claim that this is due to correlation between risks (fire and thift) and race?\n\np1 &lt;- ggplot(chredlin,aes(race,fire)) + geom_point() +stat_smooth(method=\"lm\")\np2 &lt;- ggplot(chredlin,aes(race,theft)) + geom_point() +stat_smooth(method=\"lm\")\nggarrange(p1,p2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThere seems to be a positive relationship between race (percentage of minorities) and the variables fire and theft."
  },
  {
    "objectID": "resources/tutorials/github.html",
    "href": "resources/tutorials/github.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "Important\n\n\n\nPlease read the full guide if something does not work: it is very likely that the answer to your problem lies just a few lines after wherever you stopped reading.\n\n\nFirst follow this set up Git tutorial1 and then follow this tutorial to get a gentle introduction to Git and GitHub.\nSummary of the steps\n\nCreate a GitHub account\nInstall Git\nCreate a repository and clone it to your computer\nCreate a file and push it to GitHub\n\nIf all of the above worked, you are ready to go.\nNext steps\nYou should follow the instructions you received by email to join the GitHub organization of your course. This will allow you to access the course repository and to submit your assignments.\n\n\n\n\nFootnotes\n\n\nIf you need more help installing and using git, see this tutorial.↩︎"
  },
  {
    "objectID": "resources/tutorials/github_classroom.html",
    "href": "resources/tutorials/github_classroom.html",
    "title": "Github Classroom",
    "section": "",
    "text": "We will use Github Classroom for the assignments in this course. This document will give you a brief introduction to Github Classroom and how to use it."
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#introduction",
    "href": "resources/tutorials/github_classroom.html#introduction",
    "title": "Github Classroom",
    "section": "Introduction",
    "text": "Introduction\nThis guide is here to help you get started. GitHub Classroom is a platform that we will use to manage and distribute assignments, making it easier for you to collaborate on coursework."
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#what-is-github-classroom",
    "href": "resources/tutorials/github_classroom.html#what-is-github-classroom",
    "title": "Github Classroom",
    "section": "What is GitHub Classroom?",
    "text": "What is GitHub Classroom?\nGitHub Classroom is a tool that simplifies the process of creating, distributing, and submitting assignments on GitHub. It leverages the power of Git, a version control system, and GitHub, a web-based platform for code hosting and collaboration. With GitHub Classroom, you’ll be able to access your assignments, work on them, and submit your work – all in one place."
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#prerequisites",
    "href": "resources/tutorials/github_classroom.html#prerequisites",
    "title": "Github Classroom",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore we dive into using GitHub Classroom, make sure you have the following:\n\nGitHub Account: If you don’t already have a GitHub account, you can sign up for free at GitHub. This account will be essential for participating in assignments.\nGit Installed: Git is a tool that helps you manage your code changes. You can download and install Git from the official website: Git Downloads. See this tutorial if you need help installing Git."
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#getting-started",
    "href": "resources/tutorials/github_classroom.html#getting-started",
    "title": "Github Classroom",
    "section": "Getting Started",
    "text": "Getting Started\nLet’s begin by taking the first steps with GitHub Classroom:\n\n1. Accept the Classroom Invitation\nYour instructor will provide you with a link to the GitHub Classroom assignment. Click on the link, and it will guide you to accept the invitation. This action will create a private repository for your assignment within your GitHub account.\n\n\n2. Clone the Repository\nNext, you’ll want to bring the assignment to your computer so you can work on it. To do this, you need to clone the assignment repository. Don’t worry; cloning simply means making a copy of the assignment on your computer.\n\nOpen a terminal or Git Bash (if you installed Git).\nNavigate to the directory where you want to store your assignment (e.g., Documents).\nUse the following command, replacing &lt;repository_url&gt; with the URL of your assignment repository (you can find it on the GitHub page of your assignment):\n\ngit clone &lt;repository_url&gt;\n\n\n3. Work on the Assignment\nNow that you have the assignment on your computer, explore the repository. Inside, you’ll find instructions and possibly some starter code or files. These will guide you on what to do. You can use any code editor or development environment you’re comfortable with.\n\n\n4. Save Your Work\nWhile working on your assignment, be sure to save your progress regularly. In the Git world, this is called “committing.” It helps you keep track of your changes and lets you go back to previous versions if needed.\n\nTo commit your changes, open the terminal in the project folder and run these commands:\n\ngit add .\ngit commit -m \"Your commit message here\"\n\n\n\n\n\n\nNote\n\n\n\nYour commit message should be short and descriptive. It should explain what changes you made in this commit. See for example this article for some tips on how to write good commit messages.\n\n\n\n\n5. Share Your Progress\nTo collaborate with classmates or ask for help from your instructor, you need to share your work with them on GitHub. You do this by “pushing” your changes to GitHub. This action will update your assignment repository on GitHub with your latest changes. You don’t have to do it everytime you make a change, but keep in mind that you’ll need to push your changes before submitting your assignment, and that pushing helps you keep your work safe in case something happens to your computer. When working in group, don’t forget to pull the changes from your teammates before pushing your own changes.\n\nTo push your changes to GitHub, use this command:\n\ngit push\n\n\n6. Submit Your Assignment\nWhen you’re satisfied with your work and ready to turn it in, you’ll need to submit your assignment through GitHub. This is typically done through a “Pull Request” on GitHub. Your instructor will provide instructions on how to submit. (If nothing is mentioned, you can assume that you only need to push your work on the main branch.)\n\n\n7. Review Feedback\nAfter the submission deadline, your instructor will review your assignment and may provide feedback or grades through GitHub. You can see this feedback by checking the pull request in your assignment repository on GitHub."
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#conclusion",
    "href": "resources/tutorials/github_classroom.html#conclusion",
    "title": "Github Classroom",
    "section": "Conclusion",
    "text": "Conclusion\nGitHub Classroom is a valuable tool that can make your Master’s program coursework more manageable and collaborative. While it might seem intimidating at first, remember that it’s a skill worth learning, and you can always reach out to your instructor or classmates for assistance. With this guide, you’re well on your way to successfully using GitHub Classroom for your assignments. Happy learning and coding!"
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#tips-and-tricks",
    "href": "resources/tutorials/github_classroom.html#tips-and-tricks",
    "title": "Github Classroom",
    "section": "Tips and tricks",
    "text": "Tips and tricks\nThere exists an integration with VSCode that can make your life easier when working with Github Classroom if you are using VSCode."
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html",
    "title": "Good practices in R",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction"
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html#writing-clean-and-readable-code",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html#writing-clean-and-readable-code",
    "title": "Good practices in R",
    "section": "Writing Clean and Readable Code",
    "text": "Writing Clean and Readable Code\nExample in R:\n# Good: Use meaningful variable names and comments\npopulation_size &lt;- 1000  # Number of individuals in the population\nsample_size &lt;- 100       # Number of individuals in the sample\n\n# Avoid: Using unclear or abbreviated variable names\nps &lt;- 1000\nss &lt;- 100"
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html#using-comments-and-documentation",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html#using-comments-and-documentation",
    "title": "Good practices in R",
    "section": "Using Comments and Documentation",
    "text": "Using Comments and Documentation\n# Good: Add comments to explain complex operations or logic\ncalculate_mean &lt;- function(data) {\n    # Calculate the sum of elements in the data\n    total &lt;- sum(data)\n    \n    # Calculate the mean by dividing the total by the number of elements\n    mean_value &lt;- total / length(data)\n    \n    return(mean_value)\n}\n\n# Avoid: Lack of comments and explanation\ncalc_mean &lt;- function(d) {\n    t &lt;- sum(d)\n    m &lt;- t / length(d)\n    return(m)\n}"
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html#avoiding-global-variables-and-code-dependencies",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html#avoiding-global-variables-and-code-dependencies",
    "title": "Good practices in R",
    "section": "Avoiding Global Variables and Code Dependencies",
    "text": "Avoiding Global Variables and Code Dependencies\n# Good: Use local variables within functions to avoid global scope pollution\ncalculate_variance &lt;- function(data) {\n    n &lt;- length(data)\n    mean_value &lt;- sum(data) / n\n    \n    # Calculate the variance using local variables\n    variance &lt;- sum((data - mean_value)^2) / (n - 1)\n    \n    return(variance)\n}\n\n# Avoid: Using global variables in functions\nmean_value &lt;- 0\ncalc_variance &lt;- function(data) {\n    n &lt;- length(data)\n    mean_value &lt;&lt;- sum(data) / n\n    \n    # Calculate the variance using global variables\n    variance &lt;- sum((data - mean_value)^2) / (n - 1)\n    \n    return(variance)\n}"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html",
    "href": "resources/computing/intro_to_r/3_tidyverse.html",
    "title": "Data manipulation with tidyverse",
    "section": "",
    "text": "In the last section, we imported a clinical cancer dataset and learned to manipulate it using base R (tools included in every R installation).\nIn this session, we’ll continue to work with the same dataset, but will introduce a set of tools specifically designed for data science in R. By the end of this session, you should be able to:\n\ninstall and load packages\nuse tidyverse tools to import data and access rows/columns\ncombine commands using pipes\ntransform and summarize data"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#objectives",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#objectives",
    "title": "Data manipulation with tidyverse",
    "section": "",
    "text": "In the last section, we imported a clinical cancer dataset and learned to manipulate it using base R (tools included in every R installation).\nIn this session, we’ll continue to work with the same dataset, but will introduce a set of tools specifically designed for data science in R. By the end of this session, you should be able to:\n\ninstall and load packages\nuse tidyverse tools to import data and access rows/columns\ncombine commands using pipes\ntransform and summarize data"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#installing-and-loading-packages",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#installing-and-loading-packages",
    "title": "Data manipulation with tidyverse",
    "section": "Installing and loading packages",
    "text": "Installing and loading packages\nPlease ensure RStudio is open with your project directory path (e.g., ~/Desktop/intro_r) listed at the top of your Console. If you do not see the path to your project directory, go to File -&gt; Open Project and navigate to the location of your project directory. Alternatively, using your operating system’s file browser, double click on the r_intro.Rrpoj file.\nCreate a new R script called class3.R, and add # Introduction to R: Class 3 as a title.\nFor this lesson, we’ll be working with a group of R packages called tidyverse. A package is a group of related functions that help you accomplish particular tasks. tidyverse packages have been designed specifically to support tasks related to data science, such as data manipulation, filtering, and visualization.\nThe first thing we need to do is install the software:\n\n\nThe following package(s) will be installed:\n- tidyverse [2.0.0]\nThese packages will be installed into \"~/Documents/Teaching/computational_statistics/2023/new_website/renv/library/R-4.3/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing tidyverse ...                      OK [linked from cache]\nSuccessfully installed 1 package in 6 milliseconds.\n\n\nA few notes about installing packages:\n\nYou only need to perform this installation once per computer, or when updating R or the package.\nIf you see red text output in the Console during this installation, don’t be alarmed: this doens’t necessarily indicate a problem. You are seeing a report of the various pieces of software being downloaded and installed.\nIf prompted, you should install all packages (say yes or all), as well as yes to compiling any packages\nWhen the installation is complete (this may take several minutes), you’ll see the command prompt (&gt;) in your Console.\n\nOnce you have the software installed, you’ll need to load it:\n\n# load library/package\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nLoading packages is similar to opening a software application on your computer; it makes a previously installed set of software available for use. A few notes about loading packages:\n\nYou’ll need to load packages every time you open RStudio (or R restarts)\nLoading tidyverse loads a collection of packages; these are listed under “Attaching packages”\nThere are many other packages included as dependencies. If some of them did not install successfully, you will receive an error at this step. For this lesson, you can try library(dplyr), and ask your instructor for help later.\nThe section in the output above referencing “Conflicts” shows you which functions you just loaded have names identical to packages you already have loaded (in base R). This shouldn’t affect the code we write in this lesson, though it’s useful to know the double colon syntax (::) allows you to reference functions in a different package with same name.\n\nYou can check to make sure the new package we’ll be using is available by executing ?select in the Console, or by searching for that function in the help panel. You can also look in the “Packages” tab in the same panel. If the package (in this case, either tidyverse or dplyr) is present in the list, it’s installed. If the box next to the package name is checked, it’s loaded. In this lesson, if you receive an error saying a function isn’t available or recognized, check to make sure the package is loaded."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#selecting-columns-and-rows",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#selecting-columns-and-rows",
    "title": "Data manipulation with tidyverse",
    "section": "Selecting columns and rows",
    "text": "Selecting columns and rows\nThe first task we’ll undertake with our newly installed tidyverse tools is importing our data:\n\n# import data\nclinical &lt;- read_csv(\"data/clinical.csv\")\n\nRows: 6832 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): primary_diagnosis, tumor_stage, vital_status, morphology, state, t...\ndbl  (8): age_at_diagnosis, days_to_death, days_to_birth, days_to_last_follo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that this function looks similar to what we used in the last lesson (read.csv), but the underscore replacing the dot means it’s actually a different function. In fact, this is the data import function from tidyverse. The output provided by this function indicates a few key differences with our import yesterday.\nThe data import execution includes a description of how each variable (column) is interpreted. In our data’s case, the numeric data are col_double and the character data are col_character (not factors!).\nWe can explore these differences further:\n\n# inspect object\nstr(clinical)\n\nspc_tbl_ [6,832 × 20] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ primary_diagnosis          : chr [1:6832] \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ tumor_stage                : chr [1:6832] \"stage ia\" \"stage ib\" \"stage ib\" \"stage ia\" ...\n $ age_at_diagnosis           : num [1:6832] 24477 26615 28171 27154 29827 ...\n $ vital_status               : chr [1:6832] \"dead\" \"dead\" \"dead\" \"alive\" ...\n $ morphology                 : chr [1:6832] \"8070/3\" \"8070/3\" \"8070/3\" \"8083/3\" ...\n $ days_to_death              : num [1:6832] 371 136 2304 NA 146 ...\n $ state                      : chr [1:6832] \"live\" \"live\" \"live\" \"live\" ...\n $ tissue_or_organ_of_origin  : chr [1:6832] \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_birth              : num [1:6832] -24477 -26615 -28171 -27154 -29827 ...\n $ site_of_resection_or_biopsy: chr [1:6832] \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_last_follow_up     : num [1:6832] NA NA 2099 3747 NA ...\n $ cigarettes_per_day         : num [1:6832] 10.96 2.19 1.64 1.1 NA ...\n $ years_smoked               : num [1:6832] NA NA NA NA NA NA NA NA NA NA ...\n $ gender                     : chr [1:6832] \"male\" \"male\" \"female\" \"male\" ...\n $ year_of_birth              : num [1:6832] 1936 1931 1927 1930 1923 ...\n $ race                       : chr [1:6832] \"white\" \"asian\" \"white\" \"white\" ...\n $ ethnicity                  : chr [1:6832] \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" ...\n $ year_of_death              : num [1:6832] 2004 2003 NA NA 2004 ...\n $ bcr_patient_barcode        : chr [1:6832] \"TCGA-18-3406\" \"TCGA-18-3407\" \"TCGA-18-3408\" \"TCGA-18-3409\" ...\n $ disease                    : chr [1:6832] \"LUSC\" \"LUSC\" \"LUSC\" \"LUSC\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   primary_diagnosis = col_character(),\n  ..   tumor_stage = col_character(),\n  ..   age_at_diagnosis = col_double(),\n  ..   vital_status = col_character(),\n  ..   morphology = col_character(),\n  ..   days_to_death = col_double(),\n  ..   state = col_character(),\n  ..   tissue_or_organ_of_origin = col_character(),\n  ..   days_to_birth = col_double(),\n  ..   site_of_resection_or_biopsy = col_character(),\n  ..   days_to_last_follow_up = col_double(),\n  ..   cigarettes_per_day = col_double(),\n  ..   years_smoked = col_double(),\n  ..   gender = col_character(),\n  ..   year_of_birth = col_double(),\n  ..   race = col_character(),\n  ..   ethnicity = col_character(),\n  ..   year_of_death = col_double(),\n  ..   bcr_patient_barcode = col_character(),\n  ..   disease = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nYou may notice the presence of tbl and related labels in the classes for this object. tbl stands for tibble, which is a type of data frame with specific constraints to ensure better data handling.\nIf you preview the dataset, it will look the same, and we can interact with the data in the same way. These assumptions about the data mesh nicely with the other tools in the tidyverse.\nNow that our data are imported, we can explore the tidyverse functions for extracting parts of the dataset.\nFirst, we can explore selecting certain columns by name:\n\n# selecting columns with tidyverse (dplyr)\nsel_columns &lt;- select(clinical, tumor_stage, ethnicity, disease)\n\nThe syntax for the select function is to specify the dataset first, then the names of each of the columns you would like to retain in the output object. If we look at the object, we’ll see it has only three columns but all rows.\nYou’ll note that the column headers don’t require quotation marks; this is a shortcut programmed into tidyverse functions.\nAs with base R functions, we can also select a range of columns:\n\n# select range of columns\nsel_columns2 &lt;- select(clinical, tumor_stage:vital_status)\n\nIn addition to these approaches, we can also use other helper functions for selecting columns: starts_with(), ends_with(), and contains() are examples that assist in extracting columns with headers that meet certain conditions. For example, using starts_with(tumor) in place of the column names will give you all columns that start with the word tumor.\nWe can use a separate function to extract rows that meet particular conditions:\n\n# select rows conditionally: keep only lung cancer cases\nfiltered_rows &lt;- filter(clinical, disease == \"LUSC\") \n\nThe syntax here is similar to select, and the conditional filters can be applied in similarly to base R functions.\n\nChallenge-columns\nCreate a new object from clinical called race_disease that includes only the race, &gt; ethnicity, and disease columns.\n\n\nChallenge-rows\nCreate a new object from race_disease called race_BRCA that includes only BRCA (from disease)."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#combining-commands",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#combining-commands",
    "title": "Data manipulation with tidyverse",
    "section": "Combining commands",
    "text": "Combining commands\nThe last challenges used an intermediate object to obtain an object with two subsetting methods applied. It’s common in data science to apply more than one requirement for extracting data. If you want to avoid creating an intermediate object, you could nest one command inside the other:\n\n# same task as challenges, but nested commands \nrace_BRCA2 &lt;- select(filter(clinical, disease == \"BRCA\"), race, ethnicity, disease)\n\nIn this case, filter(clinical, disease == \"BRCA\") becomes the input for select.\nWhile this is a common approach, especially in base R, it can be difficult for us as coders to read and interpret the code.\nOne of the most useful features of tidyverse is its inclusion of a programming method called pipes. This approach can be found in many programming languages, in part because of its utility: a pipe sends the output from the lefthand side of the symbol as the input for the righthand side. In R, pipes are represented as %&gt;%.\nWe can use pipes to connect the same two data extraction tasks:\n\n# same task as above, but with pipes\npiped &lt;- clinical %&gt;%\n  select(race, ethnicity, disease) %&gt;%\n  filter(disease == \"BRCA\")\n\nThe command above starts by naming the object that will result from this assignment. The dataset is named as the first input. Because executing the name of an object sends the object contents as output, this means the second line receives the object as input. The output from the select line is sent as input to the filter line. This effectively demonstrates how pipes can be used to connect multiple commands together.\n\nNow that we are running code in chunks that span multiple lines, you can see one of the other nice features of RStudio: your cursor can be placed on any line of the multi-line chunk when you execute, and the entire set of code will run together.\n\nThese examples also help highlight the importance of style and convention in code formatting. After the first line, the code is indented. While this isn’t necessary for the code to work, it does make it a lot easier to read and understand the code.\nLet’s take a look at another example of piped commands:\n\n# extract race, ethinicity, and disease from cases born prior to 1930\npiped2 &lt;- clinical %&gt;%\n  filter(year_of_birth &lt; 1930) %&gt;%\n  select(race, ethnicity, disease)\n\nIn the code above, we’re applying a mathematical condition to find specific rows, and the selecting certain columns. Does the order of commands differ? We can switch the order of the filter and select lines to see:\n\npiped3 &lt;- clinical %&gt;%\n  select(race, ethnicity, disease) %&gt;%\n  filter(year_of_birth &lt; 1930)\n\nThe code above should give you an error, because in this case, the order does matter! The output from the second line does not include the year_of_birth column, so R is unable to apply the filter in the third line.\n\nChallenge-pipes\nUse pipes to extract the columns gender, years_smoked, and year_of_birth from the object clinical for only living patients (vital_status) who have smoked fewer than 1 cigarettes_per_day."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#mutate",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#mutate",
    "title": "Data manipulation with tidyverse",
    "section": "Mutate",
    "text": "Mutate\nThis lesson so far has mostly shown new ways of accomplishing the same tasks we learned in the last lesson. tidyverse includes much more functionality, however, including the ability to mutate columns. Common tasks for which mutate is useful include unit conversions, transformation, and creating ratios from among existing columns.\nWe can use this function to convert the days_to_death column to years:\n\n# convert days to years\nclinical_years &lt;- clinical %&gt;%\n  mutate(years_to_death = days_to_death / 365)\n\nThe actual conversion works by providing a formula (days_to_death / 365) and the name of the new column (years_to_death). If you inspect the resulting object, you’ll see years_to_death added as a new column at the end of the table.\n\nmutate works by retaining all previous columns and creating new columns as per the formula specified. tidyverse also includes transmute, which drops the existing columns used to calculcate the new columns.\n\nWe can use mutate to perform multiple conversions at once:\n\n# convert days to year and months at same time, and we don't always need to assign to object\nclinical %&gt;%\n  mutate(years_to_death = days_to_death / 365,\n         months_to_death = days_to_death / 30) %&gt;%\n  glimpse() # preview data output\n\nRows: 6,832\nColumns: 22\n$ primary_diagnosis           &lt;chr&gt; \"C34.1\", \"C34.1\", \"C34.3\", \"C34.1\", \"C34.3…\n$ tumor_stage                 &lt;chr&gt; \"stage ia\", \"stage ib\", \"stage ib\", \"stage…\n$ age_at_diagnosis            &lt;dbl&gt; 24477, 26615, 28171, 27154, 29827, 23370, …\n$ vital_status                &lt;chr&gt; \"dead\", \"dead\", \"dead\", \"alive\", \"dead\", \"…\n$ morphology                  &lt;chr&gt; \"8070/3\", \"8070/3\", \"8070/3\", \"8083/3\", \"8…\n$ days_to_death               &lt;dbl&gt; 371, 136, 2304, NA, 146, NA, 345, 716, 280…\n$ state                       &lt;chr&gt; \"live\", \"live\", \"live\", \"live\", \"live\", \"l…\n$ tissue_or_organ_of_origin   &lt;chr&gt; \"C34.1\", \"C34.1\", \"C34.3\", \"C34.1\", \"C34.3…\n$ days_to_birth               &lt;dbl&gt; -24477, -26615, -28171, -27154, -29827, -2…\n$ site_of_resection_or_biopsy &lt;chr&gt; \"C34.1\", \"C34.1\", \"C34.3\", \"C34.1\", \"C34.3…\n$ days_to_last_follow_up      &lt;dbl&gt; NA, NA, 2099, 3747, NA, 3576, NA, NA, 1810…\n$ cigarettes_per_day          &lt;dbl&gt; 10.9589041, 2.1917808, 1.6438356, 1.095890…\n$ years_smoked                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 26…\n$ gender                      &lt;chr&gt; \"male\", \"male\", \"female\", \"male\", \"male\", …\n$ year_of_birth               &lt;dbl&gt; 1936, 1931, 1927, 1930, 1923, 1942, 1953, …\n$ race                        &lt;chr&gt; \"white\", \"asian\", \"white\", \"white\", \"not r…\n$ ethnicity                   &lt;chr&gt; \"not hispanic or latino\", \"not hispanic or…\n$ year_of_death               &lt;dbl&gt; 2004, 2003, NA, NA, 2004, NA, 2005, 2006, …\n$ bcr_patient_barcode         &lt;chr&gt; \"TCGA-18-3406\", \"TCGA-18-3407\", \"TCGA-18-3…\n$ disease                     &lt;chr&gt; \"LUSC\", \"LUSC\", \"LUSC\", \"LUSC\", \"LUSC\", \"L…\n$ years_to_death              &lt;dbl&gt; 1.0164384, 0.3726027, 6.3123288, NA, 0.400…\n$ months_to_death             &lt;dbl&gt; 12.366667, 4.533333, 76.800000, NA, 4.8666…\n\n\nThe code above also features a new function, glimpse, that can be useful when developing new piped code. Note that we did not assign the output above to a new object; we allowed it to be printed to the Console. Because this is a large dataset, that type of output can be unweildy. glimpse allows us to see a preview of the data, including the two new columns created.\n\nChallenge-lung\nExtract only lung cancer patients (LUSC, from disease) and create a new column called total_cig representing an estimate of the total number of cigarettes smoked (use columns years_smoked and cigarettes_per_day)."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#split-apply-combine",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#split-apply-combine",
    "title": "Data manipulation with tidyverse",
    "section": "Split-apply-combine",
    "text": "Split-apply-combine\nOur clinical dataset includes categorical (character) data. One example is the gender column. We can assess the different categories available using a base R function:\n\n# show categories in gender\nunique(clinical$gender)\n\n[1] \"male\"   \"female\" NA      \n\n\ntidyverse includes an approach called split-apply-combine that allows us to:\n\nsplit data into groups,\napply a task for each group,\ncombine the results back together into a single table.\n\nWe can try out this approach by counting the number of each gender in our dataset:\n\n# count number of individuals of each gender\nclinical %&gt;%\n  group_by(gender) %&gt;%\n  tally() \n\n# A tibble: 3 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 female  3535\n2 male    3258\n3 &lt;NA&gt;      39\n\n\ngroup_by is not particularly useful by itself, but powerful together with a second function like tally. The two columns in the resulting tibble represent the categories from group_by and the number of cases for each gender (n).\nAn additional function for use with group_by is summarize:\n\n# summarize average days to death by gender\nclinical %&gt;%\n  group_by(gender) %&gt;%\n  summarize(mean_days_to_death = mean(days_to_death, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  gender mean_days_to_death\n  &lt;chr&gt;               &lt;dbl&gt;\n1 female               947.\n2 male                 826.\n3 &lt;NA&gt;                 NaN \n\n\nSimilar to mutate, we provide summarize with a formula indicating how we would like the groups to be handled.\nIn the command above, we use na.rm = TRUE to exclude missing data from the calculation of mean from days_to_death. We still have NA reported in the output table, though, because of the NA category in gender.\nWe can apply an additional filter to remove this missing data, prior to grouping:\n\n# remove NA\nclinical %&gt;%\n  filter(!is.na(gender)) %&gt;%\n  group_by(gender) %&gt;%\n  summarize(mean_days_to_death = mean(days_to_death))\n\n# A tibble: 2 × 2\n  gender mean_days_to_death\n  &lt;chr&gt;               &lt;dbl&gt;\n1 female                 NA\n2 male                   NA\n\n\n\nChallenge-smoke-complete\nCreate object called smoke_complete from clinical that contains no missing data for cigarettes per day or age at diagnosis.\n\n\nChallenge-save\nHow do you save resulting table to file? How would you find this answer?\n\nThe solution to the challenges above represent the first of two datasets we’ll be using for data visualization in our next class. Make sure you’ve executed this code to save the filtered data file for use next time:\n\nsmoke_complete &lt;- clinical %&gt;%\n  filter(!is.na(age_at_diagnosis)) %&gt;%\n  filter(!is.na(cigarettes_per_day))\nwrite_csv(smoke_complete, \"data/smoke_complete.csv\")\n\nThe command above uses write_csv, which is the tidyverse method of saving a csv file. Base R possesses a function, write.csv, that performs a similar task, but by default includes quotation marks around cells with character data as well as row names (sequential numbers, unless otherwise specified).\n\nChallenge-birth-complete\nCreate a new object called birth_complete that contains no missing data for year of birth or vital status.\n\nThis challenge begins filtering the second of our two datasets for next time. Make sure you include the filter to remove missing data that’s been encoded as “not reported”!\n\n# make sure ALL missing data is removed!\nbirth_complete &lt;- clinical %&gt;%\n  filter(!is.na(year_of_birth)) %&gt;%\n  filter(!is.na(vital_status)) %&gt;%\n  filter(vital_status != \"not reported\")"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#filtering-data-based-on-number-of-cases-of-each-type",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#filtering-data-based-on-number-of-cases-of-each-type",
    "title": "Data manipulation with tidyverse",
    "section": "Filtering data based on number of cases of each type",
    "text": "Filtering data based on number of cases of each type\nWe’re going to perform one last manipulation on this second dataset for next time, which will allow us to reduce the total number of cancer types present in this dataset.\nFirst, we’ll need to count how many cases for each cancer type exist in the dataset:\n\n# counting number of records in each cancer\ncancer_counts &lt;- clinical %&gt;%\n  count(disease) %&gt;%\n  arrange(n) \n\nThe count function is similar to tally, but doesn’t need to have group_by applied first. The arrange function added at the end sorts the table using the column specified. Although this isn’t necessary for the analysis to proceed, it makes it easier for us to interpret the results.\nNext, we’ll identify which cancer types are represented by at least 500 cases in this dataset:\n\n# get names of frequently occurring cancers\nfrequent_cancers &lt;- cancer_counts %&gt;%\n  filter(n &gt;= 500) \n\nWe can then use this object to filter based on the number of cases:\n\n# extract data from cancers to keep\nbirth_reduced &lt;- birth_complete %&gt;%\n  filter(disease %in% frequent_cancers$disease)\n\nThe new syntax here is %in%, which allows you to compare each entry in disease from birth_complete to the disease column in frequent_cancers (remember that frequent_cancers$disease means the disease column from frequent_cancers). This keeps only cases from the birth_complete dataset that are from cancers that are frequently occurring.\nFinally, we’ll write the final output to a file:\n\n# save results to file in data/ named birth_reduced\nwrite_csv(birth_reduced, \"data/birth_reduced.csv\")\n\n\nChallenge-tumor\nExtract all tumor stages with more than 200 cases (Hint: also check to see if there are any other missing/ambiguous data!)"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#wrapping-up",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#wrapping-up",
    "title": "Data manipulation with tidyverse",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this session, we acquainted ourselves with tidyverse, and learned some tools for data filtering and manipulation. We covered examples from many of the main categories of data manipulation tasks; if you’d like more information on these functions and others available (including methods of joining multiple tables together), please check out the dplyr cheatsheet.\nIn the next session, we’ll wrap up the course by creating publication-quality images using ggplot2, a data visualization package in tidyverse, and the two datasets we filtered in the sections above."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#extra-exercises",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#extra-exercises",
    "title": "Data manipulation with tidyverse",
    "section": "Extra exercises",
    "text": "Extra exercises\nThe following exercises all use the same clinical data from this class.\n\nChallenge-ethnicity\nHow many hispanic or latino individuals in clinical are not also white? What are their races?\n\n\nChallenge-years\nCreate a new column for clinical called age_at_death that calculates this statistic (in years) from year_of_birth and year_of_death.\n\n\nChallenge-helpers\ndplyr includes several “helpers” that allows selection of columns meeting particular criteria (described on the first page of the dplyr cheatsheet near the top of the right hand column: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf). Using one of these tools, extract all columns that include “diagnosis”.\n\n\nChallenge-combine\nHow many patients are hispanic or latino patients (column ethnicity), died after the year 2000 (year_of_death), and possess no missing data for cigarettes per day?"
  },
  {
    "objectID": "resources/computing/intro_to_r/index.html",
    "href": "resources/computing/intro_to_r/index.html",
    "title": "Introduction to R",
    "section": "",
    "text": "You might want to choose from other tutorials as they might suit your learning style better. See Section 0.1 for a list of other tutorials.\n\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nFunctions and objects\n\n\nR syntax, assigning objects, using functions\n\n\n\n\nWorking with data\n\n\nData types and structures; slicing and subsetting data\n\n\n\n\nData manipulation with tidyverse\n\n\nData manipulation with dplyr\n\n\n\n\nData visualization in R\n\n\nData visualization in ggplot2\n\n\n\n\nGood practices in R\n\n\nTips and tricks for writing better code\n\n\n\n\n\nNo matching items\n\n\n\n0.1 Additional resources\n\nR for Data Science (2e)\nData Analysis and Visualisation in R for Ecologists\nIntroduction to R for Geospatial Data\nHands-On Programming with R\n\n\n\n\n\n\n\nAttributions\n\n\n\nAll these classes are taken almost verbatim from fredhutch.io, the data and computational analysis training program at Fred Hutch, which was adapted from content originally appearing in R for data analysis and visualization of Ecological Data, Copyright (c) Data Carpentry."
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html",
    "href": "resources/computing/intro_to_python/good_practice_python.html",
    "title": "Good practices in Python",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction"
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html#writing-clean-and-readable-code",
    "href": "resources/computing/intro_to_python/good_practice_python.html#writing-clean-and-readable-code",
    "title": "Good practices in Python",
    "section": "Writing Clean and Readable Code",
    "text": "Writing Clean and Readable Code\n# Good: Use meaningful variable names and comments\npopulation_size = 1000  # Number of individuals in the population\nsample_size = 100       # Number of individuals in the sample\n\n# Avoid: Using unclear or abbreviated variable names\nps = 1000\nss = 100"
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html#using-comments-and-documentation",
    "href": "resources/computing/intro_to_python/good_practice_python.html#using-comments-and-documentation",
    "title": "Good practices in Python",
    "section": "Using Comments and Documentation",
    "text": "Using Comments and Documentation\n# Good: Add comments to explain complex operations or logic\ndef calculate_mean(data):\n    # Calculate the sum of elements in the data\n    total = sum(data)\n    \n    # Calculate the mean by dividing the total by the number of elements\n    mean_value = total / len(data)\n    \n    return mean_value\n\n# Avoid: Lack of comments and explanation\ndef calc_mean(d):\n    t = sum(d)\n    m = t / len(d)\n    return m"
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html#avoiding-global-variables-and-code-dependencies",
    "href": "resources/computing/intro_to_python/good_practice_python.html#avoiding-global-variables-and-code-dependencies",
    "title": "Good practices in Python",
    "section": "Avoiding Global Variables and Code Dependencies",
    "text": "Avoiding Global Variables and Code Dependencies\n# Good: Use local variables within functions to avoid global scope pollution\ndef calculate_variance(data):\n    n = len(data)\n    mean_value = sum(data) / n\n    \n    # Calculate the variance using local variables\n    variance = sum((data - mean_value)**2) / (n - 1)\n    \n    return variance\n\n# Avoid: Using global variables in functions\nmean_value = 0\ndef calc_variance(data):\n    n = len(data)\n    global mean_value\n    mean_value = sum(data) / n\n    \n    # Calculate the variance using global variables\n    variance = sum((data - mean_value)**2) / (n - 1)\n    \n    return variance"
  },
  {
    "objectID": "resources/computing/intro_to_julia/index.html",
    "href": "resources/computing/intro_to_julia/index.html",
    "title": "Introduction to Julia",
    "section": "",
    "text": "For a refresher on Julia basics, you can check out one of the following resources:\n\nTutorials from the official documentation\nJulia Workshop for Data Science\nJulia academy\nIntroduction to Scientific Programming and Machine Learning with Julia\nand many others\n\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nGood practices in Julia\n\n\nTips and tricks for writing better code\n\n\n\n\n\nNo matching items\n\n\n\nA very useful Julia package is RDatasets.jl, which provides access to many of the datasets available in R packages."
  },
  {
    "objectID": "resources/resources.html",
    "href": "resources/resources.html",
    "title": "All the additional resources",
    "section": "",
    "text": "Supplementary material for the course\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nOLS estimator\n\n\nDeriving the Least-Squares Estimates for Simple Linear Regression\n\n\n\n\nEDA example\n\n\nData exploration of insurance redlining data\n\n\n\n\nExploring Data with tidyverse\n\n\n\n\n\n\n\nKernel Density Estimation\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nTips and tricks\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nBest practices for collaborative work\n\n\nUsing code style guidelines, sharing code and collaborating.\n\n\n\n\nGood practices for coding\n\n\nA collection of tips and tricks to write better code\n\n\n\n\nReproducible scientific computing\n\n\nWhy and how to make your scientific computing reproducible.\n\n\n\n\nVirtual environments\n\n\nHow to make it possible for others to run your code with the same dependencies as you\n\n\n\n\n\nNo matching items\n\n\n\n\nTutorials\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nGit and GitHub\n\n\nA guide to set-up Git and GitHub\n\n\n\n\nGithub Classroom\n\n\nHow to use Github Classroom to submit assignments\n\n\n\n\nSoftware installation\n\n\nHow to have everything ready for the course\n\n\n\n\n\nNo matching items\n\n\n\n\nIntroduction to Programming\n\n\n\n\n\n\nNote\n\n\n\nYou might want to choose from other tutorials as they might suit your learning style better. See the additional ressources for each language.\n\n\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nR cheatsheets\n\n\nA collection of cheatsheets forR\n\n\n\n\nIntroduction to R\n\n\nA collection of resources for learning R\n\n\n\n\nIntroduction to Julia\n\n\nA collection of resources for learning Julia\n\n\n\n\nIntroduction to Python\n\n\nA collection of resources for learning Python\n\n\n\n\n\nNo matching items\n\n\n\nAdditional resources\n\nR for Data Science (2e)\nData Analysis and Visualisation in R for Ecologists\nIntroduction to R for Geospatial Data"
  },
  {
    "objectID": "resources/tips/tips_coding.html",
    "href": "resources/tips/tips_coding.html",
    "title": "Good practices for coding",
    "section": "",
    "text": "What will happen when you show your work to someone else if you don’t follow a guide style. (From Xkcd: code quality)"
  },
  {
    "objectID": "resources/tips/tips_coding.html#why-should-you-care",
    "href": "resources/tips/tips_coding.html#why-should-you-care",
    "title": "Good practices for coding",
    "section": "",
    "text": "What will happen when you show your work to someone else if you don’t follow a guide style. (From Xkcd: code quality)"
  },
  {
    "objectID": "resources/tips/tips_coding.html#clear-naming-of-variables-and-functions",
    "href": "resources/tips/tips_coding.html#clear-naming-of-variables-and-functions",
    "title": "Good practices for coding",
    "section": "Clear naming of variables and functions",
    "text": "Clear naming of variables and functions\n\nBe explicit in your naming. If you name a variable my_variable, don’t name another one my_var or my_var_. If you name a function my_function, don’t name another one my_func or my_func_.\nNames should be self-explanatory. If you need to add a comment to explain what a variable or a function does, it means that you should change its name. For example, my_variable is a bad name, but number_of_samples is a good name. df, df2, … are bad names, but raw_data, ìmputed_data, … are good names."
  },
  {
    "objectID": "resources/tips/tips_coding.html#consistency",
    "href": "resources/tips/tips_coding.html#consistency",
    "title": "Good practices for coding",
    "section": "Consistency",
    "text": "Consistency\n\nBe consistent in your style. If you start a project, try to follow the style of the project. If you join a project, try to follow the style of the project.\nBe consistent in your naming. If you name a variable my_variable, don’t name another one myVariable or myVariable_. If you name a function my_function, don’t name another one myFunction or myFunction_.\nBe consistent in your formatting. If you use 2 spaces for indentation, don’t use 4 spaces for indentation. If you use 2 spaces for indentation, don’t use tabs for indentation.\n\nAll of this will make your code easier to read and understand."
  },
  {
    "objectID": "resources/tips/tips_coding.html#code-style-guidelines",
    "href": "resources/tips/tips_coding.html#code-style-guidelines",
    "title": "Good practices for coding",
    "section": "Code Style Guidelines",
    "text": "Code Style Guidelines\nConsistency in coding style is essential for making the code more readable and understandable by others. Adopting a standard coding style across your team helps avoid confusion and reduces the time spent on code reviews. For each language (Julia, R, and Python), there are widely accepted coding style guidelines:\n\n\nJulia\n\nJulia official style guide\nmore opinionated SciML Style Guide\n\n\n\n\nPython\n\nPEP 8\n\n\n\n\nR\n\ntidyverse style guide\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere exists tools to help you check your code style and correct the basic mistakes. They are called linters. For example, in Julia, you can use JuliaFormatter.jl, in Python black and in R styler."
  },
  {
    "objectID": "resources/tips/virtual_environments.html",
    "href": "resources/tips/virtual_environments.html",
    "title": "Virtual environments",
    "section": "",
    "text": "Package managers typically maintain a database of software dependencies and version information to prevent software mismatches and missing prerequisites. When package versions collide, this can lead to problems ranging from error messages and frustration to silent bugs and unexpected code behavior !"
  },
  {
    "objectID": "resources/tips/virtual_environments.html#why-cant-i-just-install-the-packages-i-need",
    "href": "resources/tips/virtual_environments.html#why-cant-i-just-install-the-packages-i-need",
    "title": "Virtual environments",
    "section": "Why can’t I just install the packages I need ?",
    "text": "Why can’t I just install the packages I need ?\nInstead of installing everything globally and risking conflicts, virtual environments give you separate spaces for each project. This means no more worrying about messing up your setup! Plus, package managers make installing, updating, and removing packages a breeze, saving you time and hassle. You can easily share your projects with classmates and reproduce your work on any machine.\n\n\n\n\n\n\nNote\n\n\n\nThis is a very brief introduction to virtual environments and package managers. For more details, please see the documentation of the package manager you use.\n\nPython virtual environment tutorial\nJulia package manager documentation\nR renv documentation"
  },
  {
    "objectID": "resources/tips/virtual_environments.html#benefits",
    "href": "resources/tips/virtual_environments.html#benefits",
    "title": "Virtual environments",
    "section": "Benefits",
    "text": "Benefits\nHere are some examples of why using virtual environments and package managers can be incredibly useful for scientific computing:\n\nProject Isolation: Let’s say you’re working on two different projects—one in R and another in Python. Each project requires different versions of certain packages. By using virtual environments, you can create separate environments for each project, ensuring that the specific package versions needed for each don’t interfere with one another.\nReproducibility: With virtual environments, you can easily share your projects with classmates or professors, ensuring that they can replicate your exact setup without any compatibility issues. This enhances the reproducibility of your work and allows others to verify your results.\nDependency Management: Sometimes, a package may rely on a specific version of another package to work correctly. Package managers handle these dependencies automatically, saving you the headache of figuring out and managing dependencies manually.\nExperimentation: Working on a new statistical model and want to test different libraries or versions? With virtual environments, you can create a sandbox to experiment freely without worrying about affecting your main setup.\nCollaboration: When collaborating with classmates or researchers, having consistent environments through virtual environments ensures that everyone is on the same page. It prevents conflicts arising from different package versions and improves overall productivity.\nSystem Cleanliness: Installing packages globally can clutter your system, making it difficult to manage and potentially leading to conflicts between different software. Virtual environments keep your system clean and organized.\nVersion Control: Using virtual environments makes it easier to integrate your projects with version control systems like Git. You can include the configuration files for your virtual environment in the repository, making it simpler for others to work on the project.\nEfficient Updates: Package managers allow you to update packages quickly and efficiently. You can easily check for updates, install the latest versions, and keep your project up-to-date with the latest features and bug fixes.\n\nBy embracing virtual environments and package managers, you’ll have a smoother, more organized, and productive workflow, making your research and analysis process much more enjoyable and effective."
  },
  {
    "objectID": "resources/tips/virtual_environments.html#downsides",
    "href": "resources/tips/virtual_environments.html#downsides",
    "title": "Virtual environments",
    "section": "Downsides",
    "text": "Downsides\nYou will have to run a few commands everytime you start a new project. This is a small price to pay for the benefits you get. (You may also need to activate the virtual environment everytime you start a new shell session, but this can be automated)."
  },
  {
    "objectID": "resources/tips/virtual_environments.html#how",
    "href": "resources/tips/virtual_environments.html#how",
    "title": "Virtual environments",
    "section": "How ?",
    "text": "How ?\nrenv for R package management, venv, conda or others for Python package management. Julia has this feature built in using Pkg.\n\nCreating a virtual environment\n\n\n\nIn terminal\npython3.6 -m venv my_env \nsource my_env/bin/activate\n\n\n\nIn Julia repl\nusing Pkg\nPkg.activate(\"my_env\")\n\n\n\nIn R console\nrenv::init()\n\n\n\n\n\nAdding packages to the virtual environment (already activated)\n\n\n\nIn terminal\npip install numpy\n\n\n\nIn Julia repl\nPkg.add(Plots)\n\n\n\nIn R console\nrenv::install(\"tidyverse\")\n\n\n\n\n\nRecreating the virtual environment from a file (after creating the environment)\n\n\n\nIn terminal\npip install -r requirements.txt\n\n\n\nIn Julia repl\nPkg.instantiate()\n\n\n\nIn R console\nrenv::restore()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPython is the only one that has a specific command to create a file with the list of packages.\npip freeze &gt; requirements.txt\nFor R and Julia, the file is created automatically when you add a package to the environment and updates automatically when you add or remove packages. (For Renv you may need to run renv::snapshot() to update the file sometimes).\n\n\nFor more details on the commands or the OS specificity please see the documentation of the package manager you are using:\n\nrenv renv wignet\nJulia Julia pkg\nPython Python venv and pip, conda\n\n\nStep by step tutorial\nPython | Julia | R\n\nPython Virtual Environments (venv):\nVirtual environments in Python enable you to create isolated environments for each project. Here’s how to use venv:\n\nOpen your terminal or command prompt.\nNavigate to your project’s directory.\nCreate a new virtual environment:\npython -m venv my_project_env\nActivate the virtual environment:\n\nOn Windows:\n\nmy_project_env\\Scripts\\activate\n\nOn macOS/Linux:\n\nsource my_project_env/bin/activate\nInstall packages within the virtual environment:\npip install package_name\nDeactivate the virtual environment when you’re done:\ndeactivate\n\n\n\nJulia Package Manager (Pkg):\nJulia’s Pkg allows you to manage and install packages effortlessly. Here’s how to use Pkg:\n\nOpen the Julia REPL (Read-Eval-Print Loop).\nTo enter package management mode, type ].\nCreate a new environment and activate it:\nactivate my_project_env\nInstall packages within the environment:\nadd package_name\nUpdate packages:\nupdate\nTo exit package management mode, press Ctrl + C or type exit().\n\n\n\nR Package Manager (renv):\nIn R, renv provides a similar functionality to Python’s venv and Julia’s Pkg. Here’s how to use renv:\n\nOpen your R console or RStudio.\nInstall the renv package (if not already installed):\ninstall.packages(\"renv\")\nInitialize renv for your project:\nrenv::init()\nInstall packages within the renv environment:\ninstall.packages(\"package_name\")\nRestore the project’s environment to remove any packages that aren’t listed in the lockfile:\nrenv::restore()\nor update the lockfile to include any new packages:\nrenv::snapshot()\nDeactivate the renv environment (optional):\nrenv::deactivate()\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\ninitialization\nactivate\ndeactivate\nadd package\n\n\n\n\nrenv\nrenv::init()\nrenv:activate()\nrenv::deactivate()\nrenv::install()\n\n\nvenv\npython -m venv {name}\nsource {name}/bin/activate\ndeactivate\npip install ...\n\n\nJulia\n] activate {name}\n] activate {name}\n] activate\n] add ...\n\n\n\n\n\n\n\n\n\n\n\n\nfiles to share\nto recreate\n\n\n\n\nR and renv\nrenv.lock\nrenv::restore()\n\n\nPython and venv\nrequirements.txt\npip install -r requirements.txt\n\n\nJulia\nProject.toml\n] instantiate"
  },
  {
    "objectID": "notes/week_02.html",
    "href": "notes/week_02.html",
    "title": "Exploring Data with tidyverse",
    "section": "",
    "text": "We will use two data sets for illustration.\n\n\nInsurance redlining refers to the practice of refusing to issue insurance to certain types of people or within some geographic area. The name comes from the act of drawing a red line around an area on a map. Now few would quibble with an insurance company refusing to sell auto insurance to a frequent drunk driver, but other forms of discrimination would be unacceptable.\nIn the late 1970s, the US Commission on Civil Rights examined charges by several Chicago community organizations that insurance companies were redlining their neighborhoods. Because comprehensive information about individuals being refused homeowners insurance was not available, the number of FAIR plan policies written and renewed in Chicago by ZIP code for the months of December 1977 through May 1978 was recorded. The FAIR plan was offered by the city of Chicago as a default policy to homeowners who had been rejected by the voluntary market. Information on other variables that might affect insurance writing such as fire and theft rates was also collected at the ZIP code level. The variables are:\n\ninvolact new FAIR plan policies and renewals per 100 housing units,\nfire fires per 100 housing units,\ntheft thefts per 1000 population,\nrace racial composition in percentage of minority,\nage (of housing) percentage of housing units built before 1939,\nincome median family income in thousands of dollars,\nside north or south side of Chicago.\n\nThe variable involact acts as a measure of insurance availability in the voluntary market, since most FAIR plan policyholders secure such coverage only after they have been rejected by the voluntary market. Insurance companies claim to reject insurances based on their past losses (captured in variables theft and fire). The U.S. Commission on Civil Rights in 1979 was interested in how much income, age of housing, and in particular race affect insurance availability.\n\nlibrary(faraway)\ndata(chredlin, package=\"faraway\") # attaches the data from the faraway package\nhead(chredlin)\n\n      race fire theft  age involact income side\n60626 10.0  6.2    29 60.4      0.0 11.744    n\n60640 22.2  9.5    44 76.5      0.1  9.323    n\n60613 19.6 10.5    36 73.5      1.2  9.948    n\n60657 17.3  7.7    37 66.9      0.5 10.656    n\n60614 24.5  8.6    53 81.4      0.7  9.730    n\n60610 54.0 34.1    68 52.6      0.3  8.231    n\n\n\n\n\n\nThe following data set contains about 1/4 million flights that departed from New York City in 2014.\n\nlibrary(data.table)\nflights &lt;- fread(\"https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv\")\n# 1/4 million rows, so subsample\n\nThe variables are self-explanatory in this case\nSince the data has so many rows, let’s sub-sample it.\n\n# flights &lt;- flights[sample(1:dim(flights)[1], 5000),] # base R version of sub-sampling\nlibrary(tidyverse)\nflights &lt;- flights %&gt;%\n  slice_sample(n=5000)                               # tidyverse version of sub-sampling\nhead(flights)\n\n   year month day dep_delay arr_delay carrier origin dest air_time distance\n1: 2014    10  11        23        35      UA    LGA  DEN      238     1620\n2: 2014     6   3        10        47      B6    EWR  MCO      127      937\n3: 2014     2  25        -5       -38      DL    LGA  TPA      144     1010\n4: 2014     8  27        -6       -16      US    EWR  CLT       76      529\n5: 2014     1  18         3         2      UA    EWR  DEN      211     1605\n6: 2014     1  29        97        87      EV    EWR  RIC       53      277\n   hour\n1:    8\n2:   17\n3:   15\n4:   13\n5:    8\n6:   22"
  },
  {
    "objectID": "notes/week_02.html#chicago-redlining",
    "href": "notes/week_02.html#chicago-redlining",
    "title": "Exploring Data with tidyverse",
    "section": "",
    "text": "Insurance redlining refers to the practice of refusing to issue insurance to certain types of people or within some geographic area. The name comes from the act of drawing a red line around an area on a map. Now few would quibble with an insurance company refusing to sell auto insurance to a frequent drunk driver, but other forms of discrimination would be unacceptable.\nIn the late 1970s, the US Commission on Civil Rights examined charges by several Chicago community organizations that insurance companies were redlining their neighborhoods. Because comprehensive information about individuals being refused homeowners insurance was not available, the number of FAIR plan policies written and renewed in Chicago by ZIP code for the months of December 1977 through May 1978 was recorded. The FAIR plan was offered by the city of Chicago as a default policy to homeowners who had been rejected by the voluntary market. Information on other variables that might affect insurance writing such as fire and theft rates was also collected at the ZIP code level. The variables are:\n\ninvolact new FAIR plan policies and renewals per 100 housing units,\nfire fires per 100 housing units,\ntheft thefts per 1000 population,\nrace racial composition in percentage of minority,\nage (of housing) percentage of housing units built before 1939,\nincome median family income in thousands of dollars,\nside north or south side of Chicago.\n\nThe variable involact acts as a measure of insurance availability in the voluntary market, since most FAIR plan policyholders secure such coverage only after they have been rejected by the voluntary market. Insurance companies claim to reject insurances based on their past losses (captured in variables theft and fire). The U.S. Commission on Civil Rights in 1979 was interested in how much income, age of housing, and in particular race affect insurance availability.\n\nlibrary(faraway)\ndata(chredlin, package=\"faraway\") # attaches the data from the faraway package\nhead(chredlin)\n\n      race fire theft  age involact income side\n60626 10.0  6.2    29 60.4      0.0 11.744    n\n60640 22.2  9.5    44 76.5      0.1  9.323    n\n60613 19.6 10.5    36 73.5      1.2  9.948    n\n60657 17.3  7.7    37 66.9      0.5 10.656    n\n60614 24.5  8.6    53 81.4      0.7  9.730    n\n60610 54.0 34.1    68 52.6      0.3  8.231    n"
  },
  {
    "objectID": "notes/week_02.html#flights",
    "href": "notes/week_02.html#flights",
    "title": "Exploring Data with tidyverse",
    "section": "",
    "text": "The following data set contains about 1/4 million flights that departed from New York City in 2014.\n\nlibrary(data.table)\nflights &lt;- fread(\"https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv\")\n# 1/4 million rows, so subsample\n\nThe variables are self-explanatory in this case\nSince the data has so many rows, let’s sub-sample it.\n\n# flights &lt;- flights[sample(1:dim(flights)[1], 5000),] # base R version of sub-sampling\nlibrary(tidyverse)\nflights &lt;- flights %&gt;%\n  slice_sample(n=5000)                               # tidyverse version of sub-sampling\nhead(flights)\n\n   year month day dep_delay arr_delay carrier origin dest air_time distance\n1: 2014    10  11        23        35      UA    LGA  DEN      238     1620\n2: 2014     6   3        10        47      B6    EWR  MCO      127      937\n3: 2014     2  25        -5       -38      DL    LGA  TPA      144     1010\n4: 2014     8  27        -6       -16      US    EWR  CLT       76      529\n5: 2014     1  18         3         2      UA    EWR  DEN      211     1605\n6: 2014     1  29        97        87      EV    EWR  RIC       53      277\n   hour\n1:    8\n2:   17\n3:   15\n4:   13\n5:    8\n6:   22"
  },
  {
    "objectID": "notes/week_02.html#histograms",
    "href": "notes/week_02.html#histograms",
    "title": "Exploring Data with tidyverse",
    "section": "3.1 Histograms",
    "text": "3.1 Histograms\nBasic bar plot showing number of flights for different carriers:\n\nggplot(data = flights) + \n  geom_bar(mapping = aes(x = carrier))\n\n\n\n\nWe can split every bar by departure airport (i.e., variable origin) by adding the fill argument.\n\nggplot(data = flights) + \n  geom_bar(mapping = aes(x = carrier, fill = origin))\n\n\n\n\nIf we wanted one histogram for each NY airport instead, we could use facet_wrap:\n\nggplot(data = flights) + \n  geom_bar(mapping = aes(x = carrier)) +\n  facet_wrap(~ origin)\n\n\n\n\nWhat if we wanted to add numbers denoting the bin sizes?\n\nggplot(data = flights) + \n  geom_bar(mapping = aes(x = carrier)) +\n  stat_count(mapping = aes(x = carrier, label=..count.., y=..count.. + 40), geom=\"text\", size=4, color=\"blue\")\n\n\n\n\nTo show labels for split bars above, one just adds group=origin in the stat_count’s aes, but it is impossible to vertically align the text properly. We bypass it by creating a new variable veralign, which controls the vertical alignment.\n\nn_flights &lt;- flights %&gt;%\n  group_by(origin, carrier) %&gt;%\n  summarise(count = n()) %&gt;% # now we have a table with the hist cell counts\n  ungroup() %&gt;%\n  group_by(carrier) %&gt;%\n  arrange(desc(origin)) %&gt;% # needed because histogram ordered alphabetically from the top\n  mutate(veralign = cumsum(count) - count/2) %&gt;% # here we calculate the vertical alignment and append it to the data set\n  ungroup() # drop the grouping\n\nggplot() + \n  geom_bar(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_text(data=n_flights, mapping = aes(x = carrier, label=count, y=veralign), size=4)\n\n\n\n\nAs long as we are plotting frequencies of occurrence, there is little difference between bar plots and histograms. If the variable on the x-axis is categorical, we speak of bar plots, while if the variable is numerical and some binning has to be done, we speak of histograms. For example:\n\nlibrary(viridis)\nflights %&gt;%\n  ggplot( aes(x=arr_delay_log, fill=origin)) +\n    geom_histogram( color=\"#e9ecef\", alpha=0.6) +\n    scale_fill_viridis(discrete = TRUE) +\n    labs(fill=\"\")\n\n\n\n\nBut we may also scale the y-axis to show probabilities (total area equal to one), instead of frequencies (total area equal to the number of observations). In such a case, histogram can be considered as an estimator of density function (more on that later in the semester). Hence histograms are often overlaid with other density estimators. We will see that later."
  },
  {
    "objectID": "notes/week_02.html#scatterplots",
    "href": "notes/week_02.html#scatterplots",
    "title": "Exploring Data with tidyverse",
    "section": "3.2 Scatterplots",
    "text": "3.2 Scatterplots\nHere we take a look again at the insurance redlining data, where we are interested in the effect of race on the response variable. Let’s start with the marginal relationship of the response on race, which is what we are mostly interested in. We can distinguish\n\nggplot(data = chredlin,\n       mapping = aes(x = race, y = involact, color = side, shape = side)) +\n  geom_point() + scale_color_manual(values = c(\"blue\",\"red\")) +\n  scale_shape_manual(values = c(1,2))\n\n\n\n\n\nggplot(data = chredlin, mapping = aes(x = race, y = involact,\n                                      shape = side, color = side)) +\n  geom_point() + # adds a layer to the empty plot above\n  stat_smooth(method=lm) +# adds a regression line too \n  scale_color_manual(values = c(\"blue\",\"red\")) +\n  scale_shape_manual(values = c(1,2))\n\n\n\n\nThere is probably no difference between north and south w.r.t. the relationship between the response and race, but it might be a different story for fire\n\nggplot(data = chredlin, mapping = aes(x = fire, y = involact, shape = side, color = side)) +\n  geom_point() + # adds a layer to the empty plot above\n  stat_smooth(method=lm)+ # adds a regression line too\n  scale_color_manual(values = c(\"blue\",\"red\")) +\n  scale_shape_manual(values = c(1,2))\n\n\n\n\nWhat if I want only a single regression line but ability to distinguish between points at the same time? I could either use different mappings for every layer\n\nggplot(data = chredlin) +\n  geom_point(mapping = aes(x = fire, y = involact, shape = side, color = side)) +\n  stat_smooth(method=lm, mapping=aes(x = fire, y = involact), color = \"purple\")+\n  scale_color_manual(values = c(\"blue\",\"red\")) +\n  scale_shape_manual(values = c(1,2))\n\n\n\n\nor, equivalently, use one global mapping in ggplot (the most general one) and then specify it a bit more for some layers\n\nggplot(data = chredlin, mapping=aes(x = fire, y = involact)) +\n  geom_point(mapping = aes(shape = side, color = side)) +\n  stat_smooth(method=lm, color = \"purple\")\n\n\n\n\nIf we want to split into multiple plots, use the facet_wrap function\n\nggplot(data = chredlin, mapping = aes(x = fire, y = involact, shape = side, color = side)) +\n  geom_point() +\n  stat_smooth(method=lm, color = \"purple\") +\n  facet_wrap(~ side) # split by a value of a factor\n\n\n\n\nFor faceting by two factor variables, we can use facet_grid instead, such as if we wanted to create scatterplots for our flights data, split by carrier and origin (we filter only some carriers so we don’t have too many plots).\n\nflights %&gt;%\n  filter(carrier %in% c(\"AA\", \"UA\", \"DL\", \"US\")) %&gt;%\n  ggplot(mapping = aes(x = dep_delay, y = arr_delay)) +\n  geom_point() +\n  stat_smooth(method=lm) +\n  facet_grid(origin ~ carrier)\n\n\n\n\nExercise: create histograms of dep_delay for all three origins and seven consecutive days in the flights data set.\nBack to the chredlin data set, we take a closer look at some observations in the scatterplot of involact and fire, that might be outliers or leverage points:\n\noutliers &lt;- c(3,6,35)\nleverage_pts &lt;- c(7,24)\noutliers &lt;- chredlin[outliers,]\nleverage_pts &lt;- chredlin[leverage_pts,]\n\nLet’s now make some scatterplots with the outliers and leverage points highlighted by overploting.\n\nggplot() +\n  geom_point(data = chredlin, mapping = aes(x = fire, y = involact)) +\n  geom_point(data = outliers, mapping = aes(x = fire, y = involact), col = \"blue\", pch = 17, size=3) +\n  geom_point(data = leverage_pts, mapping = aes(x = fire, y = involact), col = \"red\", pch = 18, size=3)\n\n\n\n\nAlternatively, we could incorporate the information about which points are outliers and which are leverage points into the data set as a new variable and use it for plotting. The code is quite ugly (notice how we start with booleans, then turn them into numericals, and then re-type them into factors), but it does the job.\n\noutliers &lt;- I(1:nrow(chredlin) %in% c(3,6,35))\nleverage_pts &lt;- I(1:nrow(chredlin) %in% c(7,24))\nchredlin &lt;- chredlin %&gt;%\n  mutate(out_lev =  as.factor(outliers + 2*leverage_pts))\nlevels(chredlin$out_lev) &lt;- c(\"normal obs\", \"outlier\", \"leverage point\")\n\nggplot(data = chredlin) +\n  geom_point(mapping = aes(x = fire, y = involact, shape = out_lev, color = out_lev, size = out_lev)) +\n  scale_size_manual(values = c(2,4,4))\n\n\n\n\nAnother alternative would be to use fortify() to add residuals and Cook’s distances from a linear model fit directly to the data set. Then we could use mutate() more naturally to decide which observations are outliers and leverage points."
  },
  {
    "objectID": "notes/week_02.html#boxplots",
    "href": "notes/week_02.html#boxplots",
    "title": "Exploring Data with tidyverse",
    "section": "3.3 Boxplots",
    "text": "3.3 Boxplots\nSimilarly to histograms, boxplots can give us some idea about distributions. Unlike histograms, boxplots do not really capture an underlying density shape, but only visually give several summary statistics and points in the tails.\n\nflights %&gt;%\n  filter(carrier %in% c(\"AA\", \"B6\", \"DL\", \"EV\")) %&gt;%\n  ggplot( aes(x=carrier, y= arr_delay_log, fill=carrier)) +\n    geom_boxplot() +\n    scale_fill_viridis(discrete = TRUE) +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"Boxplot by factor\") +\n    xlab(\"Carrier\")\n\n\n\n\nUsing the fill argument here creates a grouped boxplot, where grouping is given by the fill variable. To reduce the size of the plot, we filter only four carriers.\n\nflights %&gt;%\n  filter(carrier %in% c(\"AA\", \"B6\", \"DL\", \"EV\")) %&gt;%\n  ggplot( aes(x=carrier, y= arr_delay_log, fill=origin)) +\n    geom_boxplot() +\n    scale_fill_viridis(discrete = TRUE) +\n    theme(\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"Boxplot by two factors\") +\n    xlab(\"Carrier\")"
  },
  {
    "objectID": "notes/week_02.html#multiple-plots",
    "href": "notes/week_02.html#multiple-plots",
    "title": "Exploring Data with tidyverse",
    "section": "3.4 Multiple Plots",
    "text": "3.4 Multiple Plots\nThere are numerous ways how to display multiple plots. The default is to set par(mfrow = c(n_rows, n_cols)) before plotting, an alternative is the lattice package, but these do not work with ggplot. With ggplot, one can utilize ggarrange() from the ggpubr package. However, these will require you to do a lot of copy-pasting since you still need to create plots individually, but sometimes we want to create numerous plots similar in nature. To this point, the ggplot2’s facet_wrap function allows you to split into numerous plots by a value of a factor.\nBut what if we wish to create one plot for every variable? This is doable with facet_wrap provided we have:\n\nall the values (in our data frame) given as a single variable\nanother variable linking the original variables to the values.\n\nThis is exactly what we get by using pivot_longer() below. The argument everything() specifies we want to keep all the variables, otherwise we could use the variable names like with select() (just here, multiple variables have to be wrapped in c() due to the function’s syntax) to keep only some or discard some variables. pivot_longer() will give us a long-format data frame with two columns: value and name. Then we plot value and wrap the facets based on name.\n\ndata(chredlin, package=\"faraway\")\nchredlin %&gt;% mutate(side = as.numeric(side)) %&gt;% \n  pivot_longer(everything()) %&gt;%\n  ggplot(aes(value)) + facet_wrap(~ name, scales = \"free\") + geom_histogram()\n\n\n\n\nThe complement of pivot_longer() is pivot_wider(). We may want to use it, e.g., after group_by() and summarize(), which naturally lead to a long table. For example, assume we want to produce a heatmap of the average departure delay of flights on different days (x-axis) and months (y-axis), in order to see average departure delay in a calendar-like fashion. While this is not terribly useful, it is done below.\n\nnew_dat &lt;- flights %&gt;% \n  group_by(month, day) %&gt;%\n  summarize(count = mean(dep_delay)) %&gt;%\n  pivot_wider(names_from = day, values_from = count) %&gt;%\n  ungroup() %&gt;% select(-month)\n\nlibrary(lattice)\nlibrary(viridisLite)\ncoul &lt;- plasma(100)\nlevelplot(t(as.matrix(new_dat)), col.regions = rev(coul), xlab=\"Day\", ylab=\"Month\", main=\"Departure delay\")\n\n\n\n\nWhile we could create a heatmap using ggplot2’s geom_tile(), I personally find levelplot() of the lattice package more useful for quick plotting. One can do great heatmaps with ggplot2 and geom_tile(), but it requires some work to set up the color schemes and to magnify the colorkey.\nWhile the previous heatmap is not so useful, we will later see that heatmaps are quite handy e.g. when probing performance of a certain method based on two parameters."
  },
  {
    "objectID": "notes/week_02.html#marginal-scatterplots-with-regression-lines-and-outliers",
    "href": "notes/week_02.html#marginal-scatterplots-with-regression-lines-and-outliers",
    "title": "Exploring Data with tidyverse",
    "section": "3.5 Marginal Scatterplots with Regression Lines and Outliers",
    "text": "3.5 Marginal Scatterplots with Regression Lines and Outliers\nIf we fit a linear model to the response variable involact with all available variables (except side) as covariates, we notice that there are some outliers and leverage points (looking at the Cook’s distance). Wouldn’t it be nice to add these into the marginal scatterplots directly to see which scatter points belong to the outliers?\nFirstly, If we wish to have scatterplots with the response variable involact on the y-axes, we naturally need one more column with involact values in the long format. This is actually done automatically by dropping involact from pivoting.\n\nchredlin %&gt;% mutate(side = as.numeric(side), income=log(income)) %&gt;% \n  pivot_longer(-involact) %&gt;%\n  ggplot(aes(y = involact, x = value)) + facet_wrap(~ name, scales = \"free\") +\n  geom_point() + stat_smooth(method=lm)\n\n\n\n\nSecondly, let’s denote the outliers. We will use the second extra-variable approach (as opposed to the over-plotting approach). We create an additional variable which tells us, which of the observations are outliers.\n\noutliers &lt;- I(1:nrow(chredlin) %in% c(3,6,35))\nleverage_pts &lt;- I(1:nrow(chredlin) %in% c(7,24))\nchredlin &lt;- chredlin %&gt;%\n  mutate(out_lev =  as.factor(outliers + 2*leverage_pts))\nlevels(chredlin$out_lev) &lt;- c(\"normal obs\", \"outlier\", \"leverage point\")\n\nchredlin %&gt;% mutate(side = as.numeric(side), income=log(income)) %&gt;% \n  pivot_longer(cols=c(-involact, -out_lev)) %&gt;%\n  ggplot() + facet_wrap(~ name, scales = \"free\") +\n  geom_point(mapping = aes(y = involact, x = value, color = out_lev)) +\n  stat_smooth(mapping = aes(y = involact, x = value), method=lm)"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "Help & FAQ",
    "section": "",
    "text": "Tip\n\n\n\nIf you do not find the answer to your question here, you can use the search bar on the top left to search the entire site. If you are still unable to find the answer, you can ask your question during the exercise hours. If you found the solution and think it would be useful to other students, you can report an issue (see on the right) with the questions and the solution, and we will add it to the FAQ."
  },
  {
    "objectID": "course-faq.html#faq",
    "href": "course-faq.html#faq",
    "title": "Help & FAQ",
    "section": "FAQ",
    "text": "FAQ\n\nOrganizational\n\nWhich language will we be using?\nYou can use either Python,R, or Julia. We will be providing code examples and suppport in all three languages.\n\n\nWhat software do I need to install?\nYou will need to use git, and a text editor or IDE.\nFor git, please see here for instructions on how to install it.\nYou can use any kind of text editor or IDE you like. We recomment using VS Code with the extension corresponding to your language of choice. You can also use Rstudio for R. See here for instructions on how to install the software and IDEs.\n\n\nHow do I submit an assignment?\nWe will be using Github-classroom to distribute and collect assignments. You will need to create a Github account if you don’t already have one.\n\n\n\nTechnical issues\n\nI am lost with Github, what should I do?\nGitHub has possibly the best tutorials and documentation of any software out there. You can find the Hello world there. Most of the time googling your issue will lead you to the right place.\n\n\nI have bugs in my code/ something doesn’t work/ I don’t know how to do something, what should I do?\nFirst google your issue. 90 times out of 100 this will solve your issues. Another 9% can be resolved by reading the documentation of the software you are using. For the very last percent you can ask your question during the exercises hours.\n\n\nI cannot find the repository linked to the assignment, what should I do?\nAt the top of each assignment there should be a link you can manually click to accept the assignment. If this happens after the first assignment, please let us know."
  }
]