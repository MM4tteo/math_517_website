[
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "This is the homepage for MATH-517 Statistical computation and visualisation in Fall 2023 at EPFL. All course materials will be posted on this site.\nYou can find the course syllabus here and the course schedule here. All the additional ressources can be found either on this page or by using the search bar on the top left corner of the website. These ressources include supplementary material, tutorials, tips and tricks, etc…"
  },
  {
    "objectID": "course-overview.html#class-meetings",
    "href": "course-overview.html#class-meetings",
    "title": "Course overview",
    "section": "Class meetings",
    "text": "Class meetings\n\n\n\n\n\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLectures\nGCD 03 86\nFri 10:15 - 12:00\n\n\nExercises\nGCD 03 86\nFri 13:15 - 15:00"
  },
  {
    "objectID": "course-overview.html#assignments",
    "href": "course-overview.html#assignments",
    "title": "Course overview",
    "section": "Assignments",
    "text": "Assignments\nWe will use GitHub Classroom to handle the assignments for this course. See this page for more information on how to use GitHub Classroom."
  },
  {
    "objectID": "course-overview.html#license",
    "href": "course-overview.html#license",
    "title": "Course overview",
    "section": "License",
    "text": "License\n\nThis online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\nThis website and part of the course materials where adapted from different sources:\n\nDr. Mine Çetinkaya-Rundel MATH 517 website\nfredhutch.io and R for data analysis and visualization of Ecological Data"
  },
  {
    "objectID": "ae/ae4.html",
    "href": "ae/ae4.html",
    "title": "Exploratory Data Analysis II",
    "section": "",
    "text": "lab 1 due tonight at 11:59pm on Gradescope\nbe sure to complete prepare material (on the schedule) before each class"
  },
  {
    "objectID": "ae/ae4.html#bulletin",
    "href": "ae/ae4.html#bulletin",
    "title": "Exploratory Data Analysis II",
    "section": "",
    "text": "lab 1 due tonight at 11:59pm on Gradescope\nbe sure to complete prepare material (on the schedule) before each class"
  },
  {
    "objectID": "ae/ae4.html#today",
    "href": "ae/ae4.html#today",
    "title": "Exploratory Data Analysis II",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nLearn and practice the big 7 dplyr verbs"
  },
  {
    "objectID": "ae/ae4.html#getting-started",
    "href": "ae/ae4.html#getting-started",
    "title": "Exploratory Data Analysis II",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae4.qmd\",\n  destfile = \"ae4.qmd\")"
  },
  {
    "objectID": "ae/ae4.html#load-packages-and-data",
    "href": "ae/ae4.html#load-packages-and-data",
    "title": "Exploratory Data Analysis II",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nType ?palmerpenguins to learn more about this package. Or better yet, check it out here.\n\ndata(penguins)"
  },
  {
    "objectID": "ae/ae4.html#a-package-within-a-package",
    "href": "ae/ae4.html#a-package-within-a-package",
    "title": "Exploratory Data Analysis II",
    "section": "A package within a package…",
    "text": "A package within a package…\nWhen we load the tidyverse library, dplyr is packaged with it.\ndplyr, a grammar of data manipulation offers intuitive ‘verb’ functions that describe actions we commonly want to perform with data. The big 7 we’ll cover today are:\n\nmutate() adds new variables (columns) that are functions of existing variables\nselect() picks variables based on their names.\nfilter() picks rows based on their values in specific columns.\ngroup_by() sets us up to summarize across groups\nsummarize() reduces multiple values down to a single summary.\narrange() changes the ordering of the rows.\nslice() select, remove and duplicate rows based on their index\n\n(as described at https://dplyr.tidyverse.org/)\n\nCheck out documentation with ?\n\n\nMutate\n\nmutate() adds new variables (columns) that are functions of existing variables\n\nApproximate bill area (in \\(m^2\\)) as bill length * bill depth:\n\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm)\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_area_mm2 &lt;dbl&gt;\n\n\n\n\nSelect\n\nselect() picks variables (columns) based on their names.\n\nIt’s hard to see bill length, depth and area in the same output, let’s select a smaller subset of the variables to look at.\n\n# Example 1\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm) %&gt;%\n  select(-c(year, species, island, flipper_length_mm, body_mass_g, sex))\n\n# A tibble: 344 × 3\n   bill_length_mm bill_depth_mm bill_area_mm2\n            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1           39.1          18.7          731.\n 2           39.5          17.4          687.\n 3           40.3          18            725.\n 4           NA            NA             NA \n 5           36.7          19.3          708.\n 6           39.3          20.6          810.\n 7           38.9          17.8          692.\n 8           39.2          19.6          768.\n 9           34.1          18.1          617.\n10           42            20.2          848.\n# ℹ 334 more rows\n\n# Example 2\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm) %&gt;%\n  select(bill_length_mm, bill_depth_mm, bill_area_mm2)\n\n# A tibble: 344 × 3\n   bill_length_mm bill_depth_mm bill_area_mm2\n            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1           39.1          18.7          731.\n 2           39.5          17.4          687.\n 3           40.3          18            725.\n 4           NA            NA             NA \n 5           36.7          19.3          708.\n 6           39.3          20.6          810.\n 7           38.9          17.8          692.\n 8           39.2          19.6          768.\n 9           34.1          18.1          617.\n10           42            20.2          848.\n# ℹ 334 more rows"
  },
  {
    "objectID": "ae/ae4.html#filter",
    "href": "ae/ae4.html#filter",
    "title": "Exploratory Data Analysis II",
    "section": "Filter",
    "text": "Filter\n\nfilter() picks rows based on their values in specific columns.\n\nLet’s just examine penguins on Dream island\n\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm) %&gt;%\n  select(-year)\n# code here"
  },
  {
    "objectID": "ae/ae4.html#group-by-summarize",
    "href": "ae/ae4.html#group-by-summarize",
    "title": "Exploratory Data Analysis II",
    "section": "Group by + Summarize",
    "text": "Group by + Summarize\n\ngroup_by() sets us up to summarize across groups\nsummarize() reduces multiple values down to a single summary.\n\n\nExercise 2:\nFind mean bill area across sex. Fill in the blanks\n\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm) %&gt;%\n  select(-year) %&gt;%\n  # filter for Dream\n  group_by(___) %&gt;%\n  summarize(mean_bill_area_mm2 = ___)"
  },
  {
    "objectID": "ae/ae4.html#arrange-slice",
    "href": "ae/ae4.html#arrange-slice",
    "title": "Exploratory Data Analysis II",
    "section": "Arrange + Slice",
    "text": "Arrange + Slice\n\narrange() changes the ordering of the rows.\nslice() select, remove and duplicate rows based on their index\n\nLet’s use arrange() and slice() to report the five penguins with the greatest bill area.\n\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm) %&gt;%\n  select(bill_area_mm2, bill_length_mm) %&gt;%\n  arrange(desc(bill_area_mm2))\n\n# A tibble: 344 × 2\n   bill_area_mm2 bill_length_mm\n           &lt;dbl&gt;          &lt;dbl&gt;\n 1         1127.           54.2\n 2         1105.           55.8\n 3         1076.           52  \n 4         1065.           53.5\n 5         1056            52.8\n 6         1050.           51.7\n 7         1043.           52.7\n 8         1032.           58  \n 9         1021.           51.3\n10         1013.           59.6\n# ℹ 334 more rows\n\n\n\nExercise 3:\nAre these the same five penguins with the longest bills?\nOptional hint: if you want to be exactly precise about which penguins are which, you could add an ID column, e.g.\n\n penguins %&gt;%\n  mutate(id = seq(1:nrow(penguins)))\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, id &lt;int&gt;\n\n\nThis takes advantage of the nrow() function. Can you guess what it returns?\n\n\nExercise 4:\nCompute the average bill length, bill depth, flipper length and body mass across all islands.\n\n# code here\n\n\n\nOptional bonus:\nIs every species on every island?\n\n# code here"
  },
  {
    "objectID": "ae/ae25.html",
    "href": "ae/ae25.html",
    "title": "Cryptanalysis",
    "section": "",
    "text": "Office hours after class today\nLab 09 due tonight. Draft peer-report due Friday (in lab review)\ncourse evaluations open. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project"
  },
  {
    "objectID": "ae/ae25.html#bulletin",
    "href": "ae/ae25.html#bulletin",
    "title": "Cryptanalysis",
    "section": "",
    "text": "Office hours after class today\nLab 09 due tonight. Draft peer-report due Friday (in lab review)\ncourse evaluations open. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project"
  },
  {
    "objectID": "ae/ae25.html#getting-started",
    "href": "ae/ae25.html#getting-started",
    "title": "Cryptanalysis",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae25.qmd\",\ndestfile = \"ae25.qmd\")"
  },
  {
    "objectID": "ae/ae25.html#load-packages",
    "href": "ae/ae25.html#load-packages",
    "title": "Cryptanalysis",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(reshape2)"
  },
  {
    "objectID": "ae/ae25.html#background",
    "href": "ae/ae25.html#background",
    "title": "Cryptanalysis",
    "section": "Background",
    "text": "Background\nThis application exercise builds on the R code found here and The Markov Chain Monte Carlo Revolution by Persi Diaconis.\n\nLet’s load the data\n\nwarandpeace = readLines(\"https://sta101.github.io/static/appex/data/warandpeace.txt\")\nfrequency = read.table(\"https://sta101.github.io/static/appex/data/frequencies.txt\")\ncolnames(frequency) = c(toupper(letters), \"\") # edit column names\nsecret_message = readLines(\"https://sta101.github.io/static/appex/data/secret-message.txt\")"
  },
  {
    "objectID": "ae/ae25.html#exercise-1",
    "href": "ae/ae25.html#exercise-1",
    "title": "Cryptanalysis",
    "section": "Exercise 1",
    "text": "Exercise 1\nTake a look at secret-message. Each letter is a stand in for exactly one other letter. This sort of cipher is known as a “substitution cipher”.\n‘A’ could be encoded as one of 26 characters (A, B, C, …). Once the encoding for ‘A’ is chosen, ‘B’ has 25 possibilities and so on so there are, in total \\(26 \\times 25 \\times 24 \\times \\ldots \\times 3 \\times 2 \\times 1\\) possibilities.\n\nn = 26\ninput = n + 1\nkeys = gamma(input)\nkeys\n\n[1] 4.032915e+26\n\n\nThat’s over \\(4 \\times 10^{26}\\) possible keys! If you could check 10M keys per second, it would take approximately \\(1 \\times 10^{12}\\) (trillion) years to check every possible key. Trying every possible key is known as a “brute force” approach.\n\nChat with your neighbor and develop a strategy better than the brute force approach. Detail your strategy below."
  },
  {
    "objectID": "ae/ae25.html#exercise-2",
    "href": "ae/ae25.html#exercise-2",
    "title": "Cryptanalysis",
    "section": "Exercise 2",
    "text": "Exercise 2\nHere we determine how often one character follows another using the text from the very long book, War and Peace. We include a whitespace character as a 27th character in our alphabet.\nTo reduce computational demand, we will load the object created by this analysis but leave the code below for reference.\nThe result of the below analysis is in the object frequency.\nThe letter row denotes the first character in a 2 character sequence while columns determine the second character in the character sequence.\n\nWhat should be the sum of a row? Check this for the first row of the data frame.\n\n\n# code here\n\nCreate a heatmap of character frequency, where the y-axis is the first letter and the x-axis is the second letter in a two letter chain.\n\nUncomment and complete the code below.\n\nHint: We’ll use melt from the reshape2 package. Click here for an example.\n\nmelt_freq = melt(as.matrix(frequency))\n\n# melt_freq %&gt;%\n#   ggplot(aes(x = __, y = __, fill = __))"
  },
  {
    "objectID": "ae/ae25.html#mcmc",
    "href": "ae/ae25.html#mcmc",
    "title": "Cryptanalysis",
    "section": "MCMC",
    "text": "MCMC\nWe will use a famous statistical algorithm, Markov chain Monte Carlo (MCMC) to break the substitution cipher.\nMCMC is composed of three essential components:\n\nAbility. A way to propose any possible key.\nFeedback. A way to evaluate how good a given key is.\nCuriosity. A way to leave a good key for a worse key.\n\n\nThe analogy of the blind monkey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere’s a monkey on an island with many ponds. The monkey has been tasked with finding the largest body of water on the island. The only trouble is, he is blind. In order to find the largest body of water, he throws rocks randomly and listens for a splash. If he hears a splash, he knows there’s a body of water where he last threw. He continues to throw more rocks in that direction to find out how big the pool is. Occasionally he gets bored and wanders off to look for another pond. In this way, he uses the MCMC algorithm to find the largest body of water.\n\n\nMaking the connection.\n\nAbility (throw a rock). The Monkey can walk around and throw the rock anywhere on the island, ensuring that given enough time, he will cover every inch of the island.\nFeedback (test for waters). Every time the Monkey throws the rock, he receives feedback by listening for a splash. A large splash means a deep pond and encourages him to continue throwing in that direction to figure out the perimeter of the pond.\nCuriosity (boredom). If the monkey finds the second largest pond on the island, he might get stuck throwing rocks in it for a long time. By occasionally walking away from a large pond, he will reach the largest pond quicker."
  },
  {
    "objectID": "ae/ae25.html#exercise-3",
    "href": "ae/ae25.html#exercise-3",
    "title": "Cryptanalysis",
    "section": "Exercise 3",
    "text": "Exercise 3\nLet’s write out together what MCMC looks like when decrpyting a secret message.\n\nAbility\nFeedback\nCuriosity"
  },
  {
    "objectID": "ae/ae25.html#exercise-4",
    "href": "ae/ae25.html#exercise-4",
    "title": "Cryptanalysis",
    "section": "Exercise 4",
    "text": "Exercise 4\nHere we load some functions that will help us decode the message.\nRun the code below to break the secret message. If the message is unintelligible after several iterations, you may try re-starting with a new seed. What is this equivalent to in the monkey analogy above?"
  },
  {
    "objectID": "ae/ae25.html#exercise-5",
    "href": "ae/ae25.html#exercise-5",
    "title": "Cryptanalysis",
    "section": "Exercise 5",
    "text": "Exercise 5\nTry your own message!\nCreate your own message in the code below and call mcmcAttack(coded) on your message to decode it!\n\nYou might think about what makes the message easy or difficult to attack, e.g. does length of the message affect its susceptibility to attack? What else might?"
  },
  {
    "objectID": "ae/ae-01-unvotes.html",
    "href": "ae/ae-01-unvotes.html",
    "title": "UN Votes",
    "section": "",
    "text": "How do various countries vote in the United Nations General Assembly, how have their voting patterns evolved throughout time, and how similarly or differently do they view certain issues? Answering these questions (at a high level) is the focus of this analysis.\n\n\nWe will use the tidyverse, lubridate, and scales packages for data wrangling and visualization, and the DT package for interactive display of tabular output, and the unvotes package for the data.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DT)\nlibrary(unvotes)\nlibrary(ggthemes)\n\n\n\n\nThe data we’re using originally come from the unvotes package. In the chunk below we modify the data by joining the various data frames provided in the package to help you get started with the analysis.\n\nunvotes &lt;- un_votes |&gt;\n  inner_join(un_roll_calls, by = \"rcid\") |&gt;\n  inner_join(un_roll_call_issues, by = \"rcid\", relationship = \"many-to-many\")"
  },
  {
    "objectID": "ae/ae-01-unvotes.html#introduction",
    "href": "ae/ae-01-unvotes.html#introduction",
    "title": "UN Votes",
    "section": "",
    "text": "How do various countries vote in the United Nations General Assembly, how have their voting patterns evolved throughout time, and how similarly or differently do they view certain issues? Answering these questions (at a high level) is the focus of this analysis.\n\n\nWe will use the tidyverse, lubridate, and scales packages for data wrangling and visualization, and the DT package for interactive display of tabular output, and the unvotes package for the data.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DT)\nlibrary(unvotes)\nlibrary(ggthemes)\n\n\n\n\nThe data we’re using originally come from the unvotes package. In the chunk below we modify the data by joining the various data frames provided in the package to help you get started with the analysis.\n\nunvotes &lt;- un_votes |&gt;\n  inner_join(un_roll_calls, by = \"rcid\") |&gt;\n  inner_join(un_roll_call_issues, by = \"rcid\", relationship = \"many-to-many\")"
  },
  {
    "objectID": "ae/ae-01-unvotes.html#un-voting-patterns",
    "href": "ae/ae-01-unvotes.html#un-voting-patterns",
    "title": "UN Votes",
    "section": "UN voting patterns",
    "text": "UN voting patterns\nLet’s create a data visualization that displays how the voting record of the UK & NI changed over time on a variety of issues, and compares it to two other countries: US and Turkey.\nWe can easily change which countries are being plotted by changing which countries the code above filters for. Note that the country name should be spelled and capitalized exactly the same way as it appears in the data. See the Appendix for a list of the countries in the data.\n\nunvotes |&gt;\n  filter(country %in% c(\"United Kingdom\", \"United States\", \"Turkey\")) |&gt;\n  mutate(year = year(date)) |&gt;\n  group_by(country, year, issue) |&gt;\n  summarize(percent_yes = mean(vote == \"yes\")) |&gt;\n  ggplot(mapping = aes(x = year, y = percent_yes, color = country)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  facet_wrap(~issue) +\n  scale_y_continuous(labels = percent) +\n  scale_color_colorblind() +\n  labs(\n    title = \"Percentage of 'Yes' votes in the UN General Assembly\",\n    subtitle = \"1946 to 2019\",\n    y = \"% Yes\",\n    x = \"Year\",\n    color = \"Country\"\n  )"
  },
  {
    "objectID": "ae/ae-01-unvotes.html#references",
    "href": "ae/ae-01-unvotes.html#references",
    "title": "UN Votes",
    "section": "References",
    "text": "References\n\nRobinson D (2021). unvotes: United Nations General Assembly Voting Data. R package version 0.3.0, https://github.com/dgrtwo/unvotes.\nErik Voeten “Data and Analyses of Voting in the UN General Assembly” Routledge Handbook of International Organization, edited by Bob Reinalda (published May 27, 2013).\nMuch of the analysis has been modeled on the examples presented in the unvotes package vignette."
  },
  {
    "objectID": "ae/ae-01-unvotes.html#appendix",
    "href": "ae/ae-01-unvotes.html#appendix",
    "title": "UN Votes",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of countries in the dataset:"
  },
  {
    "objectID": "ae/ae11.html",
    "href": "ae/ae11.html",
    "title": "Prediction",
    "section": "",
    "text": "Lab 04 now due Friday October 7\nFriday is last day for project group swaps"
  },
  {
    "objectID": "ae/ae11.html#bulletin",
    "href": "ae/ae11.html#bulletin",
    "title": "Prediction",
    "section": "",
    "text": "Lab 04 now due Friday October 7\nFriday is last day for project group swaps"
  },
  {
    "objectID": "ae/ae11.html#today",
    "href": "ae/ae11.html#today",
    "title": "Prediction",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nbe able to make new predictions from your fitted linear models\nvisualize the fit of your model"
  },
  {
    "objectID": "ae/ae11.html#getting-started",
    "href": "ae/ae11.html#getting-started",
    "title": "Prediction",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae11.qmd\",\ndestfile = \"ae11.qmd\")"
  },
  {
    "objectID": "ae/ae11.html#load-packages-and-data",
    "href": "ae/ae11.html#load-packages-and-data",
    "title": "Prediction",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae11.html#notes",
    "href": "ae/ae11.html#notes",
    "title": "Prediction",
    "section": "Notes",
    "text": "Notes\n\nPrediction\npredict() is a powerful function that takes two arguments:\n\nyour model fit\nnew data you want to make predictions from\n\nThere are several ways you can use the predict() function.\nFor standard linear regression,\npredict(model_fit, test_data) # returns predicted outcome\nFor logistic regression you can use the code above to obtain the predicted outcome (0 or 1) or alternatively use one of the formulations below to quickly grab the log-odds or the probability of the outcome “1”.\npredict(model_fit$fit, test_data) # returns log-odds\npredict(model_fit, test_data, type = \"prob\") # returns probability of a 1."
  },
  {
    "objectID": "ae/ae11.html#practice",
    "href": "ae/ae11.html#practice",
    "title": "Prediction",
    "section": "Practice",
    "text": "Practice\nLoad data:\n\nparkinsons_train = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/parkinsons_train.csv\")\nparkinsons_test = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/parkinsons_test.csv\")\n\nThis dataset comes from Little et al. (2008). The data includes various measurements of dysphonia (disorders of the voice) from 32 people, 24 with Parkinson’s disease (PD). Multiple measurements were taken per individual. The measurements we examine in this subset of the data include:\n\nname: patient ID\njitter: a measure of relative variation in fundamental frequency\nshimmer: a measure of variation in amplitude (dB)\nPPE: pitch period entropy\nHNR: a ratio of total components vs. noise in the voice recording\nstatus: health status (1 for PD, 0 for healthy)\n\n\nExercise 1\nWrite down a main effects model to predict Parkinson’s status from HNR, shimmer, jitter and PPE.\n\n\nExercise 2\nFit your model from the previous exercise using the parkinsons_train data set.\n\n# code here\n\n\n\nExercise 3\nUse your model to predict PD status in the parkinsons_test data set with a decision boundary of p = 0.5. How many false positives do you observe? How many false negatives?\nNext change the decision boundary to 0.25. How many false positives and false negatives do you observe?\nWhich decision boundary do you prefer?\n\n# code here\n\n\n\nVisualizing model fits\nThere are many ways you can visualize a fitted model. Plotting the hyperplane is limited to simple two-variable (predictor, outcome) and three-variable (predictor, predictor, outcome) plots. Here we explore some useful visualizations for high-dimensional multivariate models.\n\nExample: logistic regression\n\nCreate a stacked bar plot with status on the x-axis and fill by whether or not the predicted status is correct or incorrect.\n\n\n# code here\n\n\n\nExample: ordinary least squares regression:\nScenario: we are trying to predict vocal amplitude variation (shimmer) from jitter and pitch period entropy (PPE).\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\] where\n\\(y\\): shimmer \\(x_1\\): jitter \\(x_2\\): PPE\n\nmyPredictiveModel = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(shimmer ~ jitter + PPE, data = parkinsons_train)\n\nEven if we don’t have a test data set, we could still create a new column of predictions like before:\n\n# predict based on new data\npredict_train = parkinsons_train %&gt;%\n  mutate(myPrediction = predict(myPredictiveModel, parkinsons_train)$.pred)\n\nFrom here we can plot \\(\\hat{y}\\) vs \\(y\\):\n\npredict_train %&gt;%\n  ggplot(aes(x = shimmer, y = myPrediction)) +\n  geom_point() +\n  labs(x = \"True Shimmer (dB)\", y = \"Predicted shimmer (dB)\", title = \"Predicted vs true shimmer values\") +\n  geom_abline(slope = 1, intercept = 0, color = \"steelblue\")\n\n\n\n\nAlternatively, we could create a residual plot. Residual plots can be used to assess whether a linear model is appropriate.\nA common assumption of linear regression models is that the error term, \\(\\epsilon\\), has constant variance everywhere.\n\nIf the linear model is appropriate, a residual plot should show this.\nPatterned or nonconstant residual spread may sometimes be indicative a model is missing predictors or missing interactions.\n\n\n\nExercise 4\nCreate a new column residuals in predict_train and save your data frame as predict_train_2\n\n# code here\n\n\npredict_train_2 %&gt;%\n  ggplot(aes(x = myPrediction, y = residuals)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(x = \"Predicted shimmer (dB)\", y = \"Residual\")"
  },
  {
    "objectID": "ae/ae5.html",
    "href": "ae/ae5.html",
    "title": "EDA III: Joins",
    "section": "",
    "text": "Lab 02 due Thursday"
  },
  {
    "objectID": "ae/ae5.html#bulletin",
    "href": "ae/ae5.html#bulletin",
    "title": "EDA III: Joins",
    "section": "",
    "text": "Lab 02 due Thursday"
  },
  {
    "objectID": "ae/ae5.html#today",
    "href": "ae/ae5.html#today",
    "title": "EDA III: Joins",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\njoin data frames"
  },
  {
    "objectID": "ae/ae5.html#getting-started",
    "href": "ae/ae5.html#getting-started",
    "title": "EDA III: Joins",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae5.qmd\",\n  destfile = \"ae5.qmd\")"
  },
  {
    "objectID": "ae/ae5.html#load-packages",
    "href": "ae/ae5.html#load-packages",
    "title": "EDA III: Joins",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "ae/ae5.html#joins",
    "href": "ae/ae5.html#joins",
    "title": "EDA III: Joins",
    "section": "Joins",
    "text": "Joins\nThere are six join functions in the dplyr package. Join functions take two data frames as arguments and return a data frame.\nThe six joins can be broken down into two categories:\n\nMutating joins: joining data frames results in mutating new columns\nFiltering joins: joining data frames results in filtering observations in one data frame based on another.\n\nIn all of the following examples, x and y are two data frames:\n\nx = tibble(value = c(100, 200, 300),\n            xcol = c(\"x1\", \"x2\", \"x3\"))\ny = tibble(value = c(100, 200, 400),\n            ycol = c(\"y1\", \"y2\", \"y4\"))\nx\n\n# A tibble: 3 × 2\n  value xcol \n  &lt;dbl&gt; &lt;chr&gt;\n1   100 x1   \n2   200 x2   \n3   300 x3   \n\ny\n\n# A tibble: 3 × 2\n  value ycol \n  &lt;dbl&gt; &lt;chr&gt;\n1   100 y1   \n2   200 y2   \n3   400 y4   \n\n\n\nMutating joins\nThe most popular 2 joins:\n\nleft_join(x, y): keep all rows from x and adds columns from y\nright_join(x, y): keeps all rows from y and adds columns from x\n\nTwo more helpful joins:\n\ninner_join(x, y): join all rows from x where there are matching values in y.Returns all combinations in case of multiple matches\nfull_join(x, y): include all rows in x or y\n\nToy examples:\n\nx %&gt;%\n  left_join(y)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 3 × 3\n  value xcol  ycol \n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   100 x1    y1   \n2   200 x2    y2   \n3   300 x3    &lt;NA&gt; \n\n\n\nWhat do you think Joining, by = \"value\" means?\n\n\nx %&gt;%\n  right_join(y, by = \"value\")\n\n# A tibble: 3 × 3\n  value xcol  ycol \n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   100 x1    y1   \n2   200 x2    y2   \n3   400 &lt;NA&gt;  y4   \n\n\n\nx %&gt;%\n  inner_join(y)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 2 × 3\n  value xcol  ycol \n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   100 x1    y1   \n2   200 x2    y2   \n\n\n\nfull_join(x, y)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 4 × 3\n  value xcol  ycol \n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   100 x1    y1   \n2   200 x2    y2   \n3   300 x3    &lt;NA&gt; \n4   400 &lt;NA&gt;  y4   \n\n\n\n\nFiltering joins\n\nsemi_join(x, y): return all rows from x with match in y\nanti_join(x, y): return all rows from x without a match in y\n\nIn both of these “filtering” cases we do not add any new columns to our first argument (the data frame x).\nToy examples:\n\nx %&gt;%\nsemi_join(y)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 2 × 2\n  value xcol \n  &lt;dbl&gt; &lt;chr&gt;\n1   100 x1   \n2   200 x2   \n\n\n\nx %&gt;%\nanti_join(y)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 1 × 2\n  value xcol \n  &lt;dbl&gt; &lt;chr&gt;\n1   300 x3   \n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can manually specify which columns to join by and the columns do not have to have the same name! See the example below.\n\n\nCheck out the new data frame x2:\n\nx2 = x %&gt;%\n  mutate(new_value = value) %&gt;%\n  select(new_value, xcol)\n\nx2\n\n# A tibble: 3 × 2\n  new_value xcol \n      &lt;dbl&gt; &lt;chr&gt;\n1       100 x1   \n2       200 x2   \n3       300 x3   \n\n\nWe can still join x2 with y but left_join(x2, y) won’t work. We have to manually specify which columns to join by:\n\nx2 %&gt;%\n  left_join(y, by = c(\"new_value\" = \"value\"))\n\n# A tibble: 3 × 3\n  new_value xcol  ycol \n      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1       100 x1    y1   \n2       200 x2    y2   \n3       300 x3    &lt;NA&gt;"
  },
  {
    "objectID": "ae/ae5.html#practice",
    "href": "ae/ae5.html#practice",
    "title": "EDA III: Joins",
    "section": "Practice",
    "text": "Practice\nWe’ll take a look at some New York flights data.\nThis data set contains on-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013.\nThis data is a subset of the data set provided by the nycflights13 package.\n\nCodebook\nflights2 contains\n\nyear: departure year\ntime_hour: departure time\ndep_delay: departure delay in minutes\narr_delay: arrival delay in minutes\norigin: origin\ndest: destination\ncarrier two letter carrier abbreviation\n\nairlines contains\n\ncarrier: two letter carrier abbrevation\nname: full carrier name\n\nairports contains\n\nfaa: FAA airport code\nname: name of airport\nlat: latitude\nlon: longitude\n\n\nflights = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/flights.csv\")\n\nRows: 10000 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): origin, dest, tailnum, carrier\ndbl  (3): year, dep_delay, arr_delay\ndttm (1): time_hour\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nairlines = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/airlines.csv\")\n\nRows: 16 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): carrier, name\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nairports = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/airports.csv\")\n\nRows: 1458 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): faa, name\ndbl (2): lat, lon\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nExercise 1\nflights contains the two letter carrier abbreviations and airlines contains a dictionary. It would be nice if this information was in one data frame so we could read off easily e.g. which airlines are most likely to have arrival delays.\nUse an appropriate join to add the column of airlines to flights and save the resulting data frame as flights2.\n\n# code here\n\n\n\nExercise 2\nUsing flights2, report the average arrival delay for each carrier. Print only the 5 carrier airline with the worst arrival delays on average. No joins required here.\n\n# code here\n\n\n\nExercise 3\nCreate a new data set called dest_delays that reports the median arrival delay at each destination airport.\n\n# code here\n\n\n\nExercise 4\nWhich destination has the worst arrival delay? (Note: you will need to join dest_delays with airports to answer this question)\n\n# code here\n\n\n\nOptional bonus\n\nIs there anything else you might want to learn from the data before declaring one airport is most likely to have delayed arrival or one carrier is most likely to result in a delayed flight?"
  },
  {
    "objectID": "ae/ae13.html",
    "href": "ae/ae13.html",
    "title": "The Normal Distribution",
    "section": "",
    "text": "Mid-semester grades posted\nRegression project due Saturday October 15"
  },
  {
    "objectID": "ae/ae13.html#bulletin",
    "href": "ae/ae13.html#bulletin",
    "title": "The Normal Distribution",
    "section": "",
    "text": "Mid-semester grades posted\nRegression project due Saturday October 15"
  },
  {
    "objectID": "ae/ae13.html#today",
    "href": "ae/ae13.html#today",
    "title": "The Normal Distribution",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ndefine stable distribution, normal distribution, location and scale parameters\nplot normal distributions and calculate probabilities"
  },
  {
    "objectID": "ae/ae13.html#getting-started",
    "href": "ae/ae13.html#getting-started",
    "title": "The Normal Distribution",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae13.qmd\",\ndestfile = \"ae13.qmd\")"
  },
  {
    "objectID": "ae/ae13.html#load-packages",
    "href": "ae/ae13.html#load-packages",
    "title": "The Normal Distribution",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae13.html#normal-distribution",
    "href": "ae/ae13.html#normal-distribution",
    "title": "The Normal Distribution",
    "section": "Normal distribution",
    "text": "Normal distribution\nThe normal distribution, also known as “Gaussian distribution” is a distribution of a continuous random variable. The sample space of a normal random variable is \\(\\{- \\infty, + \\infty \\}\\) and is defined by two parameters: a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\). The mean is known as the location parameter while the standard deviation is the scale.\nWe can sample N times from a normal with mean mu and standard deviation sigma using rnorm(n = N, mean = mu, sd = sigma).\n\nset.seed(123)\n# example\nsample = rnorm(1000, mean = 0, sd = 1)\n\nmean(sample)\nsd(sample)\n\nhist(sample)\n\nLet’s visualize the normal function curve using the code below.\n\nmu1 = 100\ns1 = 6\n\nmu2 = 105\ns2 = 3\n\nggplot(data = data.frame(x = c(mu1 - s1*3, mu1 + s1*3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = mu1, sd = s1),\n                color = \"steelblue\") +\n  stat_function(fun = dnorm, args = list(mean = mu2, sd = s2),\n                color = \"orange\") + \n  theme_bw() +\n  labs(title = \"Two normal curves\")\n\n\n\n\n\nExercise 1\nTry setting mu2 to 99 and s2 to 4. What do you notice? Play with a few more settings and describe what the mean and standard deviation do to the shape of the curve.\n\n\nExercise 2\nStart with the code below of a histogram but change your geometry to geom_histogram(aes(y = ..density..)). This will re-scale your histogram so that the area under the curve is 1. Next, use the stat_function code above as a template to superimpose a normal distribution on top of your histogram. Adjust the mean and standard deviation until you obtain a good looking fit. What do you notice?\n\nset.seed(1013)\nx = rbinom(n = 10000, size = 100, prob = 0.8)\ndf = data.frame(x) \n\ndf %&gt;%\n  ggplot(aes(x = x)) + \n  geom_histogram(bins = 35) +\n  theme_bw() +\n  labs(x = \"X\", title = \"Distribution of heads in 100 biased coin flips\")"
  },
  {
    "objectID": "ae/ae13.html#properties-of-a-gaussian",
    "href": "ae/ae13.html#properties-of-a-gaussian",
    "title": "The Normal Distribution",
    "section": "Properties of a Gaussian",
    "text": "Properties of a Gaussian\nIf \\(X\\) is a random variable, and \\(X\\) is normally distributed, then it the distribution of \\(X\\) is fully specified by the location (mean) and scale (standard deviation) parameters, \\(\\mu\\) and \\(\\sigma\\) respectively. In mathematical notation we write\n\\[\nX \\sim N(\\mu, \\sigma)\n\\] and this reads: “X is normally distributed with mean mu and standard deviation sigma.” It is worth noting that in some contexts (see e.g. “notation” on wikipedia) the second term of a Normal distribution refers to the variance or \\(\\sigma^2\\).\nA very useful property of Normal distribution is that it is stable. What this means is that linear combinations of normal random variables are themselves normal. In other words, if \\(X\\) and \\(Y\\) are normal random variables then \\(aX + bY\\) is normal for all \\(a\\) and \\(b\\). The two properties to remember when adding Gaussian random variables are:\n\nmean(X + Y) = mean(X) + mean(Y)\nvariance(X + Y) = variance(X) + variance(Y) when X and Y are independent\n\nWe can see this in an example.\nLet \\(X \\sim N(5, 3)\\) and let \\(Y \\sim N(-5, 1)\\)\nAccording to our rules above \\(X + Y \\sim N(0, \\sqrt{10})\\). Let’s check ourselves with code:\n\nset.seed(1)\nnormal_df = data.frame(X = rnorm(1000, mean = 5, sd = 3),\n                 Y = rnorm(1000, mean = -5, sd = 1))\n\n\n\nnormal_df = normal_df %&gt;%\n  mutate(Z = X + Y)\n\nnormal_df %&gt;%\n  ggplot(aes(x = Z)) +\n  geom_histogram(aes(y = ..density..), fill = 'gold3', alpha = 0.5) +\n  theme_bw() +\n  labs(y = \"Density\", title = \"Matching densities\") +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sqrt(10)),\n                color = \"darkblue\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nExercise 3\nLet \\(Z \\sim N(0, 1)\\) and \\(X \\sim N(10, 4)\\)\n\\(aZ + b \\stackrel{d}{=} X\\) for some \\(a\\) and \\(b\\). What are \\(a\\) and \\(b\\)? Fill them in for the ? in the code below and the uncomment the code. Here “\\(\\stackrel{d}{=}\\) means”equal in distribution”.\n\nset.seed(1)\nnormal_df = data.frame(z = rnorm(1000, mean = 0, sd = 1),\n                 x = rnorm(1000, mean = 10, sd = 4))\n\nsample_mean = normal_df %&gt;%\n  summarize(sample_mean = mean(x)) %&gt;%\n  pull()\n\nnormal_df %&gt;%\n  ggplot(aes(x = x)) +\n  geom_histogram(aes(y = ..density..), fill = 'gold3', alpha = 0.5) +\n  geom_histogram(aes(x = z, y = ..density..), fill = 'steelblue', alpha = 0.7) +\n  #geom_density(aes(x = z*? + ?), color = 'red') + \n  theme_bw() +\n  labs(x = \"\", y = \"\", title = \"Sampling from two normals\")"
  },
  {
    "objectID": "ae/ae13.html#computing-probabilities",
    "href": "ae/ae13.html#computing-probabilities",
    "title": "The Normal Distribution",
    "section": "Computing probabilities",
    "text": "Computing probabilities\npnorm “probability normal” takes three arguments:\n\nq, mean and sd\n\nand pnorm(q = q, mean = mu, sd = sigma) answers the question:\nIf \\(X \\sim N(\\mu, \\sigma)\\), what is \\(p(X &lt; q)\\)?\nFor example, imagine that the resting heart rates in the classroom are normally distributed with mean 70 beats per minute (bpm) and standard deviation 5 bpm. What’s the probability a randomly selected individual has a heart rate less than 63 bpm?\nIn math: let \\(X\\) be the bpm of an individual in this class. Assume \\(X \\sim N(70, 5)\\). What is \\(p(X &lt; 63)\\)? Given heartbeats are normally distributed, randomly selecting an individual from the classroom is called “drawing from a normal distribution”.\nWe can compute this easily:\n\npnorm(63, 70, 5)\n\n[1] 0.08075666\n\n\n0.08 or about 8% chance. In picture, the probability is the proportion of area under the curve shaded:\n\n\n\n\n\n\nExercise 4\nWhat is the probability a randomly selected student in the class has a heart beat greater than 75 bpm?\n\n# code here"
  },
  {
    "objectID": "ae/ae6.html",
    "href": "ae/ae6.html",
    "title": "Simple Regression",
    "section": "",
    "text": "lab 02 due tonight\nexam 01 next week"
  },
  {
    "objectID": "ae/ae6.html#bulletin",
    "href": "ae/ae6.html#bulletin",
    "title": "Simple Regression",
    "section": "",
    "text": "lab 02 due tonight\nexam 01 next week"
  },
  {
    "objectID": "ae/ae6.html#today",
    "href": "ae/ae6.html#today",
    "title": "Simple Regression",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nunderstand the grammar of linear modeling, including \\(y\\), \\(x\\), \\(\\beta\\), \\(\\epsilon\\), fitted estimates and residuals\nadd linear regression plots to your 2D graphs\nbe able to write a simple linear regression model mathematically and\nfit the model to data in R in a tidy way"
  },
  {
    "objectID": "ae/ae6.html#getting-started",
    "href": "ae/ae6.html#getting-started",
    "title": "Simple Regression",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae6.qmd\",\ndestfile = \"ae6.qmd\")"
  },
  {
    "objectID": "ae/ae6.html#load-packages-and-data",
    "href": "ae/ae6.html#load-packages-and-data",
    "title": "Simple Regression",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nToday’s data is Apple and Microsoft stock prices from January 1st 2020 to December 31st 2021. I pulled this data off the Yahoo finance using their API via the tidyquant package July 2022.\n\nstocks = read_csv(\"https://sta101.github.io/static/appex/data/stocks1.csv\")"
  },
  {
    "objectID": "ae/ae6.html#notes",
    "href": "ae/ae6.html#notes",
    "title": "Simple Regression",
    "section": "Notes",
    "text": "Notes\n\nThe simple regression model and notation\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\n\n\\(y\\): the outcome variable. Also called the “response” or “dependent variable”. In prediction problems, this is what we are interested in predicting.\n\\(x\\): the predictor. Also commonly referred to as “regressor”, “independent variable”, “covariate”, “feature”, “the data”.\n\\(\\beta_0\\), \\(\\beta_1\\) are called “constants” or coefficients. They are fixed numbers. These are population parameters. \\(\\beta_0\\) has another special name, “the intercept”.\n\\(\\epsilon\\): the error. This quantity represents observational error, i.e. the difference between our observation and the true population-level expected value: \\(\\beta_0 + \\beta_1 x\\).\n\nEffectively this model says our data \\(y\\) is linearly related to \\(x\\) but is not perfectly observed due to some error.\n\n\nA simple example\nLet’s examine January 2020 open prices of Microsoft and Apple stocks to illustrate some ideas.\n\nstocks_subset = stocks %&gt;%\n  slice(1:21)\n\nstocks_subset %&gt;%\n  ggplot(aes(x = MSFT.Open, y = AAPL.Open)) +\n  geom_point() + \n  labs(x = \"MSFT Open\", y = \"AAPL Open\", title = \"Open prices of MSFT and AAPL January 2020\") +\n  theme_bw() \n\n\n\n  # more code here\n\n\nExercise 1\nAdd geom_abline() to the above plot and try different slopes and intercepts until you find a trendline you are satisfied with. The equation below describes your fitted model. Re-write the equation below, filling in \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) with your estimates.\n\\[\n\\hat{y} = \\hat{\\beta_0} +  \\hat{\\beta_1} x\n\\]\n\n\\(\\hat{y}\\) is the expected outcome.\n\\(\\hat{\\beta}\\) is the estimated or fitted coefficient\nthere is no error term here because we do not predict error\n\nThe equation of my line above:\n\\[\n\\text{[your equation here]}\n\\]\nThe central idea is that if we measure every \\(x\\) and every \\(y\\) in existence, (“the entire population”) there is some true “best” \\(\\beta_0\\) and \\(\\beta_1\\) that describe the relationship between \\(x\\) and \\(y\\). Since we only have a sample of the data, we estimate \\(\\beta_0\\) and \\(\\beta_1\\). We call our estimates \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) “beta hat”. We never have all the data, thus we never can really know what the true \\(\\beta\\)s are."
  },
  {
    "objectID": "ae/ae6.html#ordinary-least-squares-ols-regression",
    "href": "ae/ae6.html#ordinary-least-squares-ols-regression",
    "title": "Simple Regression",
    "section": "Ordinary least squares (OLS) regression",
    "text": "Ordinary least squares (OLS) regression\n\nThe residuals\nFor any linear equation we write down, there will be some difference between the predicted outcome of our linear model (\\(\\hat{y}\\)) and what we observe (\\(y\\))… (But of course! Otherwise everything would fall on a perfect straight line!)\nThis difference between what we observe and what we predict \\(y - \\hat{y}\\) is known as a residual \\(r\\).\nMore concisely,\n\\[\nr = y - \\hat{y}\n\\]\nResiduals are dependent on the line we draw. Visually, here is a model of the data, \\(y = -5 + \\frac{1}{2}x\\) and 1 of the residuals is outlined in red.\n\n\n\n\n\nThere is, in fact, a residual associated with every single point in the plot.\n\npredictAAPL = function(x) {\n  return(-5 + (0.5*x))\n}\n\nxPoints = stocks$MSFT.Open[1:21]\nyPoints = stocks$AAPL.Open[1:21]\nyHat = predictAAPL(xPoints)\n\nstocks_subset %&gt;%\n  ggplot(aes(x = MSFT.Open, y = AAPL.Open)) +\n  geom_point() + \n  labs(x = \"MSFT Open\", y = \"AAPL Open\", title = \"Open prices of MSFT and AAPL January 2020\") +\n  theme_bw() +\n  geom_abline(slope = 0.5, intercept = -5) +\n  geom_segment(x = xPoints, xend = xPoints, y  = yPoints, yend = yHat, color = 'red')\n\n\n\n\nWe often wish to find a line that fits the data “really well”, but what does this mean? Well, we want small residuals! So we pick an objective function. That is, a function we wish to minimize or maximize.\n\n\nThe objective function\n\nExercise 2\nAt first, you might be tempted to minimize \\(\\sum_i r_i\\), but this is problematic. Why? Can you come up with a better solution (other than the one listed below)?\n[answer here]\nIn practice, we minimize the sum of squared residuals:\n\\[\n\\sum_i r_i^2\n\\]\nNote, this is the same as\n\\[\n\\sum_i (y_i - \\hat{y})^2\n\\]\n\n\nExercise 3\nCheck out an interactive visualization of “least squares regression” here. Click on I and drag the points around to get started. Describe what you see.\n[response here]\n\n\nExercise 4\n\nCheck for understanding\n\nHow far off is your model (from exercise 1) from the actual observed data on January 11 2020? The observed value is MSFT: $164.35 and AAPL: $78.4. Compute the single square residual using your fitted model from exercise 1.\n\n# code here"
  },
  {
    "objectID": "ae/ae6.html#plotting-the-ols-regression-line",
    "href": "ae/ae6.html#plotting-the-ols-regression-line",
    "title": "Simple Regression",
    "section": "Plotting the OLS regression line",
    "text": "Plotting the OLS regression line\nPlotting the OLS regression line, that is, the line that minimizes the sum of square residuals is very easy with ggplot. Simply add\ngeom_smooth(method = 'lm', se = F)\nto your plot.\nmethod = lm says to draw a line according to a “linear model” and se = F turns off standard error bars. You can try without these options for comparison.\nOptionally, you can change the color of the line, e.g.\ngeom_smooth(method = 'lm', se = F, color = 'red')\n\nExercise 5\nCopy your code from exercise 1 below. Add geom_smooth() as described above with color = 'steelblue' to see how close your line is.\n\n# code here"
  },
  {
    "objectID": "ae/ae6.html#finding-hatbeta",
    "href": "ae/ae6.html#finding-hatbeta",
    "title": "Simple Regression",
    "section": "Finding \\(\\hat{\\beta}\\)",
    "text": "Finding \\(\\hat{\\beta}\\)\nTo fit the model in R, i.e. to “find \\(\\hat{\\beta}\\)”, use the code below as a template:\nmodelFit = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y-variable-here ~ x-variable-here, data = data-frame-here)\n\nlinear_reg tells R we will perform linear regression\nset_engine tells R to use the standard linear modeling (lm) machinery\nfit defines the outcome \\(y\\), predictor \\(x\\) and the data set\n\nRunning the code above, but replacing the arguments of the fit command appropriately will create a new object called “modelFit” (defined on the first line) that stores all information about your fitted model.\nTo access the information, you can run, e.g.\ntidy(modelFit)\nLet’s try it out.\n\nExercise 6\nFind the OLS fitted linear model \\(y = \\hat{\\beta_0} + \\hat{\\beta_1} x\\) for January 2020, where \\(x\\) is Microsoft’s opening price and \\(y\\) is Apple’s opening price. Print your results to the screen\n\n# code here\n\n\n\nExercise 7\nRe-write the fitted equation replacing \\(\\beta_0\\) and \\(\\beta_1\\) with the OLS fitted values.\n\\[\n\\text{[your equation here]}\n\\]"
  },
  {
    "objectID": "ae/ae15.html",
    "href": "ae/ae15.html",
    "title": "The bootstrap",
    "section": "",
    "text": "Lab 05 due tonight"
  },
  {
    "objectID": "ae/ae15.html#bulletin",
    "href": "ae/ae15.html#bulletin",
    "title": "The bootstrap",
    "section": "",
    "text": "Lab 05 due tonight"
  },
  {
    "objectID": "ae/ae15.html#today",
    "href": "ae/ae15.html#today",
    "title": "The bootstrap",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nbe able to draw a bootstrap sample and calculate a bootstrap statistic\nuse infer to obtain a bootstrap distribution\ncalculate a confidence interval from the bootstrap distribution\ninterpret a confidence interval in context of the data"
  },
  {
    "objectID": "ae/ae15.html#getting-started",
    "href": "ae/ae15.html#getting-started",
    "title": "The bootstrap",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae15.qmd\",\ndestfile = \"ae15.qmd\")"
  },
  {
    "objectID": "ae/ae15.html#load-packages",
    "href": "ae/ae15.html#load-packages",
    "title": "The bootstrap",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae15.html#load-data",
    "href": "ae/ae15.html#load-data",
    "title": "The bootstrap",
    "section": "Load data",
    "text": "Load data\n\nmanhattan = read_csv(\n  \"https://sta101-fa22.netlify.app/static/appex/data/manhattan.csv\"\n  )"
  },
  {
    "objectID": "ae/ae15.html#notes-for-reference",
    "href": "ae/ae15.html#notes-for-reference",
    "title": "The bootstrap",
    "section": "Notes (for reference)",
    "text": "Notes (for reference)\nBootstrapping is a re-sampling technique. The key idea is you have already collected a sample of size \\(N\\) from the population. To create a bootstrap sample, you sample with replacement from your original sample \\(N\\) times.\nLet’s say you measure the height of five Duke students in meters:\n\nheights = c(1.51, 1.62, 1.89, 2.01, 1.78)\n\nstudents = data.frame(heights)\n\nThere are many ways to create a bootstrap sample in R. We will focus on the tidy way below. which uses the infer package that loads with tidymodels.\n\nExample\n\nset.seed(2)\nstudents %&gt;%\n  specify(response = heights) %&gt;%\n  generate(reps = 1, type = \"bootstrap\")\n\nResponse: heights (numeric)\n# A tibble: 5 × 2\n# Groups:   replicate [1]\n  replicate heights\n      &lt;int&gt;   &lt;dbl&gt;\n1         1    1.78\n2         1    1.51\n3         1    1.78\n4         1    1.51\n5         1    2.01\n\n\n\n\n\n\n\n\nNote\n\n\n\nSampling is random. Notice the seed above ensures we get the same bootstrap sample.\n\n\nFrom here, we can compute a bootstrap statistic. E.g.\n\nset.seed(2)\nstudents %&gt;%\n  specify(response = heights) %&gt;%\n  generate(reps = 1, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"median\")\n\nResponse: heights (numeric)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  1.78"
  },
  {
    "objectID": "ae/ae15.html#example-rent-in-manhattan",
    "href": "ae/ae15.html#example-rent-in-manhattan",
    "title": "The bootstrap",
    "section": "Example: rent in Manhattan",
    "text": "Example: rent in Manhattan\nOn a given day in 2018, twenty one-bedroom apartments were randomly selected on Craigslist Manhattan from apartments listed as “by owner”. The data are in the manhattan data frame. We will use this sample to conduct inference on the typical rent of 1 bedroom apartments in Manhattan.\n\nPart 1: Drawing a bootstrap sample\nLet’s start by using bootstrapping to estimate the mean rent of one-bedroom apartments in Manhattan.\n\nExercise 1\nWhat is a point estimate (i.e. single number summary) of the typical rent?\n\n\nExercise 2\nLet’s bootstrap!\n\nTo bootstrap we will sample with replacement by drawing a value from the box.\nHow many draws do we need for our bootstrap sample?\n\nFill in the values from the bootstrap sample conducted in class. Once the values are filled in, un-comment the code.\n\n# class_bootstrap = c()\n\n\n\nExercise 3\n\nAbout what value do you expect the bootstrap statistic to take?\nCalculate the statistic from the bootstrap sample.\n\n\n# add code\n\n\n\n\nPart 2: Bootstrap confidence interval\nWe will calculate a 95% confidence interval for the mean rent of one-bedroom apartments in Manhattan.\nWe start by setting a seed to ensure our analysis is reproducible.\n\nGenerating the bootstrap distribution\nWe can use R to take many bootstrap samples, compute a statistic and then view the bootstrap distribution of that statistic.\nUn-comment the lines and fill in the blanks to create the bootstrap distribution of sample means and save the results in the data frame boot_dist.\nUse 1000 reps for the in-class activity. (You will use about 10,000 reps for assignments outside of class.)\n\nset.seed(7182022)\n\nboot_dist = manhattan #%&gt;%\n  #specify(______) %&gt;%\n  #generate(______) %&gt;%\n  #calculate(______)\n\n\nHow many rows are in boot_dist?\nWhat does each row represent?\nWhat are the variables in boot_dist? What do they mean?\n\n\n\nVisualize the bootstrap distribution\nA sample statistic is a random variable, we can look at its distribution.\nVisualize the bootstrap distribution using a histogram. Describe the shape and center of the distribution.\n\n# add code\n\n\n\nCalculate the confidence interval\nUncomment the lines and fill in the blanks to construct the 95% bootstrap confidence interval for the mean rent of one-bedroom apartments in Manhattan.\n\n#___ %&gt;%\n#  summarize(lower = quantile(______),\n  #          upper = quantile(______))\n\n\n\nInterpret the interval\nWrite the interpretation for the interval calculated above.\n\nQuestion: Does a confidence interval have to be symmetric?\nWhat is one advantage to using a 90% confidence interval instead of a 95% confidence interval to estimate a parameter? - What is one advantage to using a 99% confidence interval instead of a 95% confidence interval to estimate a parameter?"
  },
  {
    "objectID": "ae/ae-02-flint.html",
    "href": "ae/ae-02-flint.html",
    "title": "Exploring Flint’s water data",
    "section": "",
    "text": "By the end of this application exercise you will\n\nmeet the computational toolkit for the course\npractice using glimpse(), names(), nrow(), ncol(), count()\ndefine and compute various statistics\nbegin to gain familiarity with making data visualizations with ggplot()\n\nWe will do this using water lead content data from Flint, MI."
  },
  {
    "objectID": "ae/ae-02-flint.html#rstudio",
    "href": "ae/ae-02-flint.html#rstudio",
    "title": "Exploring Flint’s water data",
    "section": "RStudio",
    "text": "RStudio\n\nFiles, plots, viewer, environment, etc. panes\nConsole\nEditor"
  },
  {
    "objectID": "ae/ae-02-flint.html#r",
    "href": "ae/ae-02-flint.html#r",
    "title": "Exploring Flint’s water data",
    "section": "R",
    "text": "R\n\nWriting code in the console\nBasic math with R\nCreating variables in R, the assignment operator (&lt;-), and the Environment pane\nR functions and packages and the Packages pane\nGetting help with R and the Help pane"
  },
  {
    "objectID": "ae/ae-02-flint.html#quarto",
    "href": "ae/ae-02-flint.html#quarto",
    "title": "Exploring Flint’s water data",
    "section": "Quarto",
    "text": "Quarto\n\nYAML: Metadata\nNarrative: Edited with the visual editor (or the source editor)\nCode: In code chunks\n\nChunk options (following #|)\nComments (following #)\nCode\n\nRunning individual code chunks vs. rendering a document"
  },
  {
    "objectID": "ae/ae-02-flint.html#load-packages",
    "href": "ae/ae-02-flint.html#load-packages",
    "title": "Exploring Flint’s water data",
    "section": "Load packages",
    "text": "Load packages\nWe’ll use the tidyverse package for analysis, which offers functionality for data import, wrangling, visualization, and more.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nLoading this package prints out a message. What does this message mean? How can we suppress the message from the output?"
  },
  {
    "objectID": "ae/ae-02-flint.html#load-data",
    "href": "ae/ae-02-flint.html#load-data",
    "title": "Exploring Flint’s water data",
    "section": "Load data",
    "text": "Load data\nThe read_csv() function can be used for reading CSV (comma separated values) files. The file we’re reading is called flint with the suffix (.csv) which indicates its file type. The file is in the data folder.\nBefore reading in the file, go to the data folder in the Files pane to confirm that it is, indeed, there. Then, read the file by running the code chunk below by clicking on the green triangle icon on the code chunk.\n\nflint &lt;- read_csv(\"data/flint.csv\")\n\nRows: 813 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): draw\ndbl (4): id, zip, ward, lead\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOne of two things may have happened:\n\nThe file was read successfully and you now see a dataset called flint in your Environment pane.\nThe file was not read successfully and you see an error Error in read_csv(\"data/flint.csv\") : could not find function \"read_csv\".\n\nIf (1) happened, great!\nIf (2) happened, let’s troubleshoot first before continuing."
  },
  {
    "objectID": "ae/ae-02-flint.html#data-dictionary",
    "href": "ae/ae-02-flint.html#data-dictionary",
    "title": "Exploring Flint’s water data",
    "section": "Data dictionary",
    "text": "Data dictionary\nThe following variables are in the flint data frame:\n\nid: sample ID number (identifies the home)\nzip: ZIP code in Flint of the sample’s location\nward: ward in Flint of the sample’s location\ndraw: which time point the water was sampled from\nlead: lead content in parts per billion"
  },
  {
    "objectID": "ae/ae-02-flint.html#populations-and-samples",
    "href": "ae/ae-02-flint.html#populations-and-samples",
    "title": "Exploring Flint’s water data",
    "section": "Populations and samples",
    "text": "Populations and samples\nWe want to learn about the population using a sample.\nIn the case we want to learn about the lead content in all of Flint, MI homes but only have available water readings from a sample of homes (our data set).\nExercise 1: Look at the data, how many observations are there? How many variables?\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-flint.html#frequencies",
    "href": "ae/ae-02-flint.html#frequencies",
    "title": "Exploring Flint’s water data",
    "section": "Frequencies",
    "text": "Frequencies\nLet’s count() to find the number of different time points water was sampled with the count() function.\n\nThe first argument is flint: the data frame\nThe second argument is draw: the variable\n\n\ncount(flint, draw)\n\n# A tibble: 3 × 2\n  draw       n\n  &lt;chr&gt;  &lt;int&gt;\n1 first    271\n2 second   271\n3 third    271\n\n\nWe can achieve the same result with the following “piped” operation as well.\n\nThe first line is flint: the data frame\nThen the pipe operator, read as “and then”, which places what comes before it as the first argument of what comes after it\nThe second line is count(draw)\n\n\nflint |&gt;\n  count(draw)\n\n# A tibble: 3 × 2\n  draw       n\n  &lt;chr&gt;  &lt;int&gt;\n1 first    271\n2 second   271\n3 third    271\n\n\nWe can use a similar approach to fund out how many unique homes are in the data set:\n\nflint |&gt;\n  count(id)\n\n# A tibble: 269 × 2\n      id     n\n   &lt;dbl&gt; &lt;int&gt;\n 1     1     3\n 2     2     3\n 3     4     3\n 4     5     3\n 5     6     3\n 6     7     3\n 7     8     3\n 8     9     3\n 9    12     3\n10    13     3\n# ℹ 259 more rows\n\n\nExercise 2: How many samples were taken from each zip code?\n\n# add code here\n\nExercise 3: Which ZIP code had the most samples drawn? Hint: See the help for count.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-flint.html#measures-of-central-tendency",
    "href": "ae/ae-02-flint.html#measures-of-central-tendency",
    "title": "Exploring Flint’s water data",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\n\nmean\nmedian\nmode"
  },
  {
    "objectID": "ae/ae-02-flint.html#measures-of-spread",
    "href": "ae/ae-02-flint.html#measures-of-spread",
    "title": "Exploring Flint’s water data",
    "section": "Measures of spread",
    "text": "Measures of spread\n\nvariance\nstandard deviation\nrange\nquartiles\ninter-quartile range (IQR)"
  },
  {
    "objectID": "ae/ae-02-flint.html#order-statistics",
    "href": "ae/ae-02-flint.html#order-statistics",
    "title": "Exploring Flint’s water data",
    "section": "Order statistics",
    "text": "Order statistics\n\nquantiles\nminimum (0 percentile)\nmedian (50th percentile)\nmaximum (100 percentile)\n\n… and any other arbitrary function of the data you can come up with!\nExercise 4: Compute each of these statistics for lead ppb.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-flint.html#histograms",
    "href": "ae/ae-02-flint.html#histograms",
    "title": "Exploring Flint’s water data",
    "section": "Histograms",
    "text": "Histograms\nLet’s take a look at the distribution of lead content in homes in Flint, MI.\n\nggplot(flint, aes(x = lead)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can make this plot look nicer/more useful by adjusting the number of bins and zooming into the x-axis.\n\nggplot(flint, aes(x = lead)) +\n  geom_histogram(bins = 50) +\n  coord_cartesian(xlim = c(0, 100))\n\n\n\n\nLet’s visualize some of our summary statistics on the plot.\nExercise 5: Add a new layer, geom_vline(xintercept = __, color = \"red\"), to the histogram below, filling in the blank with the mean.\n\nggplot(flint, aes(x = lead)) + \n  geom_histogram(bins = 50) + \n  coord_cartesian(xlim = c(0, 100))\n\n\n\n\nExercise 6: Add one more layer which overlays the median, in a different color.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-flint.html#box-plots",
    "href": "ae/ae-02-flint.html#box-plots",
    "title": "Exploring Flint’s water data",
    "section": "Box plots",
    "text": "Box plots\nNext, let’s narrow our focus to the zip codes 48503, 48504, 48505, 48506, and 48507 and observations with lead values less than 1,000 ppb.\n\nflint_focus &lt;- flint |&gt;\n  filter(zip %in% 48503:48507 & lead &lt; 1000)\n\nExercise 7: Below are side-by-side box plots for the three flushing times in each of the five zip codes we considered. Add x and y labels; add a title by inserting title = \"title_name\" inside the labs() function.\n\nggplot(data = flint_focus, aes(y = factor(zip), x = lead)) +\n  geom_boxplot(aes(fill = factor(draw))) +\n  labs(x = \"___\", y = \"___\", fill = \"Flushing time\") +\n  scale_fill_discrete(\n    breaks = c(\"first\", \"second\", \"third\"),\n    labels = c(\"0 (sec)\", \"45 (sec)\", \"120 (sec)\")\n  )\n\n\n\n\nExercise 8: Add labels for x, y, a title, and subtitle to the code below to update the corresponding plot.\n\nggplot(data = flint_focus, aes(y = factor(zip), x = lead)) +\n  geom_boxplot(aes(fill = factor(draw))) + \n  labs(\n    x = \"___\", y = \"___\", fill = \"Flushing time\",\n    title = \"___\",\n    subtitle = \"___\"\n    ) +\n  scale_fill_discrete(\n    breaks = c(\"first\", \"second\", \"third\"),\n    labels = c(\"0 (sec)\", \"45 (sec)\", \"120 (sec)\")\n  ) +\n  coord_cartesian(xlim = c(0, 50)) +\n  theme_bw()\n\n\n\n\nExercise 9: What is the difference between the two plots? What are the advantages and disadvantages to each plot?\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae16.html",
    "href": "ae/ae16.html",
    "title": "Central limit theorem",
    "section": "",
    "text": "Lab 6 due Thursday\nProject proposal due Friday"
  },
  {
    "objectID": "ae/ae16.html#bulletin",
    "href": "ae/ae16.html#bulletin",
    "title": "Central limit theorem",
    "section": "",
    "text": "Lab 6 due Thursday\nProject proposal due Friday"
  },
  {
    "objectID": "ae/ae16.html#today",
    "href": "ae/ae16.html#today",
    "title": "Central limit theorem",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nUse Central Limit Theorem to define distribution of sample means\nCalculate probabilities from the normal distribution\nUse Central Limit Theorem (CLT) to conduct inference on a population mean"
  },
  {
    "objectID": "ae/ae16.html#getting-started",
    "href": "ae/ae16.html#getting-started",
    "title": "Central limit theorem",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae16.qmd\",\ndestfile = \"ae16.qmd\")"
  },
  {
    "objectID": "ae/ae16.html#load-packages",
    "href": "ae/ae16.html#load-packages",
    "title": "Central limit theorem",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae16.html#load-data",
    "href": "ae/ae16.html#load-data",
    "title": "Central limit theorem",
    "section": "Load data",
    "text": "Load data\n\nmanhattan = read_csv(\n  \"https://sta101-fa22.netlify.app/static/appex/data/manhattan.csv\"\n  )"
  },
  {
    "objectID": "ae/ae16.html#notes",
    "href": "ae/ae16.html#notes",
    "title": "Central limit theorem",
    "section": "Notes",
    "text": "Notes\nHow do we know when to expect a normal distribution to show up?\nLast time we saw an example where the distribution of sample means looked approximately normal but the distribution of sample medians was not.\nExample:\n\nset.seed(1)\nboot_dist = manhattan %&gt;%\n  specify(response = rent) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\")\n\nboot_dist %&gt;%\n  calculate(stat = \"mean\") %&gt;%\n  visualize() +\n  labs(x = \"Sample mean\", \n       title = \"Simulated distribution of the sample mean\")\n\n\n\nboot_dist %&gt;%\n  calculate(stat = \"median\") %&gt;%\n  visualize() + \n  labs(x = \"Sample median\",\n       title = \"Simulated distribution of the sample median\")\n\n\n\n\nAre there times when the sample mean will not look normal?\n\nExercise 1\nThe proportion of observed successes for a binary variable is a sample mean.\nScenario: You flip a biased coin numFlips times and compute the sample mean (the proportion of flips that land heads). You repeat this experiment 1000 times and obtain a distribution of sample means.\n\nHow does the shape of the distribution change as you increase the number of coin flips per sample?\n\n\nset.seed(714)\nnumFlips = 1\nnumHeads = rbinom(n = 1000, size = numFlips, prob = 0.9)\ndf = data.frame(numHeads) # new data frame called df\ndf %&gt;%\nmutate(propHeads = numHeads / numFlips) %&gt;%\nggplot(aes(x = propHeads)) +\ngeom_histogram(binwidth = .01)"
  },
  {
    "objectID": "ae/ae16.html#what-is-the-central-limit-theorem",
    "href": "ae/ae16.html#what-is-the-central-limit-theorem",
    "title": "Central limit theorem",
    "section": "What is the central limit theorem?",
    "text": "What is the central limit theorem?\nThe central limit theorem is a statement about the distribution of the sample mean, \\(\\bar{x}\\).\nThe central limit theorem guarantees that, when certain criteria are satisfied, the sample mean (\\(\\bar{x}\\)) is normally distributed.\nSpecifically, if\n\nObservations in the sample are independent. Two rules of thumb to check this:\n\ncompletely random sampling\nif sampling without replacement, sample should be less than 10% of the population size\n\n\nand\n\nThe sample is large enough. The required size varies in different contexts, but some good rules of thumb are:\n\nif the population itself is normal, sample size does not matter.\nif numerical require, &gt;30 observations\nif binary outcome, at least 10 successes and 10 failures.\n\n\nthen\n\\[\n\\bar{x} \\sim N(\\mu, \\sigma / \\sqrt{n})\n\\]\ni.e. \\(\\bar{x}\\) is normally distributed (unimodal and symmetric with bell shape) with mean \\(\\mu\\) and standard deviation \\(\\sigma / \\sqrt{n}\\). The standard deviation of the sampling distribution is called the standard error.\n\n\n\n\n\n\nNote\n\n\n\nThe standard deviation of the sample mean depends on the number of samples, \\(n\\)."
  },
  {
    "objectID": "ae/ae16.html#practice-using-clt-normal-distribution",
    "href": "ae/ae16.html#practice-using-clt-normal-distribution",
    "title": "Central limit theorem",
    "section": "Practice using CLT & Normal distribution",
    "text": "Practice using CLT & Normal distribution\nSuppose the bone density for 65-year-old women is normally distributed with mean \\(809 mg/cm^3\\) and standard deviation of \\(140 mg/cm^3\\).\nLet \\(x\\) be the bone density of 65-year-old women. We can write this distribution of \\(x\\) in mathematical notation as\n\\[x \\sim N(809, 140)\\]"
  },
  {
    "objectID": "ae/ae16.html#visualize-the-population-distribution",
    "href": "ae/ae16.html#visualize-the-population-distribution",
    "title": "Central limit theorem",
    "section": "Visualize the population distribution",
    "text": "Visualize the population distribution\n\nggplot(data = data.frame(x = c(809 - 140*3, 809 + 140*3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 809, sd = 140),\n                color = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 809, sd = 140/sqrt(10)),\n                color = \"red\",lty = 2) + theme_bw() +\n  labs(title = \"Black solid line = population dist., Red dotted line = sampling dist.\")\n\n\n\n\n\nExercise 2\nBefore typing any code, based on what you know about the normal distribution, what do you expect the median bone density to be?\nWhat bone densities correspond to \\(Q_1\\) (25th percentile), \\(Q_2\\) (50th percentile), and \\(Q_3\\) (the 75th percentile) of this distribution? Use the qnorm() function to calculate these values.\n\n\nExercise 3\nThe densities of three woods are below:\n\nPlywood: 540 mg/cubic centimeter\nPine: 600 mg/cubic centimeter\nMahogany: 710 mg/cubic centimeter\nWhat is the probability that a randomly selected 65-year-old woman has bones less dense than Pine?\nWould you be surprised if a randomly selected 65-year-old woman had bone density less than Mahogany? What if she had bone density less than Plywood? Use the respective probabilities to support your response.\n\n\n\nExercise 4\nSuppose you want to analyze the mean bone density for a group of 10 randomly selected 65-year-old women.\n\nAre the conditions for the Central Limit Theorem met?\n\nIndependence?\nSample size/distribution?\n\nWhat is the shape, center, and spread of the distribution of \\(\\bar{x}\\), the mean bone density for a group of 10 randomly selected 65-year-old women?\nWrite the distribution of \\(\\bar{x}\\) using mathematical notation.\n\n\n\nExercise 5\n\nWhat is the probability that the mean bone density for the group of 10 randomly-selected 65-year-old women is less dense than Pine?\nWould you be surprised if a group of 10 randomly-selected 65-year old women had a mean bone density less than Mahogany? What the group had a mean bone density less than Plywood? Use the respective probabilities to support your response.\n\n\n\nExercise 6\nExplain how your answers differ in Exercises 3 and 5."
  },
  {
    "objectID": "ae/ae16.html#on-your-own",
    "href": "ae/ae16.html#on-your-own",
    "title": "Central limit theorem",
    "section": "On your own",
    "text": "On your own\nSuppose the distribution of the number of minutes users engage with apps on an iPad has a mean of 8.2 minutes and standard deviation of 1 minute. Let \\(x\\) be the number of minutes users engage with apps on an iPad, \\(\\mu\\) be the population mean and \\(\\sigma\\) the population standard deviation. Then,\n\\[x \\sim N(8.2, 1)\\]\nSuppose you take a sample of 60 randomly selected app users and calculate the mean number of minutes they engage with apps on an iPad, \\(\\bar{x}\\). The conditions (independence & sample size/distribution) to apply the Central Limit Theorem are met. Then by the Central Limit Theorem\n\\[\\bar{x} \\sim N(8.2, 1/\\sqrt{60})\\]\n\nWhat is the probability a randomly selected user engages with iPad apps for more than 8.3 minutes? Use pnorm for calculations.\n\n#add code\n\nWhat is the probability the mean minutes of app engagement for a group of 60 randomly selected iPad users is more than 8.3 minutes? Use pnorm for calculations.\n\n#add code\n\nWhat is the probability the mean minutes of app engagement for a group of 60 randomly selected iPad users is between 8.3 and 8.4 minutes? Use pnorm for calculations.\n\n\n    #add code"
  },
  {
    "objectID": "ae/ae10.html",
    "href": "ae/ae10.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Regression project released"
  },
  {
    "objectID": "ae/ae10.html#bulletin",
    "href": "ae/ae10.html#bulletin",
    "title": "Logistic Regression",
    "section": "",
    "text": "Regression project released"
  },
  {
    "objectID": "ae/ae10.html#today",
    "href": "ae/ae10.html#today",
    "title": "Logistic Regression",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nunderstand logistic regression as a linear model of binary outcomes\nbe able to fit logistic regression in R"
  },
  {
    "objectID": "ae/ae10.html#getting-started",
    "href": "ae/ae10.html#getting-started",
    "title": "Logistic Regression",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae10.qmd\",\ndestfile = \"ae10.qmd\")"
  },
  {
    "objectID": "ae/ae10.html#load-packages-and-data",
    "href": "ae/ae10.html#load-packages-and-data",
    "title": "Logistic Regression",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(boot) # contains inv.logit() function\n\nTo illustrate logistic regression, we will build a spam filter from email data. Today’s data consists of 4601 emails that are classified as spam or non-spam. The data was collected at Hewlett-Packard labs and contains 58 variables. The first 48 variables are specific keywords and each observation is the percentage of appearance (frequency) of that word in the message. Click here to read more.\n\ntype \\(= 1\\) is spam\ntype \\(= 0\\) is non-spam\n\n\nspam = read_csv(\"https://sta101.github.io/static/appex/data/spam.csv\")\nglimpse(spam)\n\nRows: 4,601\nColumns: 58\n$ make              &lt;dbl&gt; 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ address           &lt;dbl&gt; 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ all               &lt;dbl&gt; 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0.00, 0.46…\n$ num3d             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ our               &lt;dbl&gt; 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1.88, 0.61…\n$ over              &lt;dbl&gt; 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ remove            &lt;dbl&gt; 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0.00, 0.30…\n$ internet          &lt;dbl&gt; 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1.88, 0.00…\n$ order             &lt;dbl&gt; 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0.00, 0.92…\n$ mail              &lt;dbl&gt; 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0.00, 0.76…\n$ receive           &lt;dbl&gt; 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0.00, 0.76…\n$ will              &lt;dbl&gt; 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0.00, 0.92…\n$ people            &lt;dbl&gt; 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0.00, 0.00…\n$ report            &lt;dbl&gt; 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ addresses         &lt;dbl&gt; 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ free              &lt;dbl&gt; 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0.00, 0.00…\n$ business          &lt;dbl&gt; 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ email             &lt;dbl&gt; 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0.00, 0.15…\n$ you               &lt;dbl&gt; 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0.00, 1.23…\n$ credit            &lt;dbl&gt; 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0.00, 3.53…\n$ your              &lt;dbl&gt; 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0.00, 2.00…\n$ font              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num000            &lt;dbl&gt; 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ money             &lt;dbl&gt; 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ hp                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hpl               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ george            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num650            &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ lab               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ labs              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ telnet            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num857            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ data              &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ num415            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num85             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ technology        &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ num1999           &lt;dbl&gt; 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ parts             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pm                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ direct            &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ cs                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ meeting           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ original          &lt;dbl&gt; 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0.00, 0.30…\n$ project           &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ re                &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ edu               &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ table             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ conference        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ charSemicolon     &lt;dbl&gt; 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ charRoundbracket  &lt;dbl&gt; 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0.054, 0.2…\n$ charSquarebracket &lt;dbl&gt; 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ charExclamation   &lt;dbl&gt; 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0.164, 0.0…\n$ charDollar        &lt;dbl&gt; 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0.054, 0.0…\n$ charHash          &lt;dbl&gt; 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ capitalAve        &lt;dbl&gt; 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1.671, 2.4…\n$ capitalLong       &lt;dbl&gt; 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6, 11, 61,…\n$ capitalTotal      &lt;dbl&gt; 278, 1028, 2259, 191, 191, 54, 112, 49, 1257, 749, 2…\n$ type              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nThe basic logic of our model is that the frequency of certain words can help us determine whether or not an email is spam.\nFor example, these emails came from George’s inbox. If the word “george” is not present in the message and the dollar symbol (charDollar) is, you might expect the email to be spam.\nUsing this data, we want to build a model that predicts whether a new email is spam or not. How do we build a model that can do this?\n\nExercise 1\nStart by examining 1 predictor.\n\nVisualize a linear model where the outcome is type (spam or not) and george is the predictor.\nDiscuss your visualization with your neighbor. Is this a good model? Why or why not?\n\n\n# code here"
  },
  {
    "objectID": "ae/ae10.html#example",
    "href": "ae/ae10.html#example",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\nLet’s build a model centered around just two predictor variables.\nThe first will be the word you and the second will be capitalTotal (the total number of capital letters in the message).\n\nExercise 2\nCreate a visualization with you on the x-axis and capitalTotal on the y-axis. Color data points by whether or not they are spam.\n\n# code here\n\nLet’s fit the model!\n\nfit_1 = logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(as.factor(type) ~ you + capitalTotal, data = spam, family = \"binomial\")\n  \nfit_1 %&gt;%\n  tidy()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -1.50     0.0554       -27.1 2.97e-162\n2 you           0.361    0.0198        18.3 1.84e- 74\n3 capitalTotal  0.00173  0.000104      16.6 5.66e- 62\n\n\n\n\nExercise 3\n\nWhat is different in the code above from previous linear models we fit?\n\n\n\nExercise 4\n\nWhat is the probability the email is spam if the frequency of you is 5% in the email and there are 2500 capital letters. Use the model equation above.\nWhat is the log-odds? (Recall from the prep that log-odds \\(= \\log \\frac{p}{1-p}\\)). Use the code below to check your work.\n\n\nnewdata = data.frame(you = 5, capitalTotal = 2500)\n\n# code here\n\n# check work\ncheckLogOdds = predict(fit_1$fit, newdata)\ncheckLogOdds\n\n       1 \n4.633134 \n\ncheckP = inv.logit(checkLogOdds)\ncheckP\n\n        1 \n0.9903694"
  },
  {
    "objectID": "ae/ae10.html#visualize-logistic-regression",
    "href": "ae/ae10.html#visualize-logistic-regression",
    "title": "Logistic Regression",
    "section": "Visualize logistic regression",
    "text": "Visualize logistic regression\n\nbeta = fit_1$fit$coefficients\nhyperplane = function(x){\n    decisionBoundary = 0.5\n    c = logit(decisionBoundary)\n    const = c - beta[1]\n    return((-beta[2]*x + const) / beta[3])\n}\n\nspam %&gt;%\n  ggplot(aes(x = you, y = capitalTotal, color = as.factor(type))) + \n  geom_point(alpha = 0.3) +\n  geom_function(fun = hyperplane) +\n  scale_colour_manual(values = c(\"orange\", \"steelblue\")) +\n  theme_minimal()\n\nWarning: Multiple drawing groups in `geom_function()`\nℹ Did you use the correct group, colour, or fill aesthetics?\n\n\n\n\n\n\nJust because there’s greater than 50% probability an email is spam doesn’t mean we have to label it as such. We can adjust our threshold or critical probability, a.k.a. decision boundary to be more or less sensitive to spam emails.\n\nIn other words we get to select a number \\(p^*\\) such that\nif \\(p &gt; p^*\\), then label the email as spam.\n\nExercise 5\n\nWhat would you set your decision boundary to and why?\nChange decisionBoundary in the code above to 0.01 and 0.999999. Do the results surprise you? Why or why not?\nlower boundary means that we label more emails as spam, high boundary means fewer emails as spam. We can adjust the boundary depending on how much we value receiving important emails vs how much we dislike spam.\n0 means all emails are spam, 1 means no emails are spam. Note you cannot set decision boundary to 0 or 1 because of logit function (would evaluate to inf or negative inf)"
  },
  {
    "objectID": "ae/ae10.html#classify-a-new-email",
    "href": "ae/ae10.html#classify-a-new-email",
    "title": "Logistic Regression",
    "section": "Classify a new email",
    "text": "Classify a new email\n\nemail = readLines(\"https://sta101.github.io/static/appex/data/test-email.txt\")\nemail\n\n[1] \"You Have Been Selected To Win A Free Trip To Disney World! \"\n[2] \"\"                                                           \n[3] \"YOU HAVE 30 SECONDS TO CLICK HERE TO CLAIM YOUR REWARD!\"    \n[4] \"\"                                                           \n[5] \"WHAT ARE YOU WAITING FOR? ACT NOW!\"                         \n[6] \"\"                                                           \n[7] \"SINCERELY,\"                                                 \n[8] \"\"                                                           \n[9] \"WALT DISNEY\"                                                \n\ntotalWord = sum(str_count(email, \" \"))\ntotalYou = sum(str_count(tolower(email), \"you\"))\ncapitalTotal = sum(str_count(email, \"[A-Z]\"))\n\nyouFreq = 100 * totalYou / totalWord\nnewemail = data.frame(you = youFreq, capitalTotal = capitalTotal)\n\nlogOdds = predict(fit_1$fit, newemail)\nlogOdds\n\n       1 \n3.648776 \n\ninv.logit(logOdds)\n\n        1 \n0.9746371 \n\n\n\nExercise 6\n\nDoes the code above count the correct number of “you”? Why or why not?\nDo you believe the predicted odds of the email being spam? Why or why not?\nWhat is the probability the test email is spam?"
  },
  {
    "objectID": "ae/ae10.html#assessing-predictive-ability",
    "href": "ae/ae10.html#assessing-predictive-ability",
    "title": "Logistic Regression",
    "section": "Assessing predictive ability",
    "text": "Assessing predictive ability\nWe will divide the data into a training set and testing set.\n\nset.seed(6)\nsampleIndices = sample.int(n = nrow(spam), size = 2000, replace = F)\ntrain = spam[sampleIndices, ]\ntest  = spam[-sampleIndices, ] %&gt;%\n  slice_sample(n = 2000)\n\n\nExercise 7\nNext, let’s train your model on the training set. Build a predictive model using any combination of predictors from spam. Save your fitted model as myModel\n\n# code here\n\n#example (delete this):\nmyModel = logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(as.factor(type) ~ you + address, data = train, family = \"binomial\")\n\nand test it on the testing set,\n\nprediction = test %&gt;%\n  mutate(myModelPrediction = predict(myModel, test)$.pred_class) \n\nprediction\n\n# A tibble: 2,000 × 59\n    make address   all num3d   our  over remove internet order  mail receive\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  0       0     0.7      0  0     0.14   0        0     0.28  0       0   \n 2  0       0     0        0  0     0      0        0     0     0       0   \n 3  0       0     2.5      0  0     0      0        0     0     0       0   \n 4  0       2.08  0        0  3.12  0      1.04     0     0     0       0   \n 5  0       0     1.04     0  1.04  0      0        1.39  0.34  0       0   \n 6  0       0.35  0.7      0  0.35  0      0        0     0     0       0   \n 7  0.39    0     0        0  1.17  0      0        0     0     0.39    0   \n 8  0       0.25  0.75     0  1     0.25   0        0     0     0       0.25\n 9  0       0     0        0  0.87  0      0        0     0     0       1.31\n10  0       0     0        0  1.11  0      0        0.55  0     3.91    0   \n# ℹ 1,990 more rows\n# ℹ 48 more variables: will &lt;dbl&gt;, people &lt;dbl&gt;, report &lt;dbl&gt;, addresses &lt;dbl&gt;,\n#   free &lt;dbl&gt;, business &lt;dbl&gt;, email &lt;dbl&gt;, you &lt;dbl&gt;, credit &lt;dbl&gt;,\n#   your &lt;dbl&gt;, font &lt;dbl&gt;, num000 &lt;dbl&gt;, money &lt;dbl&gt;, hp &lt;dbl&gt;, hpl &lt;dbl&gt;,\n#   george &lt;dbl&gt;, num650 &lt;dbl&gt;, lab &lt;dbl&gt;, labs &lt;dbl&gt;, telnet &lt;dbl&gt;,\n#   num857 &lt;dbl&gt;, data &lt;dbl&gt;, num415 &lt;dbl&gt;, num85 &lt;dbl&gt;, technology &lt;dbl&gt;,\n#   num1999 &lt;dbl&gt;, parts &lt;dbl&gt;, pm &lt;dbl&gt;, direct &lt;dbl&gt;, cs &lt;dbl&gt;, …\n\n\n\n\nExercise 8\nWhat is the proportion of false positives (i.e. classified as spam but was not)? False negatives?\n\n# code here"
  },
  {
    "objectID": "ae/ae19.html",
    "href": "ae/ae19.html",
    "title": "Tidy hypotheses",
    "section": "",
    "text": "Lab 7 due tonight"
  },
  {
    "objectID": "ae/ae19.html#bulletin",
    "href": "ae/ae19.html#bulletin",
    "title": "Tidy hypotheses",
    "section": "",
    "text": "Lab 7 due tonight"
  },
  {
    "objectID": "ae/ae19.html#today",
    "href": "ae/ae19.html#today",
    "title": "Tidy hypotheses",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nhypothesis test the tidy way\npractice more than testing proportions"
  },
  {
    "objectID": "ae/ae19.html#getting-started",
    "href": "ae/ae19.html#getting-started",
    "title": "Tidy hypotheses",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae19.qmd\",\ndestfile = \"ae19.qmd\")"
  },
  {
    "objectID": "ae/ae19.html#load-packages",
    "href": "ae/ae19.html#load-packages",
    "title": "Tidy hypotheses",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae19.html#notes",
    "href": "ae/ae19.html#notes",
    "title": "Tidy hypotheses",
    "section": "Notes",
    "text": "Notes\n\nNot just a coin flip\n\nHere’s an example of testing a proportion outside the context of coins.\n\n\npush_pull = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/push_pull.csv\")\n\n\npush_pull %&gt;%\n  slice(1:3, 24:26)\n\n# A tibble: 6 × 7\n  participant_id   age push1 push2 pull1 pull2 training\n           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1              1    41    41    45    16    17 density \n2              2    32    35    44     9    11 density \n3              3    44    33    38    10    11 density \n4             24    36    31    60     9    15 gtg     \n5             25    50    35    42     9    12 gtg     \n6             26    34    23    39     9    13 gtg     \n\n\nThe push_pull dataset comes from a “mini study” by mountain tactical institute.\n26 individuals completed 1 of 2 exercise regiments for 3.5 weeks to increase their pushups and pullups. Codebook below:\n\nparticipant_id: unique identifier for each participant\nage: age of participant\npush1/2: push-ups at beginning and end of program respectively\npull1/2: pull-ups at beginning and end of program respectively\ntraining: which training protocol the individual participated in\n\n\npush_pull = push_pull %&gt;%\n  mutate(\n    pct_push_inc = (push2 / push1 ) - 1,\n    pct_pull_inc = (pull2 / pull1) - 1)\n\nHypothesis: “Most people who train consistently will see at least a 15% increase in push-ups over a 3.5 week training period.”\nBreaking it down:\n\n“Most” i.e. “greater than 50%” indicates we should examine a proportion.\n\n\nExercise 1\nWhat’s the null?\n\n“will see at least a 15% increase”. Each person either increases by 15% over 3.5 weeks or does not. This is our binary outcome.\ncreate a new column called over_15pct that tells you whether or not an individual achieved at least a 15% increase in push-ups\n\n\n# code here\n\n\nWhat would be a default theory (null hypothesis)?\n\n\n\nExercise 2\nWrite the null and alternative in mathematical notation.\n\n\nExercise 3\nWhat is the observed statistic? Compute and write it in mathematical notation.\n\n\nExercise 4\nNext, simulate under the null and compute the p-value. State your conclusion with \\(\\alpha = 0.05\\). As a bonus, visualize the null distribution and shade in the p-value.\n\n\n\nMore than a proportion\n\nWhat if we want to make a claim about a different population parameter than a proportion? Maybe a mean, or median? We can’t necessarily flip a coin. The answer, is once again, bootstrap sampling.\n\nHypothesis: “The mean age of push-up/pull-up training participants is greater than 30”.\nLet’s investigate this hypothesis with a significance level \\(\\alpha = 0.01\\).\n\nExercise 5\nWrite down the null and alternative hypotheses in words and mathematical notation\n\n\nExercise 6\nWhat is the observed statistic? Write it in mathematical notation.\nBootstrapping does the following…\n\n# find observed statistic\nobs_mean_age = push_pull %&gt;%\n  drop_na(age) %&gt;%\n  summarize(meanAge = mean(age)) %&gt;%\n  pull()\n# subtract observed_mean - desired_mean from age\nage_and_null = push_pull %&gt;%\n  select(age) %&gt;%\n  drop_na(age) %&gt;%\n  mutate(nullAge = age - (obs_mean_age - 30))\n# show data frame\nage_and_null\n\n# A tibble: 25 × 2\n     age nullAge\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1    41    35.8\n 2    32    26.8\n 3    44    38.8\n 4    37    31.8\n 5    37    31.8\n 6    21    15.8\n 7    33    27.8\n 8    38    32.8\n 9    49    43.8\n10    33    27.8\n# ℹ 15 more rows\n\n# show the means of each column\nage_and_null %&gt;%\n  summarize(meanAge = mean(age),\n  mean_nullAge = mean(nullAge))\n\n# A tibble: 1 × 2\n  meanAge mean_nullAge\n    &lt;dbl&gt;        &lt;dbl&gt;\n1    35.2           30\n\n\nIf we take bootstrap samples from this new nullAge column, we are sampling from data with the same variability as our original data, but a different mean. This is a nice way to explore the null!\n\nset.seed(3)\n\n# simulate null\nnull_dist = push_pull %&gt;%\n  specify(response = age) %&gt;%\n  hypothesize(null = \"point\", mu = 30) %&gt;%\n  generate(reps = 10000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n\n# get observed statistic\nobs_stat = obs_mean_age\n\np_value = null_dist %&gt;%\n  get_p_value(obs_stat, direction = \"right\")\n\np_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0001\n\n\n\nThe p-value 1e-04 is less than \\(\\alpha = 0.01\\). I reject the null hypothesis. In context, there is evidence to suggest that average push/pull trainee age is older than 30 years old.\n\n\n\nExercise 7\nSay we are interested in the performance of trainees at this particular facility and the sample is representative of the population.\nHypothesis: The median number of pull-ups trainees can perform is less than 20 even after training for 3.5 weeks.\nWrite down the null and alternative hypothesis in mathematical notation.\n\n\nExercise 8\nWrite down the observed statistic. Simulate under the null and compute the p-value. Finally, visualize and interpret the p-value in context."
  },
  {
    "objectID": "ae/ae19.html#summary",
    "href": "ae/ae19.html#summary",
    "title": "Tidy hypotheses",
    "section": "Summary",
    "text": "Summary\n\nHypothesis testing procedure\n\nSpecify the null and alternative hypothesis. Choose or know \\(\\alpha\\).\nCollect/examine the data. Compute the observed statistic.\nSimulate under the null and compute the p-value using the observed statistic and the alternative hypothesis.\nCompare the p-value to your significance level \\(\\alpha\\) and reject or fail to reject the null. Interpret your result in context."
  },
  {
    "objectID": "ae/ae19.html#which-training-method-is-better",
    "href": "ae/ae19.html#which-training-method-is-better",
    "title": "Tidy hypotheses",
    "section": "Which training method is better?",
    "text": "Which training method is better?\nTwo exercise regimes:\n\n“density” training\n“grease-the-groove” (gtg)\n\nWe want to know, is the average pull-up percent increase of a gtg trainee significantly different than a density trainee?\nFundamentally, does the categorical variable training affect the average percentage increase in pull-ups?\nState the null hypothesis:\n\\[\n\\mu_d = \\mu_{gtg}\n\\]\n\\[\nH_0: \\mu_d - \\mu_{gtg} = 0\n\\]\nWhat we want to do to simulate data under this null:\n\nrandom_training = sample(push_pull$training, replace = FALSE)\n\npush_pull %&gt;%\n  select(pct_pull_inc) %&gt;%\n  mutate(random_training = random_training)\n\n\nExercise 9:\n\nComplete the hypothesis specification above by stating the alternative. Check the observed statistic reported below.\n\n\n# code here\n\nSimulating under the null and computing the p-value:\n\nsim_num = 10000\nset.seed(1)\n# simulate null\nnull_dist = push_pull %&gt;%\n  specify(response = pct_pull_inc, explanatory = training) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = sim_num, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"density\", \"gtg\"))\n# observed statistic\nobs_stat = .196 - .489\n# visualize / get p\nvisualize(null_dist) +\n  shade_p_value(obs_stat, direction = \"both\")\n\n\n\n\n\n\nExercise 10\nCompute the p-value and state your conclusion with \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "ae/ae19.html#summary-of-generate-options",
    "href": "ae/ae19.html#summary-of-generate-options",
    "title": "Tidy hypotheses",
    "section": "Summary of generate() options",
    "text": "Summary of generate() options\n\n1. draw\n\ndescription: flip a coin with probability p, or roll a die with probabilities associated with each side. See here for reference to the multinomial setting.\ntypical case: test the proportion of a binary outcome\nnull: proportion \\(p\\) is some fixed number\nExample (hypothesis test for a single proportion):\n\n\n\n2. bootstrap\n\ndescription: re-sample your data with replacement\ntypical case (in hypothesis testing): does the mean equal a specific value? Does the median equal a specific value?\nnull: what would the data have looked like if nothing but the point estimate changed?\n\n\n\n3. permute\n\ndescription: permutes variables\ntypical case: is there a difference in the outcome between groups?\nassociated null: group membership does not matter i.e. group A and group B have the same outcome\nExample (test for independence):"
  },
  {
    "objectID": "ae/ae23.html",
    "href": "ae/ae23.html",
    "title": "Project Tips",
    "section": "",
    "text": "Reminders\n\nDraft final project report due Friday December 2. Peer-review in this lab.\nLab 09 due Thursday December 1st."
  },
  {
    "objectID": "ae/ae23.html#bulletin",
    "href": "ae/ae23.html#bulletin",
    "title": "Project Tips",
    "section": "",
    "text": "Reminders\n\nDraft final project report due Friday December 2. Peer-review in this lab.\nLab 09 due Thursday December 1st."
  },
  {
    "objectID": "ae/ae23.html#today",
    "href": "ae/ae23.html#today",
    "title": "Project Tips",
    "section": "Today",
    "text": "Today\nBy the end of today you will practice a few quarto/markdown tricks to polish your report and simplify your presentation. Specifically we will discuss:\n\ncode chunk settings\ncitations\nkable() tables\nquarto presentations"
  },
  {
    "objectID": "ae/ae23.html#getting-started",
    "href": "ae/ae23.html#getting-started",
    "title": "Project Tips",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae23.qmd\",\ndestfile = \"ae23.qmd\")\nYou can also download the references.bib file using the code below.\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/references.bib\",\ndestfile = \"references.bib\")"
  },
  {
    "objectID": "ae/ae23.html#code-chunk-settings",
    "href": "ae/ae23.html#code-chunk-settings",
    "title": "Project Tips",
    "section": "Code chunk settings",
    "text": "Code chunk settings\nSome options available for customizing output (see quarto documenation for more detail).\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nEvaluate the code chunk (if false, just echos the code into the output)\n\n\necho\nInclude the source code in output\n\n\nwarning\nInclude warnings in the output\n\n\nmessage\nWhether to preserve messages emitted by message() (similar to the option warning)\n\n\ninclude\nCatch all for preventing any output (code or results) from being included (e.g. include: false suppresses all output from the code block)\n\n\n\nThese options can be applied globally (the whole document) or locally (a specific code chunk). Global settings are controlled in the YAML (see the top of the document) while local code chunk options can be applied with #| (see example below).\n\nExercise 1\nIn the code chunk below:\n\nset warning to false\nset echo to false\n\nand re-render.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\n\nIn addition to code chunks, figures have settings as well.\nWe can set captions and an alt attributes using #| fig-cap: and #| fig-alt: respectively. alt captions specify “alternate text” for an image. Alternative text appears if an image cannot be displayed and is also read by screen-readers.\nAdditional figure options include\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nfig-width\nfigure width in inches\n\n\nfig-height\nfigure height in inches\n\n\nfig.align\ne.g. fig.align: center centers figure alignment\n\n\nfig.asp\nchanges figure height based on aspect ratio with width\n\n\nout.width\nsets figure width relative to text (1000 = 100% text width), e.g. out.width: 1000\n\n\n\nIn all cases above, we can again set options locally or globally. Note: local options override global options.\n\n\nExercise 2\nAdd a figure caption to the figure below. Next, change the output width to be 50% of the text. Finally, align the figure with the center of the page.\n\nstarwars %&gt;%\n  ggplot(aes(x = height)) +\n  geom_density() +\n  labs(x = \"Height (cm)\", y = \"Density\") +\n  theme_bw()\n\nWarning: Removed 6 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\nProject specific notes\nFor the project, you will set the option echo: FALSE and warning: FALSE to hide all code and warnings in your final report.\nSuggestion: make your figures consistently themed, e.g. use similar figure size/aspect ratio and color scheme throughout your report. Change the default gray background, see themes.\n\n\nExercise 3\nChange the global code chunk settings so the document is formatted as your final project will be. Render and take a look at the updated PDF."
  },
  {
    "objectID": "ae/ae23.html#citations",
    "href": "ae/ae23.html#citations",
    "title": "Project Tips",
    "section": "Citations",
    "text": "Citations\nYour report will include citations, e.g. the data source, previous research, and other sources as needed. At a minimum, you should have a citation for the data source.\nAll of your bibliography entries will be stored in a .bib file. The entries of the bibliography are stored using BibTex, i.e., a format to store citations in LaTeX. Let’s take a look at references.bib.\nIn addition to the .bib file:\n\nInclude bibliography: references.bib in the YAML.\nAt the end of the report, include ## References. This will list all of the references at the end of the document.\n\n\nCitation examples\n\nIn Wickham, Chang, and Wickham (2016), the authors focus present the grammar of graphics package ggplot2 for R.\nWithin the grammar of graphics, ggplot() is the first layer of any plot (Wickham, Chang, and Wickham 2016).\n\n\nExercise 4\n\nAdd a citation for tidytuesday to this document. Hint: check out the tidytuesday GitHub page."
  },
  {
    "objectID": "ae/ae23.html#links",
    "href": "ae/ae23.html#links",
    "title": "Project Tips",
    "section": "Links",
    "text": "Links\nAdd URLs to your document using the following syntax:\nDISPLAYED TEXT"
  },
  {
    "objectID": "ae/ae23.html#neat-kable-table",
    "href": "ae/ae23.html#neat-kable-table",
    "title": "Project Tips",
    "section": "Neat kable table",
    "text": "Neat kable table\n\nCalculate the mean, median, and standard deviation of mass. Display the results.\n\n\nExercise 5\n\n# code here\n\n\nLet’s neatly display the results using the kable function from the knitr package. We will\n\nDisplay results to 2 decimal places\nCustomize column names\nAdd a caption\n\n\n\n## add code"
  },
  {
    "objectID": "ae/ae23.html#presentations-demo",
    "href": "ae/ae23.html#presentations-demo",
    "title": "Project Tips",
    "section": "Presentations (demo)",
    "text": "Presentations (demo)"
  },
  {
    "objectID": "ae/ae7.html",
    "href": "ae/ae7.html",
    "title": "Linear regression II",
    "section": "",
    "text": "Lab 3 due Thursday\nExam 1 released Thursday and due Monday\n\ncheck out practice and sakai solutions\nno TA office hours Friday/Monday\nask questions early\n\nLooking towards next week, please fill out this optional form to request group members (from your lab) to work on the projects with."
  },
  {
    "objectID": "ae/ae7.html#bulletin",
    "href": "ae/ae7.html#bulletin",
    "title": "Linear regression II",
    "section": "",
    "text": "Lab 3 due Thursday\nExam 1 released Thursday and due Monday\n\ncheck out practice and sakai solutions\nno TA office hours Friday/Monday\nask questions early\n\nLooking towards next week, please fill out this optional form to request group members (from your lab) to work on the projects with."
  },
  {
    "objectID": "ae/ae7.html#recap-warmup",
    "href": "ae/ae7.html#recap-warmup",
    "title": "Linear regression II",
    "section": "Recap (warmup)",
    "text": "Recap (warmup)\nFrom last time…\n\nWhat is \\(\\hat{y}\\)? How is it different than \\(y\\)?\nWhat is \\(\\hat{\\beta}\\)? How is it different than \\(\\beta\\)?\nWhat is a residual? How is it different than error?"
  },
  {
    "objectID": "ae/ae7.html#today",
    "href": "ae/ae7.html#today",
    "title": "Linear regression II",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ncompute \\(R^2\\) and use it to select between models\nunderstand the geometric picture of multiple linear regression\nbe able to build, fit and interpret linear models with \\(&gt;1\\) predictor"
  },
  {
    "objectID": "ae/ae7.html#getting-started",
    "href": "ae/ae7.html#getting-started",
    "title": "Linear regression II",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae7.qmd\",\ndestfile = \"ae7.qmd\")"
  },
  {
    "objectID": "ae/ae7.html#load-packages-and-data",
    "href": "ae/ae7.html#load-packages-and-data",
    "title": "Linear regression II",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(scatterplot3d)\n\nToday’s data is a collection of tech stock prices from January 1st 2020 to December 31st 2021. I pulled this data off Yahoo finance using their API via the tidyquant package July 2022.\n\nstocks = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/stocks2.csv\")"
  },
  {
    "objectID": "ae/ae7.html#notes",
    "href": "ae/ae7.html#notes",
    "title": "Linear regression II",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "ae/ae7.html#r2-and-checking-model-fit",
    "href": "ae/ae7.html#r2-and-checking-model-fit",
    "title": "Linear regression II",
    "section": "\\(R^2\\) and checking model fit",
    "text": "\\(R^2\\) and checking model fit\n\nConceptual introduction\n\\(R^2\\), aka “the coefficient of determination” or “correlation squared” is a way to see how well a given model fits the data. Formally,\n\\[\nR^2 = 1 - \\frac{\\sum_i r_i^2}{\\sum_i (y_i - \\bar{y})^2}\n\\]\nwhere \\(\\bar{y}\\) is the mean of all y values.\nIn words,\n\\[\nR^2 = 1 - \\frac{\\text{sum of squared residuals}}{\\text{sum of outcome squared distance from the mean}}\n\\]\nLet’s focus on the word version to build intuition.\n\nThe sum of squared residuals is a measure of how wrong our model is (how much our model doesn’t explain)\nThe denominator is proportional to the average square distance from the mean, i.e. the variance, i.e. the amount of variability in the data.\nTogether, the fraction represents the proportion of variability that is not explained by the model.\n\nIf the sum of squared residuals is 0, then the model explains all variability and \\(R^2 = 1 - 0 = 1\\).\nSimilarly if the sum of squared residuals is the same as all the variability in the data, then model does not explain any variability and \\(R^2 = 1 - 1 = 0\\).\nFinal take-away: \\(R^2\\) is a measure of the proportion of variability the model explains. An \\(R^2\\) of 0 is a poor fit and \\(R^2\\) of 1 is a perfect fit.\n\n\nHow to find \\(R^2\\)\nTo find \\(R^2\\) simply call the function glance() on your modelFit, e.g.\nmodelFit = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(outcome ~ predictor, data = data_set)\n  \nglance(modelFit)"
  },
  {
    "objectID": "ae/ae7.html#two-predictor-main-effects-model-and-notation",
    "href": "ae/ae7.html#two-predictor-main-effects-model-and-notation",
    "title": "Linear regression II",
    "section": "Two predictor main effects model and notation",
    "text": "Two predictor main effects model and notation\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\]\n\n\\(y\\): the outcome variable. Also called the “response” or “dependent variable”. In prediction problems, this is what we are interested in predicting.\n\\(x_i\\): the \\(i^{th}\\) predictor. Also commonly referred to as “regressor”, “independent variable”, “covariate”, “feature”, “the data”.\n\\(\\beta_i\\): “constants” or coefficients i.e. fixed numbers. These are population parameters. \\(\\beta_0\\) has another special name, “the intercept”.\n\\(\\epsilon\\): the error. This quantity represents observational error, i.e. the difference between our observation and the true population-level expected value: \\(\\beta_0 + \\beta_1 x\\).\n\nEffectively this model says our data \\(y\\) is linearly related to the \\(x_1\\) and \\(x_2\\) but is not perfectly observed due to some error.\n\nA simple example\nLet’s examine the first quarter of 2020 high prices of Microsoft, IBM and Apple stocks to illustrate some ideas.\n\n\n\n\n\n\nIf we have three measurements (variables) then each observation is a point in three-dimensional space. In this example, we can choose one of our measurements to be the outcome variable (e.g. Apple stock price) and use our other two measurements (MSFT and IBM price) as predictors.\nIn general, the total number of measurements, i.e. variables (columns) in our linear model represents the spatial dimension of our model.\nOur fitted linear model no longer looks like a line, but instead looks like a plane.\n\n\n\n\n\n\n\nThis plane shows our prediction of AAPL price (\\(y\\)) given both MSFT price (\\(x_1\\)) and IBM price (\\(x_2\\))\nDemo: building intuition for higher dimensional linear models\n\n\nExercise 1\nIn \\(n\\)-dimensional space, a linear equation creates a \\(\\text{insert number here}\\)-dimensional object."
  },
  {
    "objectID": "ae/ae7.html#fitting-a-multiple-regression-model-in-r",
    "href": "ae/ae7.html#fitting-a-multiple-regression-model-in-r",
    "title": "Linear regression II",
    "section": "Fitting a multiple regression model in R",
    "text": "Fitting a multiple regression model in R\nFind the equation of the plane above with this one simple trick!\nmyModelFit = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(outcome ~ predictor1 + predictor2 + predictor3 + ..., data = data-set-here)\nwe can simply ‘add’ in new predictors! This code template will fit the model according to the ordinary least squares (OLS) objective function, i.e. we are finding the equation of the hyperplane that minimizes the sum of squared residuals.\nYou can subsequently print the coefficients (\\(\\beta\\)s) to the screen by simply typing the model name, e.g. myModelFit or calling the tidy() function on your fitted model, e.g. tidy(myModelFit).\n\nExercise 2\nIn the code chunk below, fit the multiple regression model described above where\n\\(y\\): AAPL high price, \\(x_1\\): MSFT high price, \\(x_2\\): IBM high price.\nThen write the equation of your fitted model below.\n\nNote: you should change the name of “myModelFit” to be something more meaningful, e.g. apple_high_fit\n\n\n# code here \n\nThe equation of the plane above:\n\\[\n\\text{your equation here}\n\\]\n\n\nExercise 3\nInterpret the coefficients in your equation above.\n[your interpretation here]"
  },
  {
    "objectID": "ae/ae7.html#a-better-model",
    "href": "ae/ae7.html#a-better-model",
    "title": "Linear regression II",
    "section": "A better model",
    "text": "A better model\n\nLog return\nApplying a model to values outside of the original data is called extrapolation. Extrapolation can be very unreliable.\nThat being noted, it would be nice if our model was only able to predict realistic outcomes. If we consider extrapolating our forecast, we will see that our linear model can easily predict unrealistic values. For example, with a negative slope, we can imagine that a very high Microsoft price drives our Apple prediction down to a negative value.\nHowever, stock prices cannot be negative. A more useful modeling framework used by investors is to predict the “log return” of a stock. Over the course of day, the log return is defined:\n\\[\n\\log(\\text{close price}) - \\log(\\text{open price}) = \\log \\left( \\frac{\\text{close price}}{\\text{open price}} \\right)\n\\]\n\nExercise 4\nStarting with your stocks data frame, create new columns AAPL.LogReturn, MSFT.LogReturn, IBM.LogReturn that shows the daily log return of each stock. Continue this for the remaining stocks in the data frame. Save your new data frame as stock_returns.\n\n# code here\n\n\n\nExercise 5\nFit the following model:\n\\[\ny = \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + \\epsilon\n\\] where\n\n\\(y\\): AAPL daily log return\n\\(x_1\\): MSFT daily log return\n\\(x_2\\): IBM daily log return\n\nand report \\(R^2\\).\n\n# code here \n\n\n\n\nPredicting the future\nSo far we’ve only used the present to predict the present. i.e. we’ve used January 1st IBM prices to predict January 1st AAPL prices. While the resulting models are quite good, they are not particularly useful.\nIt would be much more useful if we could predict the return of AAPL tomorrow so that we could make an informed decision about buying or selling it.\nTo begin such an endeavor, let’s build a model that uses yesterday’s log-return of IBM and MSFT to predict today’s log return of AAPL.\n\nExercise 6\nWhat should our data frame look like?\n[ your answer here ]\nLet’s make that data frame! Adapt the example below to create new columns for yesterday’s IBM and MSFT returns.\n\nstock_returns2 = stock_returns %&gt;%\n  mutate(AAPL.LogReturnYesterday = lag(AAPL.LogReturn, 1)) %&gt;%\n  filter(!is.na(AAPL.LogReturnYesterday))\n\nstock_returns2\n\n\n\nExercise 7\nFit the following model:\n\\[\ny = \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + \\epsilon\n\\]\nwhere\n\\(y\\): AAPL daily log return \\(x_1\\): MSFT log return yesterday \\(x_2\\): IBM log return yesterday\nand report \\(R^2\\). What do you notice?\n\n# code here"
  },
  {
    "objectID": "exercises/exercise-02.html",
    "href": "exercises/exercise-02.html",
    "title": "Exercise 2: mutlivariate KDE",
    "section": "",
    "text": "Consider bivariate kernel density estimator. Simulate data from bivariate normak distribution \\(\\mathcal{N}((0,0), (1, 0.3; 0.3, 1))\\).\n\nTry different bandwidths and different kernels (use the bivariate normal kernel as well as the product kernel with the univariate Epanechnikov kernel)\nFind ways to visually compare your estimates with the real density (3d plots of the density or density contour plots)\nR functions such as kde2d of package MASS, contour and persp might be helpful"
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 15 October 2023."
  },
  {
    "objectID": "assignments/assignment-02.html#recap-kernel-density-estimation",
    "href": "assignments/assignment-02.html#recap-kernel-density-estimation",
    "title": "Assignment 2",
    "section": "1 Recap: Kernel Density Estimation",
    "text": "1 Recap: Kernel Density Estimation\nRemember that the Kernel Density Estimation (KDE) of \\(f\\) based on \\(X_1,\\ldots,X_N\\) is \\[\\widehat{f}(x) = \\frac{1}{n h_n} \\sum_{i=1}^n K\\left(\\frac{X_i - x}{h_n} \\right),\\] where the \\(K(\\cdot)\\) satisfies:\n\n\n\n\\(K(x) \\geq 0\\) for all \\(x \\in \\mathbb{R}\\)\n\\(K(- x) = K(x)\\) for all \\(x \\in \\mathbb{R}\\)\n\\(\\int_\\mathbb{R} K(x) d x = 1\\)\n\n\n\n\n\n\\(\\lim_{|x| \\to \\infty} |x| K(x) = 0\\)\n\\(\\sup_x |K(x)| &lt; \\infty.\\)\n\n\n\n\n\nCode\nplot_kdes &lt;- function(bw){\n  plot(density(faithful$eruptions, kernel=\"gaussian\", bw=bw),\n       main=paste(\"bandwidth = \",bw,sep=\"\"), xlab=\"time [min]\")\n  lines(density(faithful$eruptions, kernel=\"epanechnikov\", bw=bw), col=4)\n  lines(density(faithful$eruptions, kernel=\"rectangular\", bw=bw), col=2)\n}\npar(mfrow=c(1,4), mar = c(3.2, 3, 1.6, 0.2))\nplot_kdes(1); plot_kdes(0.5); plot_kdes(0.25); plot_kdes(0.1)\n\n\n\n\n\nImpact of the bandwidth on KDE, where the Gaussian kernel is in black, the Epanechnikov kernel in blue and the rectangular kernel in red.\n\n\n\n\nwhere the kernels are defined as:\n\n\n\n\n\n\n\nKernel Name\nFormula\n\n\n\n\nEpanechnikov\n\\(K(x) \\propto (1-x^2) \\mathbb{1}_{[|x| \\leq 1]}\\)\n\n\nGaussian\n\\(K(x) \\propto \\exp(-x^2/2)\\)\n\n\nRectangular\n\\(K(x)=\\frac{1}{2} 1_{\\{-1&lt;x&lt;1\\}}\\)"
  },
  {
    "objectID": "assignments/assignment-02.html#task",
    "href": "assignments/assignment-02.html#task",
    "title": "Assignment 2",
    "section": "2 Task",
    "text": "2 Task\nWe saw in the lecture that the choice of the bandwidth \\(h\\) is crucial for the performance of the KDE. In this assignment we will explore this in more detail with a simulation study. Specifically:\n\ngenerate data from a Gaussian mixture with density function \\(f\\) (see Section 3)\nrepeat the following \\(200\\) times:\n\ngenerate \\(N=100\\) samples from the Gaussian mixture\nperform density estimation, i.e. obtain \\(\\widehat{f}\\), for\n\nGaussian, Epanechnikov, and rectangular kernels\nbandwidth values \\(h = 0.1,0.15,0.2,0.25,\\ldots,0.9\\)\n\ncalculate the error measure \\(\\| f - \\widehat{f} \\|_2\\)\n\nreport your findings as a single (well commented) figure"
  },
  {
    "objectID": "assignments/assignment-02.html#sec-GaussianMixture",
    "href": "assignments/assignment-02.html#sec-GaussianMixture",
    "title": "Assignment 2",
    "section": "3 Gaussian mixture",
    "text": "3 Gaussian mixture\nThe two functions below allow for random number generation and density evaluation for the Gaussian mixture distribution \\[f(x) = (1-\\tau) \\varphi_{\\mu_1,\\sigma_1^2}(x) + \\tau\\varphi_{\\mu_2,\\sigma_2^2}(x).\\]\n\n\nCode\n# random generation for a mixture of two normal distributions\nrmixnorm &lt;- function(N, mu1, mu2, sigma1, sigma2, tau){\n  ind &lt;- I(runif(N) &gt; tau)\n  X &lt;- rep(0,N)\n  X[ind] &lt;- rnorm(sum(ind), mu1, sigma1)\n  X[!ind] &lt;- rnorm(sum(!ind), mu2, sigma2)\n  return(X)\n}\n\n# density evaluation for a mixture of two normal distributions\ndmixnorm &lt;- function(x, mu1, mu2, sigma1, sigma2, tau){\n  y &lt;- (1-tau)*dnorm(x,mu1,sigma1) + tau*dnorm(x,mu2,sigma2)\n  return(y)\n}\n\n\nA sample call is below.\n\n\nCode\nlibrary(ggplot2)\n\nN &lt;- 300\nmu1 &lt;- 3\nmu2 &lt;- 0\nsigma1 &lt;- 0.5\nsigma2 &lt;- 1\ntau &lt;- 0.6\n\nX &lt;- rmixnorm(N, mu1, mu2, sigma1, sigma2, tau)\nx &lt;- seq(-3, 6, by = 0.01)\nfx &lt;- dmixnorm(x, mu1, mu2, sigma1, sigma2, tau)\n\nggplot() +\n    theme_bw() +\n    aes(X, after_stat(density)) +\n    geom_histogram(colour = \"#999999\", fill = \"#999999\", binwidth = 0.1) + \n    geom_line(aes(x, fx), colour = \"#E69F00\", linewidth = 1) + \n    labs(\n    title = \"Gaussian mixture density and histogram of 300 samples\",\n    subtitle = \"generated from a Gaussian mixture with tau = 0.6\")    \n\n\n\n\n\nSimilar functions will be provided for Python and Julia in the assignment repository."
  },
  {
    "objectID": "resources/week-02/mini-project-example.html",
    "href": "resources/week-02/mini-project-example.html",
    "title": "EDA example",
    "section": "",
    "text": "Data and Problem Description\nInsurance redlining refers to the practice of refusing to issue insurance to certain types of people or within some geographic area. The name comes from the act of drawing a red line around an area on a map. Now few would quibble with an insurance company refusing to sell a car insurance to a frequent drunk driver, but other forms of discrimination would be unacceptable.\nIn the late 1970s, the US Commission on Civil Rights examined charges by several Chicago community organizations that insurance companies were redlining their neighborhoods. Because comprehensive information about individuals homeowners was not available, the number of FAIR plan policies written and renewed in Chicago by ZIP code for the months of December 1977 through May 1978 was recorded. The FAIR plan was offered by the city of Chicago as a default policy to homeowners who had been rejected by the voluntary market. Information on other variables that might affect insurance writing such as fire and theft rates was also collected at the ZIP code level. The variables are:\n\ninvolact new FAIR plan policies and renewals per 100 housing units,\nfire fires per 100 housing units,\ntheft thefts per 1000 population,\nrace racial composition in percentage of minority,\nage (of housing) percentage of housing units built before 1939,\nincome median family income in thousands of dollars,\nside north or south side of Chicago.\n\nThe variable involact acts as a measure of insurance availability in the voluntary market, since most FAIR plan policyholders secure such coverage only after they have been rejected by the voluntary market. Insurance companies claim to reject insurances based on their pass losses (captured in variables theft and fire). The U.S. Commission on Civil Rights in 1979 was interested in how much income, age of housing, and in particular race affect insurance availability.\n\nlibrary(faraway)\ndata(chredlin, package=\"faraway\") # attaches the data from the faraway package\nstr(chredlin) # the name \"chredlin\" refers to CHicago REDLINing\n\n'data.frame':   47 obs. of  7 variables:\n $ race    : num  10 22.2 19.6 17.3 24.5 54 4.9 7.1 5.3 21.5 ...\n $ fire    : num  6.2 9.5 10.5 7.7 8.6 34.1 11 6.9 7.3 15.1 ...\n $ theft   : num  29 44 36 37 53 68 75 18 31 25 ...\n $ age     : num  60.4 76.5 73.5 66.9 81.4 52.6 42.6 78.5 90.1 89.8 ...\n $ involact: num  0 0.1 1.2 0.5 0.7 0.3 0 0 0.4 1.1 ...\n $ income  : num  11.74 9.32 9.95 10.66 9.73 ...\n $ side    : Factor w/ 2 levels \"n\",\"s\": 1 1 1 1 1 1 1 1 1 1 ...\n\nhead(chredlin)\n\n      race fire theft  age involact income side\n60626 10.0  6.2    29 60.4      0.0 11.744    n\n60640 22.2  9.5    44 76.5      0.1  9.323    n\n60613 19.6 10.5    36 73.5      1.2  9.948    n\n60657 17.3  7.7    37 66.9      0.5 10.656    n\n60614 24.5  8.6    53 81.4      0.7  9.730    n\n60610 54.0 34.1    68 52.6      0.3  8.231    n\n\n\nThe first column are ZIP codes, since we only have aggregated data for counties (not for individuals). This is not unreasonable here, due to the nature of redlining.\n\n\nData Exploration\nLet’s take a look at the individual variables, e.g. using histograms\n\nlibrary(tidyverse)\nchredlin %&gt;% mutate(side = as.numeric(side)) %&gt;% pivot_longer(everything()) %&gt;%\n  ggplot(aes(value)) + facet_wrap(~ name, scales = \"free\") + geom_histogram()\n\n\n\n\nThe univariate distributions look quite nice for most of the variables. For example, we have a good spread in race, so we should be capable to asses its effect quite accurately. There are more features to be noticed:\n\ninvolact has some concentration at 0, casting doubts on the usage of a linear model,\nmany variables seem to be skewed and transformations should be explored,\n\ne.g. using log-transform for income and fire seems like a no-brainer, also due to interpretation\n\nthere is an outlier visible at the right tail of theft, and other potential outliers.\n\nWill not act on these observations at this point, with the exception of taking log(income).\nNow let us explore relationships between the individual predictors and the response. The following plots show the simple regression lines with 95 % confidence bands (for the regression line). Jittering has been added to the variable side to avoid overplotting.\n\np1 &lt;- ggplot(chredlin,aes(race,involact)) + geom_point() +stat_smooth(method=\"lm\")\np2 &lt;- ggplot(chredlin,aes(fire,involact)) + geom_point() +stat_smooth(method=\"lm\")\np3 &lt;- ggplot(chredlin,aes(theft,involact)) + geom_point() +stat_smooth(method=\"lm\")\np4 &lt;- ggplot(chredlin,aes(age,involact)) + geom_point() +stat_smooth(method=\"lm\")\np5 &lt;- ggplot(chredlin,aes(income,involact)) + geom_point() +stat_smooth(method=\"lm\")\np6 &lt;- ggplot(chredlin,aes(side,involact)) + geom_point(position = position_jitter(width = .2,height=0))\n\nlibrary(ggpubr)\nggarrange(p1,p2,p3,p4,p5,p6)\n\n\n\n\nWithout a doubt, we see that race has a strong correlation with the response variable, potentially suggesting the bad practice of insurance redlining based on race. However, we also may be observing confounding: can insurance companies claim that this is due to correlation between risks (fire and thift) and race?\n\np1 &lt;- ggplot(chredlin,aes(race,fire)) + geom_point() +stat_smooth(method=\"lm\")\np2 &lt;- ggplot(chredlin,aes(race,theft)) + geom_point() +stat_smooth(method=\"lm\")\nggarrange(p1,p2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThere seem to be a positive relationships between race (percentage of minorities) and the variables fire and theft."
  },
  {
    "objectID": "resources/tips/reproducible_research.html",
    "href": "resources/tips/reproducible_research.html",
    "title": "Reproducible scientific computing",
    "section": "",
    "text": "Reproducible research is a critical aspect of modern data analysis and statistical investigations. It ensures that the results and findings obtained through data analysis can be independently verified, replicated, and built upon by others. In this section, we will explore the importance of reproducibility in statistical research and cover essential tools and practices to organize your projects for reproducibility.\n\n\nReproducible research is the practice of making your data analysis, code, and results transparent and accessible to others. By adopting reproducible research practices, you contribute to the integrity and reliability of scientific findings. It allows others to validate your results, identify potential errors, and extend your analyses. The benefits of reproducible research include increased trustworthiness of research, better collaboration, and more efficient scientific progress.\n\n\n\nA well-structured project organization is the foundation of reproducibility. When starting a new project, create a dedicated directory for it. Within this directory, consider the following structure:\n\nData: Store all relevant datasets and raw data files in this folder.\nCode: Keep your analysis scripts and code files in this directory. Use clear and meaningful filenames.\nOutput: Store the results, processed data, and visualizations generated by your code here.\nReports: Save any reports or documentation related to your analysis in this folder.\nReferences: Store any literature, papers, or external references used in your research.\nREADME.md: Create a readme file to provide an overview of the project, its purpose, and the steps to reproduce the analysis.\n\n\n\n\nVersion control systems like Git are essential tools for tracking changes in your code and project files. They allow you to maintain a history of your work, collaborate with others, and revert to previous versions if needed.\nGitHub is a popular platform that hosts Git repositories in the cloud, enabling seamless collaboration among team members. By pushing your project to a GitHub repository, you can share your work with others and receive feedback and contributions.\n\n\n\nTo ensure reproducibility, strive to create a clear and well-documented workflow. Document each step of your analysis, from data preprocessing to statistical computations and visualization. Use comments in your code to explain the rationale behind your decisions and highlight any assumptions made.\n\n\n\nIn any statistical computation, you may rely on various libraries, packages, or external data files. Make sure to specify the versions of the software and packages used in your analysis to ensure that others can reproduce your results precisely.\n\n\n\nIn this section, we have introduced the concept of reproducible research and its significance in statistical investigations. We explored how to organize your projects for reproducibility and utilize version control systems like Git and GitHub for collaboration and version tracking. By adopting these practices, you lay the groundwork for conducting reliable and transparent data analyses and creating a solid foundation for your statistical research. In the subsequent sections, we will delve into data manipulation, statistical computations, visualization, and reporting using Julia, R, and Python to support your journey towards becoming proficient in statistical computation and visualization.\n\n\n\n\n\nName\n\n\nTopic\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease see this page for good practices in programming."
  },
  {
    "objectID": "resources/tips/reproducible_research.html#introduction-to-reproducible-research",
    "href": "resources/tips/reproducible_research.html#introduction-to-reproducible-research",
    "title": "Reproducible scientific computing",
    "section": "",
    "text": "Reproducible research is a critical aspect of modern data analysis and statistical investigations. It ensures that the results and findings obtained through data analysis can be independently verified, replicated, and built upon by others. In this section, we will explore the importance of reproducibility in statistical research and cover essential tools and practices to organize your projects for reproducibility.\n\n\nReproducible research is the practice of making your data analysis, code, and results transparent and accessible to others. By adopting reproducible research practices, you contribute to the integrity and reliability of scientific findings. It allows others to validate your results, identify potential errors, and extend your analyses. The benefits of reproducible research include increased trustworthiness of research, better collaboration, and more efficient scientific progress.\n\n\n\nA well-structured project organization is the foundation of reproducibility. When starting a new project, create a dedicated directory for it. Within this directory, consider the following structure:\n\nData: Store all relevant datasets and raw data files in this folder.\nCode: Keep your analysis scripts and code files in this directory. Use clear and meaningful filenames.\nOutput: Store the results, processed data, and visualizations generated by your code here.\nReports: Save any reports or documentation related to your analysis in this folder.\nReferences: Store any literature, papers, or external references used in your research.\nREADME.md: Create a readme file to provide an overview of the project, its purpose, and the steps to reproduce the analysis.\n\n\n\n\nVersion control systems like Git are essential tools for tracking changes in your code and project files. They allow you to maintain a history of your work, collaborate with others, and revert to previous versions if needed.\nGitHub is a popular platform that hosts Git repositories in the cloud, enabling seamless collaboration among team members. By pushing your project to a GitHub repository, you can share your work with others and receive feedback and contributions.\n\n\n\nTo ensure reproducibility, strive to create a clear and well-documented workflow. Document each step of your analysis, from data preprocessing to statistical computations and visualization. Use comments in your code to explain the rationale behind your decisions and highlight any assumptions made.\n\n\n\nIn any statistical computation, you may rely on various libraries, packages, or external data files. Make sure to specify the versions of the software and packages used in your analysis to ensure that others can reproduce your results precisely.\n\n\n\nIn this section, we have introduced the concept of reproducible research and its significance in statistical investigations. We explored how to organize your projects for reproducibility and utilize version control systems like Git and GitHub for collaboration and version tracking. By adopting these practices, you lay the groundwork for conducting reliable and transparent data analyses and creating a solid foundation for your statistical research. In the subsequent sections, we will delve into data manipulation, statistical computations, visualization, and reporting using Julia, R, and Python to support your journey towards becoming proficient in statistical computation and visualization.\n\n\n\n\n\nName\n\n\nTopic\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease see this page for good practices in programming."
  },
  {
    "objectID": "resources/tips/collaboration.html",
    "href": "resources/tips/collaboration.html",
    "title": "Best practices for collaborative work",
    "section": "",
    "text": "In the world of statistics and data analysis, collaboration plays a vital role in advancing scientific knowledge and solving complex problems. When working in a team or contributing to open-source projects, it is crucial to follow best practices for collaborative work. In this section, we will explore key strategies to enhance collaboration, maintain code quality, and ensure seamless cooperation with peers."
  },
  {
    "objectID": "resources/tips/collaboration.html#code-style-guidelines",
    "href": "resources/tips/collaboration.html#code-style-guidelines",
    "title": "Best practices for collaborative work",
    "section": "Code Style Guidelines",
    "text": "Code Style Guidelines\nConsistency in coding style is essential for making the code more readable and understandable by others. Adopting a standard coding style across your team helps avoid confusion and reduces the time spent on code reviews. For each language (Julia, R, and Python), there are widely accepted coding style guidelines:\n\n\nJulia\n\nJulia official style guide\nmore opinionated SciML Style Guide\n\n\n\n\nPython\n\nPEP 8\n\n\n\n\nR\n\ntidyverse style guide\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere exists tools to help you check your code style and correct the basic mistakes. They are called linters. For example, in Julia, you can use JuliaFormatter.jl, in Python black and in R styler."
  },
  {
    "objectID": "resources/tips/collaboration.html#documentation",
    "href": "resources/tips/collaboration.html#documentation",
    "title": "Best practices for collaborative work",
    "section": "Documentation",
    "text": "Documentation\nWriting clear and comprehensive documentation for your code is crucial for effective collaboration. Documenting your functions, classes, and important code blocks with comments ensures that others can understand the purpose and functionality of each component. Additionally, provide explanations for any complex algorithms or statistical methods used in your analysis."
  },
  {
    "objectID": "resources/tips/collaboration.html#using-version-control-effectively",
    "href": "resources/tips/collaboration.html#using-version-control-effectively",
    "title": "Best practices for collaborative work",
    "section": "Using Version Control Effectively",
    "text": "Using Version Control Effectively\nWhen collaborating with others, version control becomes even more critical. Follow these best practices:\n\nCommit often\nMake small, logical commits with descriptive commit messages. Good commit messages are clear, concise, and informative. They provide context and explain the purpose of the commit in a way that anyone reading them, whether it’s your collaborators or your future self, can understand. A well-written commit message describes why the change was made, how it affects the codebase, and any relevant issues it addresses. By adhering to this practice, you make it easier to track changes, collaborate effectively, and maintain a clean and understandable version history for your projects.\nBad Commit Message:\nMade changes\nThis commit message is too generic and doesn’t provide any insight into what changes were made or why.\nCorrected Commit Message:\nAdd median calculation function for dataset variance\n\nIn response to user feedback and to enhance the statistical capabilities of our library, this commit introduces a new function, `calculate_median()`, which accurately calculates the median value for datasets. The function has been rigorously tested against various data sets to ensure its reliability and accuracy in statistical computations.\nThe corrected commit message provides specific information about the change made, mentions its purpose in improving the library’s statistical capabilities, and briefly explains the testing process to ensure the quality of the new feature. This level of detail is essential for collaboration and helps others understand the changes made to the codebase. If you are working on your own, you might not need to be so verbose in the body of the commit, but it is still a good practice to write good commit messages. See this article for some tips on how to write good commit messages, and why it could be useful.\n\n\nBranching\nUse branches for different features or tasks to keep the main development branch clean and stable.\n\n\nPull Requests (PRs)\nWhen contributing to shared repositories, submit pull requests for review before merging changes into the main branch."
  },
  {
    "objectID": "resources/tips/collaboration.html#code-reviews",
    "href": "resources/tips/collaboration.html#code-reviews",
    "title": "Best practices for collaborative work",
    "section": "Code Reviews",
    "text": "Code Reviews\nCode reviews are an integral part of the collaborative process. Reviewing each other’s code helps identify potential issues, provides valuable feedback, and improves the overall quality of the project. During code reviews, be respectful, specific, and constructive in your comments."
  },
  {
    "objectID": "resources/tips/collaboration.html#issue-tracking",
    "href": "resources/tips/collaboration.html#issue-tracking",
    "title": "Best practices for collaborative work",
    "section": "Issue Tracking",
    "text": "Issue Tracking\nUtilize issue tracking systems like GitHub Issues to keep track of tasks, bugs, and enhancements. When collaborating on larger projects, this helps organize and prioritize work effectively."
  },
  {
    "objectID": "resources/tips/collaboration.html#continuous-integration-ci",
    "href": "resources/tips/collaboration.html#continuous-integration-ci",
    "title": "Best practices for collaborative work",
    "section": "Continuous Integration (CI)",
    "text": "Continuous Integration (CI)\nConsider integrating continuous integration into your workflow. CI systems automatically build and test your code whenever changes are pushed to the repository. This ensures that the project remains in a working state and prevents introducing new bugs unintentionally."
  },
  {
    "objectID": "resources/tips/collaboration.html#collaborative-decision-making",
    "href": "resources/tips/collaboration.html#collaborative-decision-making",
    "title": "Best practices for collaborative work",
    "section": "Collaborative Decision-Making",
    "text": "Collaborative Decision-Making\nWhen making significant decisions about the project, involve all relevant team members. Encourage open discussions and consider different perspectives to arrive at the best solutions collectively."
  },
  {
    "objectID": "resources/tips/collaboration.html#communication-and-feedback",
    "href": "resources/tips/collaboration.html#communication-and-feedback",
    "title": "Best practices for collaborative work",
    "section": "Communication and Feedback",
    "text": "Communication and Feedback\nMaintain effective communication channels with your team. Use tools like Slack or Discord to discuss ideas, share progress, and address any challenges. Offer and receive feedback graciously to create a positive and productive working environment."
  },
  {
    "objectID": "resources/tips/collaboration.html#licensing-and-copyright",
    "href": "resources/tips/collaboration.html#licensing-and-copyright",
    "title": "Best practices for collaborative work",
    "section": "Licensing and Copyright",
    "text": "Licensing and Copyright\nEnsure that all code and resources used in the project comply with appropriate licenses and copyright laws. Respect intellectual property rights and provide proper attribution when using external libraries or resources.\nIn conclusion, effective collaboration is crucial for success in statistical research and data analysis projects. By adhering to code style guidelines, providing thorough documentation, using version control effectively, conducting code reviews, and maintaining open communication, you and your team can work together seamlessly, produce high-quality code, and contribute to the advancement of statistical knowledge. Collaborative work is not only about sharing ideas but also about learning from others and growing as a team. By adopting these best practices, you will become an effective collaborator and contribute to the success of your statistical projects."
  },
  {
    "objectID": "resources/computing/intro_to_python/index.html",
    "href": "resources/computing/intro_to_python/index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction\n\n\nSee (Duchesnay, Lofstedt, and Younes 2020)\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nGood practices in Python\n\n\nTips and tricks for writing better code\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nReferences\n\nDuchesnay, Edouard, Tommy Lofstedt, and Feki Younes. 2020. “Statistics and Machine Learning in Python.”"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html",
    "href": "resources/computing/intro_to_r/2_working_with_data.html",
    "title": "Working with data",
    "section": "",
    "text": "In the last section, we learned some fundamental principles of R and how to work in RStudio.\nIn this session, we’ll continue our introduction to R by working with a large dataset that more closely resembles that which you may encounter while analyzing data for research. By the end of this session, you should be able to:\n\nimport spreadsheet-style data into R as a data frame\nextract portions of data from a data frame\nmanipulate factors (categorical data)"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#objectives",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#objectives",
    "title": "Working with data",
    "section": "",
    "text": "In the last section, we learned some fundamental principles of R and how to work in RStudio.\nIn this session, we’ll continue our introduction to R by working with a large dataset that more closely resembles that which you may encounter while analyzing data for research. By the end of this session, you should be able to:\n\nimport spreadsheet-style data into R as a data frame\nextract portions of data from a data frame\nmanipulate factors (categorical data)"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#importing-spreadsheet-style-data-into-r",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#importing-spreadsheet-style-data-into-r",
    "title": "Working with data",
    "section": "Importing spreadsheet-style data into R",
    "text": "Importing spreadsheet-style data into R\nOpen RStudio, and we’ll check to make sure you’re ready to start work again. You can check to see if you’re working in your project directory by looking at the top of the Console. You should see the path (location in your computer) for the project directory you created last time (e.g., ~/Desktop/intro_r).\nIf you do not see the path to your project directory, go to File -&gt; Open Project and navigate to the location of your project directory. Alternatively, using your operating system’s file browser, double click on the r_intro.Rrpoj file.\nCreate a new R script (File -&gt; New File -&gt; R Script) and save it in your project directory with the name class2.R. Place the following comment on the top line as a title:\n# Introduction to R: Class 2\nIn the last session, we recommended organizing your work in directories (folders) according to projects. While a thorough discussion of project organization is beyond the scope of this class, we will continue to model best practices by creating a directory to hold our data:\n\n# make a directory\ndir.create(\"data\")\n\nYou should see the new directory appear in your project directory, in the lower right panel in RStudio. There is also a button in that panel you can use to create a new folder, but including the code to perform this task makes other people (and yourself) able to reproduce your work more easily.\nNow that we have a place to store our data, we can go ahead and download the dataset:\n\n# download data from url\ndownload.file(\"https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.csv\", \"data/clinical.csv\")\n\nThe code above has two arguments, both encompassed in quotation marks: first, you indicate where the data can be found online. Second, you indicate where R should store a copy of the file on your own computer.\nThe output from that command may look alarming, but it represents information confirming it worked. You can click on the data folder to ensure the file is now present.\nNotice that the URL above ends in clinical.csv, which is also the name we used to save the file on our computers. If you click on the URL and view it in a web browser, the format isn’t particularly easy for us to understand. You can also view the file by clicking on it in the lower right hand panel, then selecting “View File.”\n\nThe option to “Import Dataset” you see after clicking on the file references some additional tools present in RStudio that can assist with various kinds of data import. Because this requires installing additional software, complete exploration of these options is outside the scop of this class. For more information, check out this article.\n\nThe data we’ve downloaded are in csv format, which stands for “comma separated values.” This means the data are organized into rows and columns, with columns separated by commas.\nThese data are arranged in a tidy format, meaning each row represents an observation, and each column represents a variable (piece of data for each observation). Moreover, only one piece of data is entered in each cell.\nNow that the data are downloaded, we can import the data and assign to an object:\n\n# import data and assign to object\nclinical &lt;- read.csv(\"data/clinical.csv\")\n\nYou should see clinical appear in the Environment window on the upper right panel in RStudio. If you click on clinical there, a new tab will appear next to your R script in the Source window.\n\nClicking on the name of an object in the Environment window is a shortcut for running View(clinical); you’ll see this code appear in the Console after clicking.\n\nNow that we have the data imported and assigned to an object, we can take some time to explore the data we’ll be using for the rest of this course:\n\nThese data are clinical cancer data from the National Cancer Institute’s Genomic Data Commons, specifically from The Cancer Genome Atlas, or TCGA.\nEach row represents a patient, and each column represents information about demographics (race, age at diagnosis, etc) and disease (e.g., cancer type).\nThe data were downloaded and aggregated using an R script, which you can view in the GitHub repository for this course.\n\nThe function we used to import the data is one of a family of commands used to import the data. Check out the help documentation for read.csv for more options for importing data.\n\nYou can also import data directly into R using read.csv, using clinical &lt;- read.csv(\"https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.csv\"). For these lessons, we model downloading and importing in two steps, so you retain a copy of the data on your computer. This reflects how you’re likely to import your own data, as well as recommended practice for retaining data used in an analysis (since data online may be updated).\n\n\nChallenge-data\nDownload, inspect, and import the following data files. The URL for each sample dataset is included along with a name to assign to the object. (Hint: you can use the same function as above, but may need to update the sep = parameter) - URL: https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.tsv, object name: example1 - URL: https://raw.githubusercontent.com/fredhutchio/R_intro/master/extra/clinical.txt, object name: example2\n\nImporting data can be tricky and frustrating, However, if you can’t get your data into R, you can’t do anything to analyze or visualize it. It’s worth understanding how to do it effectively to save you time and energy later."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#data-frames",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#data-frames",
    "title": "Working with data",
    "section": "Data frames",
    "text": "Data frames\nNow that we have data imported and available, we can start to inspect the data more closely.\nThese data have been interpreted by R to be a data frame, which is a data structure (way of organizing data) that is analogous to tabular or spreadsheet style data. By definition, a data frame is a table made of vectors (columns) of all the same length. As we learned in our last session, a vector needs to include all of the same type of data (e.g., character, numeric). A data frame, however, can include vectors (columns) of different data types.\nTo learn more about this data frame, we’ll first explore its dimensions:\n\n# assess size of data frame\ndim(clinical)\n\n[1] 6832   20\n\n\nThe output reflects the number of rows first (6832), then the number of columns (20).\nWe can also preview the content by showing the first few rows:\n\n# preview first few rows\nhead(clinical) \n\n  primary_diagnosis tumor_stage age_at_diagnosis vital_status morphology\n1             C34.1    stage ia            24477         dead     8070/3\n2             C34.1    stage ib            26615         dead     8070/3\n3             C34.3    stage ib            28171         dead     8070/3\n4             C34.1    stage ia            27154        alive     8083/3\n5             C34.3   stage iib            29827         dead     8070/3\n6             C34.1  stage iiia            23370        alive     8070/3\n  days_to_death state tissue_or_organ_of_origin days_to_birth\n1           371  live                     C34.1        -24477\n2           136  live                     C34.1        -26615\n3          2304  live                     C34.3        -28171\n4            NA  live                     C34.1        -27154\n5           146  live                     C34.3        -29827\n6            NA  live                     C34.1        -23370\n  site_of_resection_or_biopsy days_to_last_follow_up cigarettes_per_day\n1                       C34.1                     NA          10.958904\n2                       C34.1                     NA           2.191781\n3                       C34.3                   2099           1.643836\n4                       C34.1                   3747           1.095890\n5                       C34.3                     NA                 NA\n6                       C34.1                   3576           2.739726\n  years_smoked gender year_of_birth         race              ethnicity\n1           NA   male          1936        white not hispanic or latino\n2           NA   male          1931        asian not hispanic or latino\n3           NA female          1927        white not hispanic or latino\n4           NA   male          1930        white not hispanic or latino\n5           NA   male          1923 not reported           not reported\n6           NA female          1942 not reported           not reported\n  year_of_death bcr_patient_barcode disease\n1          2004        TCGA-18-3406    LUSC\n2          2003        TCGA-18-3407    LUSC\n3            NA        TCGA-18-3408    LUSC\n4            NA        TCGA-18-3409    LUSC\n5          2004        TCGA-18-3410    LUSC\n6            NA        TCGA-18-3411    LUSC\n\n\nThe default number of rows shown is six. You can specify a different number using the n = parameter, demonstrated below using tail, which shows the last few rows\n\n# show last three rows\ntail(clinical, n = 3) \n\n     primary_diagnosis  tumor_stage age_at_diagnosis vital_status morphology\n6830             C54.1 not reported            27326         dead     8950/3\n6831             C54.1 not reported            24781        alive     8950/3\n6832             C54.1 not reported            20318        alive     8950/3\n     days_to_death state tissue_or_organ_of_origin days_to_birth\n6830           949  live                     C54.1        -27326\n6831            NA  live                     C54.1        -24781\n6832            NA  live                     C54.1        -20318\n     site_of_resection_or_biopsy days_to_last_follow_up cigarettes_per_day\n6830                       C54.1                     NA                 NA\n6831                       C54.1                    587                 NA\n6832                       C54.1                      0                 NA\n     years_smoked gender year_of_birth  race              ethnicity\n6830           NA female          1932 white not hispanic or latino\n6831           NA female          1945 white not hispanic or latino\n6832           NA female          1957 asian not hispanic or latino\n     year_of_death bcr_patient_barcode disease\n6830          2008        TCGA-NG-A4VW     UCS\n6831            NA        TCGA-QM-A5NM     UCS\n6832            NA        TCGA-QN-A5NN     UCS\n\n\nWe often need to reference the names of columns, so it’s useful to print only those to the screen:\n\n# view column names\nnames(clinical) \n\n [1] \"primary_diagnosis\"           \"tumor_stage\"                \n [3] \"age_at_diagnosis\"            \"vital_status\"               \n [5] \"morphology\"                  \"days_to_death\"              \n [7] \"state\"                       \"tissue_or_organ_of_origin\"  \n [9] \"days_to_birth\"               \"site_of_resection_or_biopsy\"\n[11] \"days_to_last_follow_up\"      \"cigarettes_per_day\"         \n[13] \"years_smoked\"                \"gender\"                     \n[15] \"year_of_birth\"               \"race\"                       \n[17] \"ethnicity\"                   \"year_of_death\"              \n[19] \"bcr_patient_barcode\"         \"disease\"                    \n\n\nIt’s also possible to view row names usingrownames(clinical), but our data only possess numbers for row names so it’s not very informative.\nAs we learned last time, we can use str to provide a general overview of the object:\n\n# show overview of object\nstr(clinical) \n\n'data.frame':   6832 obs. of  20 variables:\n $ primary_diagnosis          : chr  \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ tumor_stage                : chr  \"stage ia\" \"stage ib\" \"stage ib\" \"stage ia\" ...\n $ age_at_diagnosis           : int  24477 26615 28171 27154 29827 23370 19025 26938 28430 30435 ...\n $ vital_status               : chr  \"dead\" \"dead\" \"dead\" \"alive\" ...\n $ morphology                 : chr  \"8070/3\" \"8070/3\" \"8070/3\" \"8083/3\" ...\n $ days_to_death              : int  371 136 2304 NA 146 NA 345 716 2803 973 ...\n $ state                      : chr  \"live\" \"live\" \"live\" \"live\" ...\n $ tissue_or_organ_of_origin  : chr  \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_birth              : int  -24477 -26615 -28171 -27154 -29827 -23370 -19025 -26938 -28430 -30435 ...\n $ site_of_resection_or_biopsy: chr  \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_last_follow_up     : int  NA NA 2099 3747 NA 3576 NA NA 1810 956 ...\n $ cigarettes_per_day         : num  10.96 2.19 1.64 1.1 NA ...\n $ years_smoked               : int  NA NA NA NA NA NA NA NA NA NA ...\n $ gender                     : chr  \"male\" \"male\" \"female\" \"male\" ...\n $ year_of_birth              : int  1936 1931 1927 1930 1923 1942 1953 1932 1929 1923 ...\n $ race                       : chr  \"white\" \"asian\" \"white\" \"white\" ...\n $ ethnicity                  : chr  \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" ...\n $ year_of_death              : int  2004 2003 NA NA 2004 NA 2005 2006 NA NA ...\n $ bcr_patient_barcode        : chr  \"TCGA-18-3406\" \"TCGA-18-3407\" \"TCGA-18-3408\" \"TCGA-18-3409\" ...\n $ disease                    : chr  \"LUSC\" \"LUSC\" \"LUSC\" \"LUSC\" ...\n\n\nThe output provided includes:\n\ndata structure: data frame\ndimensions: 6832 rows and 20 columns\ncolumn-by-column information: each prefaced with a $, and includes the column name, data type (num, int, Factor)\n\n\nFactors are how character data are interpreted by R in data frames. We’ll talk more about working with factors at the end of this lesson.\n\nFinally, we can also examine basic summary statistics for each column:\n\n# provide summary statistics for each column\nsummary(clinical) \n\n primary_diagnosis  tumor_stage        age_at_diagnosis vital_status      \n Length:6832        Length:6832        Min.   : 3982    Length:6832       \n Class :character   Class :character   1st Qu.:19191    Class :character  \n Mode  :character   Mode  :character   Median :22842    Mode  :character  \n                                       Mean   :22320                      \n                                       3rd Qu.:26002                      \n                                       Max.   :32872                      \n                                       NA's   :114                        \n  morphology        days_to_death        state          \n Length:6832        Min.   :    0.0   Length:6832       \n Class :character   1st Qu.:  274.0   Class :character  \n Mode  :character   Median :  524.0   Mode  :character  \n                    Mean   :  878.2                     \n                    3rd Qu.: 1044.5                     \n                    Max.   :10870.0                     \n                    NA's   :4645                        \n tissue_or_organ_of_origin days_to_birth    site_of_resection_or_biopsy\n Length:6832               Min.   :-32872   Length:6832                \n Class :character          1st Qu.:-26002   Class :character           \n Mode  :character          Median :-22842   Mode  :character           \n                           Mean   :-22320                              \n                           3rd Qu.:-19191                              \n                           Max.   : -3982                              \n                           NA's   :114                                 \n days_to_last_follow_up cigarettes_per_day  years_smoked      gender         \n Min.   :  -64.0        Min.   : 0.008     Min.   : 8.00   Length:6832       \n 1st Qu.:  345.0        1st Qu.: 1.370     1st Qu.:30.75   Class :character  \n Median :  650.0        Median : 2.192     Median :40.00   Mode  :character  \n Mean   :  976.8        Mean   : 2.599     Mean   :39.96                     \n 3rd Qu.: 1259.0        3rd Qu.: 3.288     3rd Qu.:50.00                     \n Max.   :11252.0        Max.   :40.000     Max.   :63.00                     \n NA's   :1118           NA's   :5661       NA's   :6384                      \n year_of_birth      race            ethnicity         year_of_death \n Min.   :1902   Length:6832        Length:6832        Min.   :1990  \n 1st Qu.:1937   Class :character   Class :character   1st Qu.:2004  \n Median :1947   Mode  :character   Mode  :character   Median :2007  \n Mean   :1948                                         Mean   :2006  \n 3rd Qu.:1957                                         3rd Qu.:2010  \n Max.   :1993                                         Max.   :2014  \n NA's   :170                                          NA's   :5266  \n bcr_patient_barcode   disease         \n Length:6832         Length:6832       \n Class :character    Class :character  \n Mode  :character    Mode  :character  \n                                       \n                                       \n                                       \n                                       \n\n\nFor numeric data (such as year_of_death), this output includes common statistics like median and mean, as well as the number of rows (patients) with missing data (as NA). For factors (character data, such as disease), you’re given a count of the number of times the top six most frequent factors (categories) occur in the data frame."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#subsetting-data-frames",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#subsetting-data-frames",
    "title": "Working with data",
    "section": "Subsetting data frames",
    "text": "Subsetting data frames\nNow that our data are available for use, we can begin extracting relevant information from them.\n\n# extract first column and assign to a variable\nfirst_column &lt;- clinical[1]\n\nAs discussed last time with vectors, the square brackets ([ ]) are used to subset, or reference part of, a data frame. You can inspect the output object by clicking on it in the environment. It contains all of the rows for only the first column.\nWhen a single number is included in the square brackets, R assumes you are referencing a column. When you include two numbers in square brackets separated by a comma, R assumes the first number references the row and the second number references the column you desire.\nThis means you can also reference the first column as follows:\n\n# extract first column\nfirst_column_again &lt;- clinical[ , 1]\n\nLeaving one field blank means you want the entire set in the output (in this case, all rows).\n\nChallenge-extract\nWhat is the difference in results between the last two lines of code?\n\nSimilarly, we can also extract only the first row across all columns:\n\n# extract first row \nfirst_row &lt;- clinical[1, ]\n\nWe can also extract slices, or sections of rows and columns, such as a single cell:\n\n# extract cell from first row of first column\nsingle_cell &lt;- clinical[1,1]\n\nTo extract a range of cells, we use the same colon (:) syntax from last time:\n\n# extract a range of cells, rows 1 to 3, second column\nrange_cells &lt;- clinical[1:3, 2]\n\nThis works for ranges of columns as well.\nWe can also exclude particular parts of the dataset using a minus sign:\n\n# exclude first column\nexclude_col &lt;- clinical[ , -1] \n\nCombining what we know about R syntax, we can also exclude a range of cells using the c function:\n\n# exclude first 100 rows\nexclude_range &lt;- clinical[-c(1:100), ] \n\nSo far, we’ve been referencing parts of the dataset based on index position, or the number of row/column. Because we have included column names in our dataset, we can also reference columns using those names:\n\n# extract column by name\nname_col1 &lt;- clinical[\"tumor_stage\"]\nname_col2 &lt;- clinical[ , \"tumor_stage\"]\n\nNote the example above features quotation marks around the column name. Without the quotation marks, R will assume we’re attempting to reference an object.\nAs we discussed with subsetting based on index above, the two objects created above differ in the data structure. name_col1 is a data frame (with one column), while name_col2 is a vector. Although this difference in the type of object may not matter for your analysis, it’s useful to understand that there are multiple ways to accomplish a task, each of which may make particular code work more easily.\nThere are additional ways to extract columns, which use R specific for complex data objects, and may be useful to recognize as your R skills progress.\nThe first is to use double square brackets:\n\n# double square brackets syntax\nname_col3 &lt;- clinical[[\"tumor_stage\"]]\n\nYou can think of this approach as digging deeply into a complex object to retrieve data.\nThe final approach is equivalent to the last example, but can be considered a shortcut since it requires fewer keystrokes (no quotation marks, and only one symbol):\n\n# dollar sign syntax\nname_col4 &lt;- clinical$tumor_stage\n\nBoth of the last two approaches above return vectors. For more information about these different ways of accessing parts of a data frame, see this article.\nThe following challenges all use the clinical object:\n\nChallenge-days\nCode as many different ways possible to extract the column days_to_death.\n\n\nChallenge-rows\nExtract the first 6 rows for only age_at_diagnosis and days_to_death.\n\n\nChallenge-calculate\nCalculate the range and mean for cigarettes_per_day."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#factors",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#factors",
    "title": "Working with data",
    "section": "Factors",
    "text": "Factors\nNote: This section was written with a previous version of R that automatically interprets all character data as factors (this is not true of more recent versions of R). To execute the code in this section, please first import your data again, using the following modified command:\n\nclinical &lt;- read.csv(\"data/clinical.csv\", stringsAsFactors = TRUE)\n\nThis section explores one of the trickier types of data you’re likely to encounter: factors, which are how R interprets categorical data.\nWhen we imported our dataset into R, the read.csv function assumed that all the character data in our dataset are factors, or categories. Factors have predefined sets of values, called levels. We can explore what this means by first creating a factor vector:\n\n# create vector with factor data\ntest_data &lt;- factor(c(\"placebo\", \"test_drug\", \"placebo\", \"known_drug\"))\n# show factor\ntest_data\n\n[1] placebo    test_drug  placebo    known_drug\nLevels: known_drug placebo test_drug\n\n\nThis vector includes four pieces of data (often referred to as items or elements), which are printed as output above. The second line of the output shows information about the levels, or categories, of our vector. We can also access this information separately, which is useful if the data (vector) has a large number of elements:\n\n# show levels of factor\nlevels(test_data) \n\n[1] \"known_drug\" \"placebo\"    \"test_drug\" \n\n\nThe levels in this test dataset are currently listed in alphabetical order, which is the default presentation in R. The order of factors dictates how they are presented in subsequent analyses, so there are definitely cases in which you may want the levels in a specific order. In the case of test_data, we may want to keep the two drug treatments together, with placebo at the end:\n\n# reorder factors to put placebo at end\ntest_data &lt;- factor(test_data, levels = c(\"known_drug\", \"test_drug\", \"placebo\"))\n# show reordered\ntest_data\n\n[1] placebo    test_drug  placebo    known_drug\nLevels: known_drug test_drug placebo\n\n\nThis doesn’t change the data itself, but does make it easier to manage the data later.\nAnother useful aspect of factors is that they are stored as integers with labels. This means that you can easily convert them to numeric data:\n\n# converting factors to numeric\nas.numeric(test_data)\n\n[1] 3 2 3 1\n\n\nThis can be handy for some types of statistical analyses, and also illustrates the importance of ordering your levels appropriately.\nWe can apply this knowledge to our clinical dataset, by first observing how the data are presented when creating a basic plot:\n\n# quick and dirty plot\nplot(clinical$race)\n\n\n\n\nThe labels as presented by default are not particularly readable, and also lack appropriate capitalization and formatting. While it is possible to modify only the plot labels, we would have to do that for all of our subsequent analyses. It is more efficient to modify the levels once:\n\n# assign race data to new object \nrace &lt;- clinical$race \nlevels(race)\n\n[1] \"american indian or alaska native\"         \n[2] \"asian\"                                    \n[3] \"black or african american\"                \n[4] \"native hawaiian or other pacific islander\"\n[5] \"not reported\"                             \n[6] \"white\"                                    \n\n\nBy assigning the data to a new object, we can more easily perform manipulations without altering the original dataset.\nThe output above shows the current levels for race. We can access each level using their position in this order, combined with our knowledge of square brackets for subsetting:\n\nlevels(race)[1]\n\n[1] \"american indian or alaska native\"\n\n\nWe can modify them to improve their formatting by assigning a new level (name) of our choosing:\n\n# correct factor levels\nlevels(race)[1] &lt;- \"Am Indian\"\nlevels(race)[2] &lt;- \"Asian\" # capitalize asian\nlevels(race)[3] &lt;- \"black\"\nlevels(race)[4] &lt;- \"Pac Isl\"\nlevels(race)[5] &lt;- \"unknown\"\n# show revised levels\nlevels(race) \n\n[1] \"Am Indian\" \"Asian\"     \"black\"     \"Pac Isl\"   \"unknown\"   \"white\"    \n\n\nAlthough we’re not doing so here, we could also reorder the levels (as we did for test_data).\nOnce we are satisfied with the resulting levels, we assign the modified factor back to the original dataset:\n\n# replace race in data frame\nclinical$race &lt;- race\n# replot with corrected names\nplot(clinical$race)\n\n\n\n\nThis section was a very brief introduction to factors, and it’s likely you’ll need more information when working with categorical data of your own. A good place to start would be this article, and exploring some of the tools in the tidyverse (which we’ll discuss in the next lesson).\n\nChallenge-not-reported\nIn your clinical dataset, replace “not reported” in ethnicity with NA\n\n\nChallenge-remove\nWhat Google search helps you identify additional strategies for renaming missing data?"
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#optional-creating-a-data-frame-by-hand",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#optional-creating-a-data-frame-by-hand",
    "title": "Working with data",
    "section": "Optional: Creating a data frame by hand",
    "text": "Optional: Creating a data frame by hand\nThis last section shows two different approaches to creating a data frame by hand (in other words, without importing the data from a spreadsheet). It isn’t particularly useful for most of your day-to-day work, and also not a method you want to use often, as this type of data entry can introduce errors. However, it’s frequently used in online tutorials, which can be confusing, and also helps illustrate how data frames are composed.\nThe first approach is to create separate vectors (columns), and then join them together in a second step:\n\n# create individual vectors\ncancer &lt;- c(\"lung\", \"prostate\", \"breast\")\nmetastasis &lt;- c(\"yes\", \"no\", \"yes\")\ncases &lt;- c(30, 50, 100)\n# combine vectors\nexample_df1 &lt;- data.frame(cancer, metastasis, cases)\nstr(example_df1)\n\n'data.frame':   3 obs. of  3 variables:\n $ cancer    : chr  \"lung\" \"prostate\" \"breast\"\n $ metastasis: chr  \"yes\" \"no\" \"yes\"\n $ cases     : num  30 50 100\n\n\nThe resulting data frame has column headers, identified from the names of the vectors combined together.\nThe next way seems more complex, but represents the code above combined into one step:\n\n# create vectors and combine into data frame simultaneously\nexample_df2 &lt;- data.frame(cancer = c(\"lung\", \"prostate\", \"breast\"),\n                          metastasis = c(\"yes\", \"no\", \"yes\"),\n                          cases = c(30, 50, 100), stringsAsFactors = FALSE)\nstr(example_df2)\n\n'data.frame':   3 obs. of  3 variables:\n $ cancer    : chr  \"lung\" \"prostate\" \"breast\"\n $ metastasis: chr  \"yes\" \"no\" \"yes\"\n $ cases     : num  30 50 100\n\n\nAs we learned above, factors can be particularly difficult, so it’s useful to know that you can use stringsAsFactors = FALSE to import such data as character instead."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#wrapping-up",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#wrapping-up",
    "title": "Working with data",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this session, we learned to import data into R from a csv file, learned multiple ways to access parts of data frames, and manipulated factors.\nIn the next session, we’ll begin to explore a set of powerful, elegant data manipulation tools for data cleaning, transforming, and summarizing, and we’ll prepare some data to visualize in our final session."
  },
  {
    "objectID": "resources/computing/intro_to_r/2_working_with_data.html#extra-exercises",
    "href": "resources/computing/intro_to_r/2_working_with_data.html#extra-exercises",
    "title": "Working with data",
    "section": "Extra exercises",
    "text": "Extra exercises\nThe following exercises all use the same clinical data from this class.\n\nChallenge-disease-race\nExtract the last 100 rows for only disease and race and save to an object called disease_race.\n\n\nChallenge-min-max\nCalculate the minimum and maximum for days_to_death.\n\n\nChallenge-factors\nChange all of the factors of race to shorter names for each category, and appropriately indicate missing data."
  },
  {
    "objectID": "resources/computing/intro_to_r/index.html",
    "href": "resources/computing/intro_to_r/index.html",
    "title": "Introduction to R",
    "section": "",
    "text": "You might want to choose from other tutorials as they might suit your learning style better. See Section 0.1 for a list of other tutorials.\n\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nFunctions and objects\n\n\nR syntax, assigning objects, using functions\n\n\n\n\nWorking with data\n\n\nData types and structures; slicing and subsetting data\n\n\n\n\nData manipulation with tidyverse\n\n\nData manipulation with dplyr\n\n\n\n\nData visualization in R\n\n\nData visualization in ggplot2\n\n\n\n\nGood practices in R\n\n\nTips and tricks for writing better code\n\n\n\n\n\nNo matching items\n\n\n\n0.1 Additional resources\n\nR for Data Science (2e)\nData Analysis and Visualisation in R for Ecologists\nIntroduction to R for Geospatial Data\nHands-On Programming with R\n\n\n\n\n\n\n\nAttributions\n\n\n\nAll these classes are taken almost verbatim from fredhutch.io, the data and computational analysis training program at Fred Hutch, which was adapted from content originally appearing in R for data analysis and visualization of Ecological Data, Copyright (c) Data Carpentry."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html",
    "href": "resources/computing/intro_to_r/3_tidyverse.html",
    "title": "Data manipulation with tidyverse",
    "section": "",
    "text": "In the last section, we imported a clinical cancer dataset and learned to manipulate it using base R (tools included in every R installation).\nIn this session, we’ll continue to work with the same dataset, but will introduce a set of tools specifically designed for data science in R. By the end of this session, you should be able to:\n\ninstall and load packages\nuse tidyverse tools to import data and access rows/columns\ncombine commands using pipes\ntransform and summarize data"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#objectives",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#objectives",
    "title": "Data manipulation with tidyverse",
    "section": "",
    "text": "In the last section, we imported a clinical cancer dataset and learned to manipulate it using base R (tools included in every R installation).\nIn this session, we’ll continue to work with the same dataset, but will introduce a set of tools specifically designed for data science in R. By the end of this session, you should be able to:\n\ninstall and load packages\nuse tidyverse tools to import data and access rows/columns\ncombine commands using pipes\ntransform and summarize data"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#installing-and-loading-packages",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#installing-and-loading-packages",
    "title": "Data manipulation with tidyverse",
    "section": "Installing and loading packages",
    "text": "Installing and loading packages\nPlease ensure RStudio is open with your project directory path (e.g., ~/Desktop/intro_r) listed at the top of your Console. If you do not see the path to your project directory, go to File -&gt; Open Project and navigate to the location of your project directory. Alternatively, using your operating system’s file browser, double click on the r_intro.Rrpoj file.\nCreate a new R script called class3.R, and add # Introduction to R: Class 3 as a title.\nFor this lesson, we’ll be working with a group of R packages called tidyverse. A package is a group of related functions that help you accomplish particular tasks. tidyverse packages have been designed specifically to support tasks related to data science, such as data manipulation, filtering, and visualization.\nThe first thing we need to do is install the software:\n\n\nThe following package(s) will be installed:\n- tidyverse [2.0.0]\nThese packages will be installed into \"~/Documents/Teaching/computational_statistics/2023/new_website/renv/library/R-4.3/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing tidyverse ...                      OK [linked from cache]\nSuccessfully installed 1 package in 6 milliseconds.\n\n\nA few notes about installing packages:\n\nYou only need to perform this installation once per computer, or when updating R or the package.\nIf you see red text output in the Console during this installation, don’t be alarmed: this doens’t necessarily indicate a problem. You are seeing a report of the various pieces of software being downloaded and installed.\nIf prompted, you should install all packages (say yes or all), as well as yes to compiling any packages\nWhen the installation is complete (this may take several minutes), you’ll see the command prompt (&gt;) in your Console.\n\nOnce you have the software installed, you’ll need to load it:\n\n# load library/package\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nLoading packages is similar to opening a software application on your computer; it makes a previously installed set of software available for use. A few notes about loading packages:\n\nYou’ll need to load packages every time you open RStudio (or R restarts)\nLoading tidyverse loads a collection of packages; these are listed under “Attaching packages”\nThere are many other packages included as dependencies. If some of them did not install successfully, you will receive an error at this step. For this lesson, you can try library(dplyr), and ask your instructor for help later.\nThe section in the output above referencing “Conflicts” shows you which functions you just loaded have names identical to packages you already have loaded (in base R). This shouldn’t affect the code we write in this lesson, though it’s useful to know the double colon syntax (::) allows you to reference functions in a different package with same name.\n\nYou can check to make sure the new package we’ll be using is available by executing ?select in the Console, or by searching for that function in the help panel. You can also look in the “Packages” tab in the same panel. If the package (in this case, either tidyverse or dplyr) is present in the list, it’s installed. If the box next to the package name is checked, it’s loaded. In this lesson, if you receive an error saying a function isn’t available or recognized, check to make sure the package is loaded."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#selecting-columns-and-rows",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#selecting-columns-and-rows",
    "title": "Data manipulation with tidyverse",
    "section": "Selecting columns and rows",
    "text": "Selecting columns and rows\nThe first task we’ll undertake with our newly installed tidyverse tools is importing our data:\n\n# import data\nclinical &lt;- read_csv(\"data/clinical.csv\")\n\nRows: 6832 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): primary_diagnosis, tumor_stage, vital_status, morphology, state, t...\ndbl  (8): age_at_diagnosis, days_to_death, days_to_birth, days_to_last_follo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that this function looks similar to what we used in the last lesson (read.csv), but the underscore replacing the dot means it’s actually a different function. In fact, this is the data import function from tidyverse. The output provided by this function indicates a few key differences with our import yesterday.\nThe data import execution includes a description of how each variable (column) is interpreted. In our data’s case, the numeric data are col_double and the character data are col_character (not factors!).\nWe can explore these differences further:\n\n# inspect object\nstr(clinical)\n\nspc_tbl_ [6,832 × 20] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ primary_diagnosis          : chr [1:6832] \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ tumor_stage                : chr [1:6832] \"stage ia\" \"stage ib\" \"stage ib\" \"stage ia\" ...\n $ age_at_diagnosis           : num [1:6832] 24477 26615 28171 27154 29827 ...\n $ vital_status               : chr [1:6832] \"dead\" \"dead\" \"dead\" \"alive\" ...\n $ morphology                 : chr [1:6832] \"8070/3\" \"8070/3\" \"8070/3\" \"8083/3\" ...\n $ days_to_death              : num [1:6832] 371 136 2304 NA 146 ...\n $ state                      : chr [1:6832] \"live\" \"live\" \"live\" \"live\" ...\n $ tissue_or_organ_of_origin  : chr [1:6832] \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_birth              : num [1:6832] -24477 -26615 -28171 -27154 -29827 ...\n $ site_of_resection_or_biopsy: chr [1:6832] \"C34.1\" \"C34.1\" \"C34.3\" \"C34.1\" ...\n $ days_to_last_follow_up     : num [1:6832] NA NA 2099 3747 NA ...\n $ cigarettes_per_day         : num [1:6832] 10.96 2.19 1.64 1.1 NA ...\n $ years_smoked               : num [1:6832] NA NA NA NA NA NA NA NA NA NA ...\n $ gender                     : chr [1:6832] \"male\" \"male\" \"female\" \"male\" ...\n $ year_of_birth              : num [1:6832] 1936 1931 1927 1930 1923 ...\n $ race                       : chr [1:6832] \"white\" \"asian\" \"white\" \"white\" ...\n $ ethnicity                  : chr [1:6832] \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" \"not hispanic or latino\" ...\n $ year_of_death              : num [1:6832] 2004 2003 NA NA 2004 ...\n $ bcr_patient_barcode        : chr [1:6832] \"TCGA-18-3406\" \"TCGA-18-3407\" \"TCGA-18-3408\" \"TCGA-18-3409\" ...\n $ disease                    : chr [1:6832] \"LUSC\" \"LUSC\" \"LUSC\" \"LUSC\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   primary_diagnosis = col_character(),\n  ..   tumor_stage = col_character(),\n  ..   age_at_diagnosis = col_double(),\n  ..   vital_status = col_character(),\n  ..   morphology = col_character(),\n  ..   days_to_death = col_double(),\n  ..   state = col_character(),\n  ..   tissue_or_organ_of_origin = col_character(),\n  ..   days_to_birth = col_double(),\n  ..   site_of_resection_or_biopsy = col_character(),\n  ..   days_to_last_follow_up = col_double(),\n  ..   cigarettes_per_day = col_double(),\n  ..   years_smoked = col_double(),\n  ..   gender = col_character(),\n  ..   year_of_birth = col_double(),\n  ..   race = col_character(),\n  ..   ethnicity = col_character(),\n  ..   year_of_death = col_double(),\n  ..   bcr_patient_barcode = col_character(),\n  ..   disease = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nYou may notice the presence of tbl and related labels in the classes for this object. tbl stands for tibble, which is a type of data frame with specific constraints to ensure better data handling.\nIf you preview the dataset, it will look the same, and we can interact with the data in the same way. These assumptions about the data mesh nicely with the other tools in the tidyverse.\nNow that our data are imported, we can explore the tidyverse functions for extracting parts of the dataset.\nFirst, we can explore selecting certain columns by name:\n\n# selecting columns with tidyverse (dplyr)\nsel_columns &lt;- select(clinical, tumor_stage, ethnicity, disease)\n\nThe syntax for the select function is to specify the dataset first, then the names of each of the columns you would like to retain in the output object. If we look at the object, we’ll see it has only three columns but all rows.\nYou’ll note that the column headers don’t require quotation marks; this is a shortcut programmed into tidyverse functions.\nAs with base R functions, we can also select a range of columns:\n\n# select range of columns\nsel_columns2 &lt;- select(clinical, tumor_stage:vital_status)\n\nIn addition to these approaches, we can also use other helper functions for selecting columns: starts_with(), ends_with(), and contains() are examples that assist in extracting columns with headers that meet certain conditions. For example, using starts_with(tumor) in place of the column names will give you all columns that start with the word tumor.\nWe can use a separate function to extract rows that meet particular conditions:\n\n# select rows conditionally: keep only lung cancer cases\nfiltered_rows &lt;- filter(clinical, disease == \"LUSC\") \n\nThe syntax here is similar to select, and the conditional filters can be applied in similarly to base R functions.\n\nChallenge-columns\nCreate a new object from clinical called race_disease that includes only the race, &gt; ethnicity, and disease columns.\n\n\nChallenge-rows\nCreate a new object from race_disease called race_BRCA that includes only BRCA (from disease)."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#combining-commands",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#combining-commands",
    "title": "Data manipulation with tidyverse",
    "section": "Combining commands",
    "text": "Combining commands\nThe last challenges used an intermediate object to obtain an object with two subsetting methods applied. It’s common in data science to apply more than one requirement for extracting data. If you want to avoid creating an intermediate object, you could nest one command inside the other:\n\n# same task as challenges, but nested commands \nrace_BRCA2 &lt;- select(filter(clinical, disease == \"BRCA\"), race, ethnicity, disease)\n\nIn this case, filter(clinical, disease == \"BRCA\") becomes the input for select.\nWhile this is a common approach, especially in base R, it can be difficult for us as coders to read and interpret the code.\nOne of the most useful features of tidyverse is its inclusion of a programming method called pipes. This approach can be found in many programming languages, in part because of its utility: a pipe sends the output from the lefthand side of the symbol as the input for the righthand side. In R, pipes are represented as %&gt;%.\nWe can use pipes to connect the same two data extraction tasks:\n\n# same task as above, but with pipes\npiped &lt;- clinical %&gt;%\n  select(race, ethnicity, disease) %&gt;%\n  filter(disease == \"BRCA\")\n\nThe command above starts by naming the object that will result from this assignment. The dataset is named as the first input. Because executing the name of an object sends the object contents as output, this means the second line receives the object as input. The output from the select line is sent as input to the filter line. This effectively demonstrates how pipes can be used to connect multiple commands together.\n\nNow that we are running code in chunks that span multiple lines, you can see one of the other nice features of RStudio: your cursor can be placed on any line of the multi-line chunk when you execute, and the entire set of code will run together.\n\nThese examples also help highlight the importance of style and convention in code formatting. After the first line, the code is indented. While this isn’t necessary for the code to work, it does make it a lot easier to read and understand the code.\nLet’s take a look at another example of piped commands:\n\n# extract race, ethinicity, and disease from cases born prior to 1930\npiped2 &lt;- clinical %&gt;%\n  filter(year_of_birth &lt; 1930) %&gt;%\n  select(race, ethnicity, disease)\n\nIn the code above, we’re applying a mathematical condition to find specific rows, and the selecting certain columns. Does the order of commands differ? We can switch the order of the filter and select lines to see:\n\npiped3 &lt;- clinical %&gt;%\n  select(race, ethnicity, disease) %&gt;%\n  filter(year_of_birth &lt; 1930)\n\nThe code above should give you an error, because in this case, the order does matter! The output from the second line does not include the year_of_birth column, so R is unable to apply the filter in the third line.\n\nChallenge-pipes\nUse pipes to extract the columns gender, years_smoked, and year_of_birth from the object clinical for only living patients (vital_status) who have smoked fewer than 1 cigarettes_per_day."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#mutate",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#mutate",
    "title": "Data manipulation with tidyverse",
    "section": "Mutate",
    "text": "Mutate\nThis lesson so far has mostly shown new ways of accomplishing the same tasks we learned in the last lesson. tidyverse includes much more functionality, however, including the ability to mutate columns. Common tasks for which mutate is useful include unit conversions, transformation, and creating ratios from among existing columns.\nWe can use this function to convert the days_to_death column to years:\n\n# convert days to years\nclinical_years &lt;- clinical %&gt;%\n  mutate(years_to_death = days_to_death / 365)\n\nThe actual conversion works by providing a formula (days_to_death / 365) and the name of the new column (years_to_death). If you inspect the resulting object, you’ll see years_to_death added as a new column at the end of the table.\n\nmutate works by retaining all previous columns and creating new columns as per the formula specified. tidyverse also includes transmute, which drops the existing columns used to calculcate the new columns.\n\nWe can use mutate to perform multiple conversions at once:\n\n# convert days to year and months at same time, and we don't always need to assign to object\nclinical %&gt;%\n  mutate(years_to_death = days_to_death / 365,\n         months_to_death = days_to_death / 30) %&gt;%\n  glimpse() # preview data output\n\nRows: 6,832\nColumns: 22\n$ primary_diagnosis           &lt;chr&gt; \"C34.1\", \"C34.1\", \"C34.3\", \"C34.1\", \"C34.3…\n$ tumor_stage                 &lt;chr&gt; \"stage ia\", \"stage ib\", \"stage ib\", \"stage…\n$ age_at_diagnosis            &lt;dbl&gt; 24477, 26615, 28171, 27154, 29827, 23370, …\n$ vital_status                &lt;chr&gt; \"dead\", \"dead\", \"dead\", \"alive\", \"dead\", \"…\n$ morphology                  &lt;chr&gt; \"8070/3\", \"8070/3\", \"8070/3\", \"8083/3\", \"8…\n$ days_to_death               &lt;dbl&gt; 371, 136, 2304, NA, 146, NA, 345, 716, 280…\n$ state                       &lt;chr&gt; \"live\", \"live\", \"live\", \"live\", \"live\", \"l…\n$ tissue_or_organ_of_origin   &lt;chr&gt; \"C34.1\", \"C34.1\", \"C34.3\", \"C34.1\", \"C34.3…\n$ days_to_birth               &lt;dbl&gt; -24477, -26615, -28171, -27154, -29827, -2…\n$ site_of_resection_or_biopsy &lt;chr&gt; \"C34.1\", \"C34.1\", \"C34.3\", \"C34.1\", \"C34.3…\n$ days_to_last_follow_up      &lt;dbl&gt; NA, NA, 2099, 3747, NA, 3576, NA, NA, 1810…\n$ cigarettes_per_day          &lt;dbl&gt; 10.9589041, 2.1917808, 1.6438356, 1.095890…\n$ years_smoked                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 26…\n$ gender                      &lt;chr&gt; \"male\", \"male\", \"female\", \"male\", \"male\", …\n$ year_of_birth               &lt;dbl&gt; 1936, 1931, 1927, 1930, 1923, 1942, 1953, …\n$ race                        &lt;chr&gt; \"white\", \"asian\", \"white\", \"white\", \"not r…\n$ ethnicity                   &lt;chr&gt; \"not hispanic or latino\", \"not hispanic or…\n$ year_of_death               &lt;dbl&gt; 2004, 2003, NA, NA, 2004, NA, 2005, 2006, …\n$ bcr_patient_barcode         &lt;chr&gt; \"TCGA-18-3406\", \"TCGA-18-3407\", \"TCGA-18-3…\n$ disease                     &lt;chr&gt; \"LUSC\", \"LUSC\", \"LUSC\", \"LUSC\", \"LUSC\", \"L…\n$ years_to_death              &lt;dbl&gt; 1.0164384, 0.3726027, 6.3123288, NA, 0.400…\n$ months_to_death             &lt;dbl&gt; 12.366667, 4.533333, 76.800000, NA, 4.8666…\n\n\nThe code above also features a new function, glimpse, that can be useful when developing new piped code. Note that we did not assign the output above to a new object; we allowed it to be printed to the Console. Because this is a large dataset, that type of output can be unweildy. glimpse allows us to see a preview of the data, including the two new columns created.\n\nChallenge-lung\nExtract only lung cancer patients (LUSC, from disease) and create a new column called total_cig representing an estimate of the total number of cigarettes smoked (use columns years_smoked and cigarettes_per_day)."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#split-apply-combine",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#split-apply-combine",
    "title": "Data manipulation with tidyverse",
    "section": "Split-apply-combine",
    "text": "Split-apply-combine\nOur clinical dataset includes categorical (character) data. One example is the gender column. We can assess the different categories available using a base R function:\n\n# show categories in gender\nunique(clinical$gender)\n\n[1] \"male\"   \"female\" NA      \n\n\ntidyverse includes an approach called split-apply-combine that allows us to:\n\nsplit data into groups,\napply a task for each group,\ncombine the results back together into a single table.\n\nWe can try out this approach by counting the number of each gender in our dataset:\n\n# count number of individuals of each gender\nclinical %&gt;%\n  group_by(gender) %&gt;%\n  tally() \n\n# A tibble: 3 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 female  3535\n2 male    3258\n3 &lt;NA&gt;      39\n\n\ngroup_by is not particularly useful by itself, but powerful together with a second function like tally. The two columns in the resulting tibble represent the categories from group_by and the number of cases for each gender (n).\nAn additional function for use with group_by is summarize:\n\n# summarize average days to death by gender\nclinical %&gt;%\n  group_by(gender) %&gt;%\n  summarize(mean_days_to_death = mean(days_to_death, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  gender mean_days_to_death\n  &lt;chr&gt;               &lt;dbl&gt;\n1 female               947.\n2 male                 826.\n3 &lt;NA&gt;                 NaN \n\n\nSimilar to mutate, we provide summarize with a formula indicating how we would like the groups to be handled.\nIn the command above, we use na.rm = TRUE to exclude missing data from the calculation of mean from days_to_death. We still have NA reported in the output table, though, because of the NA category in gender.\nWe can apply an additional filter to remove this missing data, prior to grouping:\n\n# remove NA\nclinical %&gt;%\n  filter(!is.na(gender)) %&gt;%\n  group_by(gender) %&gt;%\n  summarize(mean_days_to_death = mean(days_to_death))\n\n# A tibble: 2 × 2\n  gender mean_days_to_death\n  &lt;chr&gt;               &lt;dbl&gt;\n1 female                 NA\n2 male                   NA\n\n\n\nChallenge-smoke-complete\nCreate object called smoke_complete from clinical that contains no missing data for cigarettes per day or age at diagnosis.\n\n\nChallenge-save\nHow do you save resulting table to file? How would you find this answer?\n\nThe solution to the challenges above represent the first of two datasets we’ll be using for data visualization in our next class. Make sure you’ve executed this code to save the filtered data file for use next time:\n\nsmoke_complete &lt;- clinical %&gt;%\n  filter(!is.na(age_at_diagnosis)) %&gt;%\n  filter(!is.na(cigarettes_per_day))\nwrite_csv(smoke_complete, \"data/smoke_complete.csv\")\n\nThe command above uses write_csv, which is the tidyverse method of saving a csv file. Base R possesses a function, write.csv, that performs a similar task, but by default includes quotation marks around cells with character data as well as row names (sequential numbers, unless otherwise specified).\n\nChallenge-birth-complete\nCreate a new object called birth_complete that contains no missing data for year of birth or vital status.\n\nThis challenge begins filtering the second of our two datasets for next time. Make sure you include the filter to remove missing data that’s been encoded as “not reported”!\n\n# make sure ALL missing data is removed!\nbirth_complete &lt;- clinical %&gt;%\n  filter(!is.na(year_of_birth)) %&gt;%\n  filter(!is.na(vital_status)) %&gt;%\n  filter(vital_status != \"not reported\")"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#filtering-data-based-on-number-of-cases-of-each-type",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#filtering-data-based-on-number-of-cases-of-each-type",
    "title": "Data manipulation with tidyverse",
    "section": "Filtering data based on number of cases of each type",
    "text": "Filtering data based on number of cases of each type\nWe’re going to perform one last manipulation on this second dataset for next time, which will allow us to reduce the total number of cancer types present in this dataset.\nFirst, we’ll need to count how many cases for each cancer type exist in the dataset:\n\n# counting number of records in each cancer\ncancer_counts &lt;- clinical %&gt;%\n  count(disease) %&gt;%\n  arrange(n) \n\nThe count function is similar to tally, but doesn’t need to have group_by applied first. The arrange function added at the end sorts the table using the column specified. Although this isn’t necessary for the analysis to proceed, it makes it easier for us to interpret the results.\nNext, we’ll identify which cancer types are represented by at least 500 cases in this dataset:\n\n# get names of frequently occurring cancers\nfrequent_cancers &lt;- cancer_counts %&gt;%\n  filter(n &gt;= 500) \n\nWe can then use this object to filter based on the number of cases:\n\n# extract data from cancers to keep\nbirth_reduced &lt;- birth_complete %&gt;%\n  filter(disease %in% frequent_cancers$disease)\n\nThe new syntax here is %in%, which allows you to compare each entry in disease from birth_complete to the disease column in frequent_cancers (remember that frequent_cancers$disease means the disease column from frequent_cancers). This keeps only cases from the birth_complete dataset that are from cancers that are frequently occurring.\nFinally, we’ll write the final output to a file:\n\n# save results to file in data/ named birth_reduced\nwrite_csv(birth_reduced, \"data/birth_reduced.csv\")\n\n\nChallenge-tumor\nExtract all tumor stages with more than 200 cases (Hint: also check to see if there are any other missing/ambiguous data!)"
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#wrapping-up",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#wrapping-up",
    "title": "Data manipulation with tidyverse",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this session, we acquainted ourselves with tidyverse, and learned some tools for data filtering and manipulation. We covered examples from many of the main categories of data manipulation tasks; if you’d like more information on these functions and others available (including methods of joining multiple tables together), please check out the dplyr cheatsheet.\nIn the next session, we’ll wrap up the course by creating publication-quality images using ggplot2, a data visualization package in tidyverse, and the two datasets we filtered in the sections above."
  },
  {
    "objectID": "resources/computing/intro_to_r/3_tidyverse.html#extra-exercises",
    "href": "resources/computing/intro_to_r/3_tidyverse.html#extra-exercises",
    "title": "Data manipulation with tidyverse",
    "section": "Extra exercises",
    "text": "Extra exercises\nThe following exercises all use the same clinical data from this class.\n\nChallenge-ethnicity\nHow many hispanic or latino individuals in clinical are not also white? What are their races?\n\n\nChallenge-years\nCreate a new column for clinical called age_at_death that calculates this statistic (in years) from year_of_birth and year_of_death.\n\n\nChallenge-helpers\ndplyr includes several “helpers” that allows selection of columns meeting particular criteria (described on the first page of the dplyr cheatsheet near the top of the right hand column: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf). Using one of these tools, extract all columns that include “diagnosis”.\n\n\nChallenge-combine\nHow many patients are hispanic or latino patients (column ethnicity), died after the year 2000 (year_of_death), and possess no missing data for cigarettes per day?"
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html",
    "title": "Good practices in Julia",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction"
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html#writing-clean-and-readable-code",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html#writing-clean-and-readable-code",
    "title": "Good practices in Julia",
    "section": "Writing Clean and Readable Code",
    "text": "Writing Clean and Readable Code\n# Good: Use meaningful variable names and comments\npopulation_size = 1000  # Number of individuals in the population\nsample_size = 100       # Number of individuals in the sample\n\n# Avoid: Using unclear or abbreviated variable names\nps = 1000\nss = 100"
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html#using-comments-and-documentation",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html#using-comments-and-documentation",
    "title": "Good practices in Julia",
    "section": "Using Comments and Documentation",
    "text": "Using Comments and Documentation\n# Good: Add comments to explain complex operations or logic\nfunction calculate_mean(data)\n    # Calculate the sum of elements in the data\n    total = sum(data)\n    \n    # Calculate the mean by dividing the total by the number of elements\n    mean_value = total / length(data)\n    \n    return mean_value\nend\n\n# Avoid: Lack of comments and explanation\nfunction calc_mean(d)\n    t = sum(d)\n    m = t / length(d)\n    return m\nend"
  },
  {
    "objectID": "resources/computing/intro_to_julia/good_practice_julia.html#avoiding-global-variables-and-code-dependencies",
    "href": "resources/computing/intro_to_julia/good_practice_julia.html#avoiding-global-variables-and-code-dependencies",
    "title": "Good practices in Julia",
    "section": "Avoiding Global Variables and Code Dependencies",
    "text": "Avoiding Global Variables and Code Dependencies\n# Good: Use local variables within functions to avoid global scope pollution\nfunction calculate_variance(data)\n    n = length(data)\n    mean_value = sum(data) / n\n    \n    # Calculate the variance using local variables\n    variance = sum((data .- mean_value).^2) / (n - 1)\n    \n    return variance\nend\n\n# Avoid: Using global variables in functions\nmean_value = 0\nfunction calc_variance(data)\n    n = length(data)\n    global mean_value = sum(data) / n\n    \n    # Calculate the variance using global variables\n    variance = sum((data .- mean_value).^2) / (n - 1)\n    \n    return variance\nend"
  },
  {
    "objectID": "resources/resources.html",
    "href": "resources/resources.html",
    "title": "All the additional resources",
    "section": "",
    "text": "Supplementary material for the course\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nOLS estimator\n\n\nDeriving the Least-Squares Estimates for Simple Linear Regression\n\n\n\n\nEDA example\n\n\nData exploration of insurance redlining data\n\n\n\n\n\nNo matching items\n\n\n\n\nTips and tricks\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nBest practices for collaborative work\n\n\nUsing code style guidelines, sharing code and collaborating.\n\n\n\n\nGood practices for coding\n\n\nA collection of tips and tricks to write better code\n\n\n\n\nReproducible scientific computing\n\n\nWhy and how to make your scientific computing reproducible.\n\n\n\n\nVirtual environments\n\n\nHow to make it possible for others to run your code with the same dependencies as you\n\n\n\n\n\nNo matching items\n\n\n\n\nTutorials\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nGit and GitHub\n\n\nA guide to set-up Git and GitHub\n\n\n\n\nGithub Classroom\n\n\nHow to use Github Classroom to submit assignments\n\n\n\n\nSoftware installation\n\n\nHow to have everything ready for the course\n\n\n\n\n\nNo matching items\n\n\n\n\nIntroduction to Programming\n\n\n\n\n\n\nNote\n\n\n\nYou might want to choose from other tutorials as they might suit your learning style better. See the additional ressources for each language.\n\n\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nR cheatsheets\n\n\nA collection of cheatsheets forR\n\n\n\n\nIntroduction to R\n\n\nA collection of resources for learning R\n\n\n\n\nIntroduction to Julia\n\n\nA collection of resources for learning Julia\n\n\n\n\nIntroduction to Python\n\n\nA collection of resources for learning Python\n\n\n\n\n\nNo matching items\n\n\n\nAdditional resources\n\nR for Data Science (2e)\nData Analysis and Visualisation in R for Ecologists\nIntroduction to R for Geospatial Data"
  },
  {
    "objectID": "resources/tutorials/installing_software.html",
    "href": "resources/tutorials/installing_software.html",
    "title": "Software installation",
    "section": "",
    "text": "Mac\n\nbrew install r\n\nLinux & Windows\n\nGo to https://cran.r-project.org/ and select the appropriate link for your operating system. Install R from the downloaded executable.\n\n\n\nGo to https://www.python.org/downloads/ and follow the instructions to install Python.\n\n\n\nGo to https://github.com/JuliaLang/juliaup and follow the instructions to install Julia."
  },
  {
    "objectID": "resources/tutorials/installing_software.html#r",
    "href": "resources/tutorials/installing_software.html#r",
    "title": "Software installation",
    "section": "",
    "text": "Mac\n\nbrew install r\n\nLinux & Windows\n\nGo to https://cran.r-project.org/ and select the appropriate link for your operating system. Install R from the downloaded executable."
  },
  {
    "objectID": "resources/tutorials/installing_software.html#python",
    "href": "resources/tutorials/installing_software.html#python",
    "title": "Software installation",
    "section": "",
    "text": "Go to https://www.python.org/downloads/ and follow the instructions to install Python."
  },
  {
    "objectID": "resources/tutorials/installing_software.html#julia",
    "href": "resources/tutorials/installing_software.html#julia",
    "title": "Software installation",
    "section": "",
    "text": "Go to https://github.com/JuliaLang/juliaup and follow the instructions to install Julia."
  },
  {
    "objectID": "resources/tutorials/installing_software.html#visual-studio-code",
    "href": "resources/tutorials/installing_software.html#visual-studio-code",
    "title": "Software installation",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\nGo to https://code.visualstudio.com/ and follow the instruction. You can choose the nightly or stable build. I am personally using the nightly build, but would not recommend it if you are not comfortable with debugging random issues that might arise.\nIt is a good idea to install extension from the visual studio marketplace. You can do that by clicking on the extension icon on the left side of the screen. I would recommend the following extensions:\n\nfor Julia: https://www.julia-vscode.org/\nfor Python: follow this https://code.visualstudio.com/docs/python/editing\nfor R : https://code.visualstudio.com/docs/languages/r"
  },
  {
    "objectID": "resources/tutorials/installing_software.html#rstudio",
    "href": "resources/tutorials/installing_software.html#rstudio",
    "title": "Software installation",
    "section": "RStudio",
    "text": "RStudio\nGo to posit.co and download the RStudio IDE. Choose RStudio Desktop free version and make sure the correct operating system was selected. Install RStudio from the downloaded executable. Open RStudio.\n\n\n\n\n\n\nNote\n\n\n\nFor help with git and GitHub, please check this tutorial"
  },
  {
    "objectID": "resources/tutorials/github_classroom.html",
    "href": "resources/tutorials/github_classroom.html",
    "title": "Github Classroom",
    "section": "",
    "text": "We will use Github Classroom for the assignments in this course. This document will give you a brief introduction to Github Classroom and how to use it."
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#introduction",
    "href": "resources/tutorials/github_classroom.html#introduction",
    "title": "Github Classroom",
    "section": "Introduction",
    "text": "Introduction\nThis guide is here to help you get started. GitHub Classroom is a platform that we will use to manage and distribute assignments, making it easier for you to collaborate on coursework."
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#what-is-github-classroom",
    "href": "resources/tutorials/github_classroom.html#what-is-github-classroom",
    "title": "Github Classroom",
    "section": "What is GitHub Classroom?",
    "text": "What is GitHub Classroom?\nGitHub Classroom is a tool that simplifies the process of creating, distributing, and submitting assignments on GitHub. It leverages the power of Git, a version control system, and GitHub, a web-based platform for code hosting and collaboration. With GitHub Classroom, you’ll be able to access your assignments, work on them, and submit your work – all in one place."
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#prerequisites",
    "href": "resources/tutorials/github_classroom.html#prerequisites",
    "title": "Github Classroom",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore we dive into using GitHub Classroom, make sure you have the following:\n\nGitHub Account: If you don’t already have a GitHub account, you can sign up for free at GitHub. This account will be essential for participating in assignments.\nGit Installed: Git is a tool that helps you manage your code changes. You can download and install Git from the official website: Git Downloads. See this tutorial if you need help installing Git."
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#getting-started",
    "href": "resources/tutorials/github_classroom.html#getting-started",
    "title": "Github Classroom",
    "section": "Getting Started",
    "text": "Getting Started\nLet’s begin by taking the first steps with GitHub Classroom:\n\n1. Accept the Classroom Invitation\nYour instructor will provide you with a link to the GitHub Classroom assignment. Click on the link, and it will guide you to accept the invitation. This action will create a private repository for your assignment within your GitHub account.\n\n\n2. Clone the Repository\nNext, you’ll want to bring the assignment to your computer so you can work on it. To do this, you need to clone the assignment repository. Don’t worry; cloning simply means making a copy of the assignment on your computer.\n\nOpen a terminal or Git Bash (if you installed Git).\nNavigate to the directory where you want to store your assignment (e.g., Documents).\nUse the following command, replacing &lt;repository_url&gt; with the URL of your assignment repository (you can find it on the GitHub page of your assignment):\n\ngit clone &lt;repository_url&gt;\n\n\n3. Work on the Assignment\nNow that you have the assignment on your computer, explore the repository. Inside, you’ll find instructions and possibly some starter code or files. These will guide you on what to do. You can use any code editor or development environment you’re comfortable with.\n\n\n4. Save Your Work\nWhile working on your assignment, be sure to save your progress regularly. In the Git world, this is called “committing.” It helps you keep track of your changes and lets you go back to previous versions if needed.\n\nTo commit your changes, open the terminal in the project folder and run these commands:\n\ngit add .\ngit commit -m \"Your commit message here\"\n\n\n\n\n\n\nNote\n\n\n\nYour commit message should be short and descriptive. It should explain what changes you made in this commit. See for example this article for some tips on how to write good commit messages.\n\n\n\n\n5. Share Your Progress\nTo collaborate with classmates or ask for help from your instructor, you need to share your work with them on GitHub. You do this by “pushing” your changes to GitHub. This action will update your assignment repository on GitHub with your latest changes. You don’t have to do it everytime you make a change, but keep in mind that you’ll need to push your changes before submitting your assignment, and that pushing helps you keep your work safe in case something happens to your computer. When working in group, don’t forget to pull the changes from your teammates before pushing your own changes.\n\nTo push your changes to GitHub, use this command:\n\ngit push\n\n\n6. Submit Your Assignment\nWhen you’re satisfied with your work and ready to turn it in, you’ll need to submit your assignment through GitHub. This is typically done through a “Pull Request” on GitHub. Your instructor will provide instructions on how to submit. (If nothing is mentioned, you can assume that you only need to push your work on the main branch.)\n\n\n7. Review Feedback\nAfter the submission deadline, your instructor will review your assignment and may provide feedback or grades through GitHub. You can see this feedback by checking the pull request in your assignment repository on GitHub."
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#conclusion",
    "href": "resources/tutorials/github_classroom.html#conclusion",
    "title": "Github Classroom",
    "section": "Conclusion",
    "text": "Conclusion\nGitHub Classroom is a valuable tool that can make your Master’s program coursework more manageable and collaborative. While it might seem intimidating at first, remember that it’s a skill worth learning, and you can always reach out to your instructor or classmates for assistance. With this guide, you’re well on your way to successfully using GitHub Classroom for your assignments. Happy learning and coding!"
  },
  {
    "objectID": "resources/tutorials/github_classroom.html#tips-and-tricks",
    "href": "resources/tutorials/github_classroom.html#tips-and-tricks",
    "title": "Github Classroom",
    "section": "Tips and tricks",
    "text": "Tips and tricks\nThere exists an integration with VSCode that can make your life easier when working with Github Classroom if you are using VSCode."
  },
  {
    "objectID": "projects/project-02.html",
    "href": "projects/project-02.html",
    "title": "Main project",
    "section": "",
    "text": "More info to be posted."
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "Help & FAQ",
    "section": "",
    "text": "Tip\n\n\n\nIf you do not find the answer to your question here, you can use the search bar on the top left to search the entire site. If you are still unable to find the answer, you can ask your question during the exercise hours. If you found the solution and think it would be useful to other students, you can report an issue (see on the right) with the questions and the solution, and we will add it to the FAQ."
  },
  {
    "objectID": "course-faq.html#faq",
    "href": "course-faq.html#faq",
    "title": "Help & FAQ",
    "section": "FAQ",
    "text": "FAQ\n\nOrganizational\n\nWhich language will we be using?\nYou can use either Python,R, or Julia. We will be providing code examples and suppport in all three languages.\n\n\nWhat software do I need to install?\nYou will need to use git, and a text editor or IDE.\nFor git, please see here for instructions on how to install it.\nYou can use any kind of text editor or IDE you like. We recomment using VS Code with the extension corresponding to your language of choice. You can also use Rstudio for R. See here for instructions on how to install the software and IDEs.\n\n\nHow do I submit an assignment?\nWe will be using Github-classroom to distribute and collect assignments. You will need to create a Github account if you don’t already have one.\n\n\n\nTechnical issues\n\nI am lost with Github, what should I do?\nGitHub has possibly the best tutorials and documentation of any software out there. You can find the Hello world there. Most of the time googling your issue will lead you to the right place.\n\n\nI have bugs in my code/ something doesn’t work/ I don’t know how to do something, what should I do?\nFirst google your issue. 90 times out of 100 this will solve your issues. Another 9% can be resolved by reading the documentation of the software you are using. For the very last percent you can ask your question during the exercises hours.\n\n\nI cannot find the repository linked to the assignment, what should I do?\nAt the top of each assignment there should be a link you can manually click to accept the assignment. If this happens after the first assignment, please let us know."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "The course will provide the opportunity to tackle real world problems requiring advanced computational skills and visualisation techniques to complement statistical thinking. Students will practice proposing efficient solutions, and effectively communicating the results with stakeholders."
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "The course will provide the opportunity to tackle real world problems requiring advanced computational skills and visualisation techniques to complement statistical thinking. Students will practice proposing efficient solutions, and effectively communicating the results with stakeholders."
  },
  {
    "objectID": "course-syllabus.html#content",
    "href": "course-syllabus.html#content",
    "title": "Syllabus",
    "section": "Content",
    "text": "Content\n\nModern statistical computing environments (e.g., R, Rstudio and Python)\nOverview of other software (e.g., MATLAB, Julia)\nAids to efficiency and reproducibility (e.g., GitHub, Markdown, Jupyter)\nData management, wrangling, and ethics\nStatistical graphics (grammar, good practices, applications, and examples)\nEM algorithm and applications\nKernel density estimation and smoothing\nResampling methods for uncertainty assessment (bootstrap, jackknife, cross-validation), with applications to regression, time series and dependent data\nMarkov chain Monte Carlo techniques (Gibbs sampler, Metropolis-Hastings algorithm, Hamiltonian Monte Carlo, convergence diagnostics) and software (e.g., Stan)\nOther methods for Bayesian inference (e.g., importance sampling, INLA, AGHQ, …)"
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nRequired courses: Probability and statistics, Linear models"
  },
  {
    "objectID": "course-syllabus.html#learning-outcomes",
    "href": "course-syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the course, the student must be able to:\n\nPlan complex visualisation and computational tasks\nPerform complex visualisation and computational tasks\nImplement reproducible computational solutions to statistical problems in modern environments and platforms"
  },
  {
    "objectID": "course-syllabus.html#transversal-skills",
    "href": "course-syllabus.html#transversal-skills",
    "title": "Syllabus",
    "section": "Transversal skills",
    "text": "Transversal skills\n\nTake feedback (critique) and respond in an appropriate manner.\nCommunicate effectively with professionals from other disciplines.\nDemonstrate the capacity for critical thinking\nIdentify the different roles that are involved in well-functioning teams and assume different roles, including leadership roles."
  },
  {
    "objectID": "projects/project-tips-resources.html",
    "href": "projects/project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "projects/project-01.html",
    "href": "projects/project-01.html",
    "title": "Small project",
    "section": "",
    "text": "The goal of this project is data exploration. Find an interesting (in the sense it interests you!) data set and\nHere is an example of what your report could look like. The first step should be done individually. Then you can form groups of 2-3 and pick up the most interesting data set and do the rest."
  },
  {
    "objectID": "projects/project-01.html#some-links-to-open-data",
    "href": "projects/project-01.html#some-links-to-open-data",
    "title": "Small project",
    "section": "Some Links to Open Data",
    "text": "Some Links to Open Data\nfivethirtyeight: article data of Nate Silver’s data journalism platform freely available (see also R package fivethirtyeight)\ndata-is-plural: weekly newsletter of datasets by Jeremy Singer-Vine\nre3data: Registry of research data repositories\nopenml datasets: many uniformly formatted datasets for training machine learning models – however, not always good descriptions available\nWorldbank Datacatalog: the World Bank data catalogue\nUK Data Service: UK’s largest collection of social, economic and population data resources (filter for open data) or also data.gov.uk\nICPSR: unit within the Institute for Social Research at the University of Michigan, social and behavioral research. In particular including replication datasets for published studies.\ngovdata: Open Government - German administrative data freely accessible\ngapminder: “an independent educational non-proﬁt ﬁghting global misconceptions”; collection and vizualisation of datasets concerning gobal developement\nnature.com: peer-reviewed, open-access journal for descriptions of datasets (broad range of natural science disciplines)\nNIH (National Institute of Health) Data Sharing Repositories: overview on different thematically sorted medical databases\nUCI Machine Learning Repository or the new beta version: containing various datasets – however, sometimes with a little few description\ndata.bris Research Data Repository: Data repository of the University of Bristol\n… no systematic selection. Much more out there"
  },
  {
    "objectID": "resources/tutorials/github.html",
    "href": "resources/tutorials/github.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "Important\n\n\n\nPlease read the full guide if something does not work: it is very likely that the answer to your problem lies just a few lines after wherever you stopped reading.\nFirst follow this set up Git tutorial and then follow this tutorial to get a gentle introduction to Git and GitHub."
  },
  {
    "objectID": "resources/tutorials/github.html#installing-git",
    "href": "resources/tutorials/github.html#installing-git",
    "title": "Git and GitHub",
    "section": "Installing Git",
    "text": "Installing Git\nDownload an appropriate installer from git-scm.org and go through the installation process. If you are not sure how useful Git will be to you, it is advisable to uncheck Windows Explorer Integration (or similar for other operating system) on the second screen. Otherwise use the default setup, but make sure that you allow to use “Git from the command line and also from 3rd-party software”."
  },
  {
    "objectID": "resources/tutorials/github.html#creating-a-github-account",
    "href": "resources/tutorials/github.html#creating-a-github-account",
    "title": "Git and GitHub",
    "section": "Creating a GitHub account",
    "text": "Creating a GitHub account\n\nSign in\nGo to github.org and sign up. After providing your e-mail, choosing a password and your GitHub name, and verifying your e-mail, choose that you will be working with 2-5 people and that you are a student. Skip the next option, and then just choose a free account.\nNow you are on GitHub and your account is empty (there is no repo yet). That is alright, keep the tab open, we will return to it in a bit.\n\n\nSSH keys\nIn order to interact with the git repository, we need to link a SSH key that will identify you (see it as a complicated password that you won’t have to type). In the past it was possible to not use a SSH key but github does not allow it anymore.\nWe present 2 options to create/add a SSH key to your github account. If it does not work out of the box, please refer to this link before panicking.\nCLI:\nThis will require you to use the terminal (recommended). You can access the terminal in RStudio by going to Tools → Terminal → Move focus to terminal.\nCheck this github tutorial and follow the steps. Make sure to pick the correct option for your operating system (Mac, Windows, Linux).\nGUI with RStudio\nFirstly, check whether you already have an SSH key by running\n\nfile.exists(\"~/.ssh/id_rsa.pub\")\n\nin RStudio console.\nIf that returns FALSE, go to Tools -&gt; Global Options -&gt; Git/SVN. (Maybe there is already an SSH key address, just in a different location, so try running file.exists with the correct location, if you see one. If you don’t see any SSH key location…) Make sure the Git executable is available. Click “Create RSA Key” and click “Create” again. If you get “Access Denied” error, restart RStudio, this time as as an administrator. Close the pop-up, click “Apply”. Still in the Git/SVN tab in Global Options, click “View public key”. A pop-up with a long string will appear. Keep this open.\nGo to https://github.com/settings/ssh and click “New SSH Key”. Put e.g. “Rstudio” in the Title box, and copy the long string (from the tab opened in RStudio in the previous section) into Key box. Now you have an SSH key specific to RStudio."
  },
  {
    "objectID": "resources/tutorials/github.html#creating-a-new-project-in-rstudio",
    "href": "resources/tutorials/github.html#creating-a-new-project-in-rstudio",
    "title": "Git and GitHub",
    "section": "Creating a new project in RStudio",
    "text": "Creating a new project in RStudio\nIn RStudio, select File -&gt; New Project -&gt; New Directory -&gt; New Project.\nChoose a directory name (let’s use the same as the name of your project, e.g. “StatComp-[your SCIPER]”) and choose the folder in which this new directory will be created. Make sure to check “Create a git repository”. Click “Create Project”. A fresh RStudio without the top-left pane but with an additional Git tab in the top-right pane will open. Now you can copy-files into the new directory and open them (in the top-left pane, which will appear), or create new files and save them to the directory.\nFor the purposes of the next section, create a new a R Script and write some sample code in it, for example\n\nx &lt;- runif(100)\n\nand save the script in your project folder."
  },
  {
    "objectID": "resources/tutorials/github.html#linking-the-project-to-github",
    "href": "resources/tutorials/github.html#linking-the-project-to-github",
    "title": "Git and GitHub",
    "section": "Linking the project to GitHub",
    "text": "Linking the project to GitHub\nNow go back to the GitHub tab (to your account on GitHub) and create a new repository of the same name as the project (e.g. “StatComp-[your SCIPER]”). Choose the private option.\nAfter clicking “Create repository”, you will see the following:\n\n\n\nnew github repository\n\n\nInstead of using the HTTPS link, click on the SSH and copy the text, which will be of the form git@github.com:TMasak/StatComp.git. Open the Terminal tab in the bottom-left pane of RStudio and begin by providing your name and your e-mail to Git:\ngit config --global user.name [your github name]\ngit config --global user.email [your email address]\nYou can check whether it worked by typing git config --global --list.\nNow to finally link your local project folder to the GitHub repo, type the following commands in the terminal:\nssh -T git@github.com\ngit remote add origin [your SSH link of the form git@github.com:your_github_name/repo_name.git goes here]\ngit push -u origin master\nNote: If you receive an error:\n\ntry git push -u origin main instead of master, and if still an error:\ntry git add --all and git commit -m \"blahblah\" before git push -u origin master or main.\n\nBut if that still doesn’t work, go to the sub-section below.\nNow the content of the project folder should be up on GitHub. Return to the webpage, and click on your repo name in the top-left to see the repository.\n\nIn case of catastrophic errors\nIf you have too many issues in these past two steps, try removing everything you created in these steps (github repository and RStudio project: don’t worry you will be able to recreate them). Then\n\nCreate a private repository, with nothing in it (in the Initialize this repository, let everything not ticked or set as None)\nFollow the …or create a new repository on the command line. You first need to open the terminal in the directory where you want to save your project (e.g., cd ~/Desktop/), and then copy-paste the instructions. This should look like\n    echo \"## test\" &gt;&gt; README.md\n    git init\n    git add README.md\n    git commit -m \"first commit\"\n    git branch -M main\n    git remote add origin git@github.com:dufourc1/test.git\n    git push -u origin main\nCreate a new RStudio project with an existing directory (pick the directory that was created by the above actions. In this example it will be ~/Desktop/test/. You can fetch the correct path by typing pwd in the terminal after the above steps).\nadd, commit and push. You’re all set !\n\n\n\nError when pushing from RStudio\nSkip this section if the procedure above has been successful.\nSometimes there can be problems with the initial commit and the procedure above may fail. Instead of trying to debug it, let’s go the other way: create a repo on GitHub, and pull it from there to a local directory.\nBefore doing this, check that\n\nyou are logged in to GitHub with the account name being the same as the one provided to Git\nyour user directory and thus your SSH key as well as your working directory contain no non-English symbols (it might be also good to avoid spaces in your paths)\n\nClose your RStudio, and delete the local directory (and the RStudio project with it). Go to your repo in GitHub and click on “creating a new file” (like in the picture above). Name it e.g. “demo.R” and type x &lt;- runif(100) in it. Scroll down and click “Commit new file”.\nOpen RStudio again, and go to File -&gt; New Project -&gt; Version Control -&gt; Git. Here type the repository URL (like in the picture above) of the form https://github.com/your_github_name/repo_name.git. Click “Create Project”. RStudio will restart and a local folder in your system will appear, containing whatever is there in the GitHub plus an R project file of the appropriate name.\nImmediately, open the Git tab in the top-right pane of the RStudio, and click on Push. If GitHub is not connected to the local Git client, a pop-up window will appear and you can simply choose to verify the access in your browser. Then the push will hopefully go through, even though “Everything up-to-date” line will be returned. (This is not true, but any changes has to be committed first before they can be pushed, which we learn to do in the following section.)"
  },
  {
    "objectID": "resources/tutorials/github.html#basic-workflow",
    "href": "resources/tutorials/github.html#basic-workflow",
    "title": "Git and GitHub",
    "section": "Basic Workflow",
    "text": "Basic Workflow\nNow when you make a change locally in your project folder, such as changing the R code to\n\nx &lt;- sin(runif(100))\n\nand you would like to push your changes to GitHub, run the following R commands first:\n\n## ## install.packages(\"devtools\")\n## library(devtools)\nuse_git()\n\nFunction use_git() from the devtools package will commit changes.\nNote: The hashtag denotes a comment in R (whatever that should NOT be evaluated). It is a common practice to include commented lines into manuals such as this one to help the reader (you) deal with a problem he might encounter. For example, here, if running use_git() returns an error, it is likely due to the fact that the use_git() function belongs to the devtools package, which has to be loaded each time a new session of R is started (usually whenever you open RStudio). Hence uncomment the middle line and try to run it again. In case loading the package probably also returns an error, likely due to the fact the package that is being loaded has not been installed, which can be done using the install.packages() function in the first line of the code. Installing a package is something you only need to do once on your system.\nThen open ‘Git’ bar in the top-right pane of Rstudio and click “Push”. Your changes are now propagated to GitHub.\nView your script on Github. Make some changes locally in RStudio, push those changes, and observe them on GitHub. Or the other way around, edit the script on GitHub"
  },
  {
    "objectID": "resources/tutorials/github.html#sharing-your-repo",
    "href": "resources/tutorials/github.html#sharing-your-repo",
    "title": "Git and GitHub",
    "section": "Sharing your Repo",
    "text": "Sharing your Repo\nYou now have your own repo on GitHub, wich looks similarly to the course repo.\n\nMake sure that the repo is private. Go to Settings -&gt; General and scroll all the way down. If you see “This repository is currently private”, skip to the next point. On the other hand, if the repo is public, click on “Change visibility” and make the repo private.\nGo to Settings -&gt; Collaborators and add me (TMasak) and Almond (Almond-S) as collaborators by clicking on “Add people”.\n\nYou may also share your repo with your friends this way, though there is a little point in doing it now."
  },
  {
    "objectID": "resources/tutorials/github.html#accessing-and-downloading-the-course-content",
    "href": "resources/tutorials/github.html#accessing-and-downloading-the-course-content",
    "title": "Git and GitHub",
    "section": "Accessing and Downloading the Course Content",
    "text": "Accessing and Downloading the Course Content\nNow, when the GitHub + RStudio connection is established, it can also be used to access the course materials easily. In RStudio, select File -&gt; New Project -&gt; Version Control -&gt; Git and type the url of the repository you want to clone.\nWhenever you want to update your local repository, you can just click “Pull” in the Git tab in the top-right pane of RStudio."
  },
  {
    "objectID": "resources/tutorials/github.html#references",
    "href": "resources/tutorials/github.html#references",
    "title": "Git and GitHub",
    "section": "References",
    "text": "References\n\nHappy Git with R – how to use Git, GitHub and RStudio together, mostly for data science purposes"
  },
  {
    "objectID": "resources/week-01/recap-linear-regression.html",
    "href": "resources/week-01/recap-linear-regression.html",
    "title": "OLS estimator",
    "section": "",
    "text": "This document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "resources/week-01/recap-linear-regression.html#introduction",
    "href": "resources/week-01/recap-linear-regression.html#introduction",
    "title": "OLS estimator",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Equation 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Equation 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "resources/week-01/recap-linear-regression.html#matrix-representation-for-the-regression-model",
    "href": "resources/week-01/recap-linear-regression.html#matrix-representation-for-the-regression-model",
    "title": "OLS estimator",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Equation 1 and Equation 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "resources/week-01/recap-linear-regression.html#estimating-the-coefficients",
    "href": "resources/week-01/recap-linear-regression.html#estimating-the-coefficients",
    "title": "OLS estimator",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Equation 7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Equation 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "resources/week-01/recap-linear-regression.html#variance-covariance-matrix-of-the-coefficients",
    "href": "resources/week-01/recap-linear-regression.html#variance-covariance-matrix-of-the-coefficients",
    "title": "OLS estimator",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Equation 9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]\n\n\n\n\n\n\nAttribution\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210."
  },
  {
    "objectID": "resources/computing/intro_to_julia/index.html",
    "href": "resources/computing/intro_to_julia/index.html",
    "title": "Introduction to Julia",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction\nPlease check this tutorial for now.\n\n\n\n\n\n\n\nSubject\n\n\nDescription\n\n\n\n\n\n\nGood practices in Julia\n\n\nTips and tricks for writing better code\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html",
    "title": "Functions and objects",
    "section": "",
    "text": "Welcome to Introduction to R from fredhutch.io! This course introduces you R by working through common tasks in data science: importing, manipulating, and visualizing data.\nR is a statistical and programming computer language widely used for a variety of applications.\nBefore proceeding with these training materials, please ensure you have installed both R and RStudio as described here.\nBy the end of this session, you should be able to:\n\nwork within the RStudio interface to run and save R code in a project\nunderstand basic R syntax to use functions and assign objects\ncreate and manipulate vectors and understand how R deals with missing data"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#objectives",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#objectives",
    "title": "Functions and objects",
    "section": "",
    "text": "Welcome to Introduction to R from fredhutch.io! This course introduces you R by working through common tasks in data science: importing, manipulating, and visualizing data.\nR is a statistical and programming computer language widely used for a variety of applications.\nBefore proceeding with these training materials, please ensure you have installed both R and RStudio as described here.\nBy the end of this session, you should be able to:\n\nwork within the RStudio interface to run and save R code in a project\nunderstand basic R syntax to use functions and assign objects\ncreate and manipulate vectors and understand how R deals with missing data"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#a-brief-orientation-to-rstudio",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#a-brief-orientation-to-rstudio",
    "title": "Functions and objects",
    "section": "A brief orientation to RStudio",
    "text": "A brief orientation to RStudio\nR is a statistical programming language, while RStudio is an integrated development environment (IDE) that allows you to code in R more easily. RStudio possesses many features that you may find useful in your work. We’ll highlight a few of the most common and useful parts for our introductory course.\nThe first time you open RStudio, you’ll see three panels, or windows.\n\nThe panel on the left is the console, where you can run R code. The text printed in this panel is basic information about R and the version you’re running. You can test how the console can be used to run code by entering 3 + 4 and then pressing enter. This instructs your computer to read, interpret, and execute the command, then print the result (7) to the Console, and show a right facing arrow (&gt;), indicating it is ready to accept additional code.\nThe panel on the top right is the environment. It’s empty right now, but we’ll learn more about this later in this lesson.\nThe panel on the lower right shows the files present in your working directory. Currently, that’s probably your Home directory, which includes folders like Documents and Downloads.\n\nYou may notice that some of the panels possess additional tabs. We’ll explore some of these features in this class, but for more information:\nHelp -&gt; Cheetsheets -&gt; RStudio IDE cheat sheet\nThis PDF includes an overview of each of the things you see in RStudio, as well as explanations of how you can use them. It may be intimidating right now, but will come in handy as you gain experience with R.\nOne of the ways that RStudio makes working in R easier is by allowing you to create R projects. You can think of a project as a discrete unit of work, such as a chapter of a thesis/dissertation, analysis for a manuscript, or a monthly report. We recommend organizing your code, data, and other associated files as projects, which allows you to keep all parts of an analysis together for easier access.\nWe’ll be creating a project to use for the duration of this course. Create a new project in RStudio:\n\nFile -&gt; New Project\nChoose New Directory, then New Project\nname your project intro_r and save it somewhere on your computer you’ll be able to find easily later (we recommend your Desktop or Documents)\nClick Create project\n\nAfter your RStudio screen reloads, note two things:\n\nThe file browser in the lower right panel will now show the contents of a new folder, intro_r, that was created as a part of your RStudio project.\nThe console window will show the path, or location in your computer, for your project directory. This is important later in class, when this path will be required to locate data for analysis.\n\nNow we’re ready to create a new R script:\n\nFile -&gt; New File -&gt; R Script\nSave the new file as class1.R. By default, RStudio will save this in your project directory.\n\nThis R script is a text file that we’ll use to save code we learn in this class. We’ll refer to this window as the script or source window. Remember to save this file periodically to retain the record of the work you’re doing, so you can re-execute the code later if necessary.\nBy convention, a script should include a title at the top, so type the following on the first line:\n# Introduction to R: Class 1"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#using-functions",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#using-functions",
    "title": "Functions and objects",
    "section": "Using functions",
    "text": "Using functions\nNow that we have a project and new script set up, we’re ready to begin adding code. Skipping a line after the title, type the following on the next two lines:\n\n# basic math\n4 + 5 \n\n[1] 9\n\n\nThe first of the two boxes above represents the code you execute. The second box (prefaced with ##) shows the output you should expect. The [1] in the second box means there is one item (in this case, 9) present in the output.\nThe first line in that example is a code comment. It is not interpreted by R, but is a human-readable explanation of the code that follows. This is also how we included a title in our script. In R, anything to the right of one or more # symbols represents a comment.\nThe code above is the same mathematical operation we executed earlier. If we wanted to re-run this command, we have two options:\n\nCopy and paste the code into the Console\nUse the Run button at the top of the script window\nUse the keyboard shortcut: Ctrl + Enter\n\nThe third option is the most efficient, especially as your coding skills progress. With your cursor on the line with 4 + 5, hold down the Control key and press Enter. You’ll see the code and answer both appear in the Console. A few things to note about this keyboard shortcut:\n\nIt doesn’t matter where your cursor is on the line of code; the entire line will be executed with the keyboard shortcut.\nIf there isn’t code on the line where your cursor is located, RStudio will attempt to execute following lines.\n\nIn practice, a script should represent code you are developing in R, and you should only save the code that you know functions. For this class, we’ll be including notes about things we learn as comments.\n\nCtrl + Enter is the only keyboard shortcut we emphasize in this course, but there are many others available. You can view them on the second page of the cheat sheet linked above, or by going to Help -&gt; Keyboard Shortcuts Help.\n\nIf you were looking carefully, you may have noticed that the + in the previous code example had spaces on either side, separating it from the numbers. You may wonder whether spaces matter in how the code is interpreted. As with many questions in coding, the easiest way to assess whether removing the spaces matters is to simply try it:\n\n# same code as above, without spaces\n4+5\n\n[1] 9\n\n\nGiven the output, we can conclude that spaces do not matter in how the code functions. In this case, however, spaces represent a common convention in formatting R code, as it makes it easier for human eyes to read. In general, you should attempt to replicate the code presented here as closely as possible, and we’ll do our best to note when something is required as opposed to convention.\n\nCode convention and style doesn’t make or break the ability of your code to run, but it does affect whether other people can easily understand your code. A brief overview of common code style is available here, and more information is available in the tidyverse style guide.\n\nSo far, we’ve used R with mathematical symbols representing operations. R possesses the ability to perform much more complex tasks using functions, which is a pre-defined set of code that allows you to repeat particular actions.\nR includes functions for other types of math:\n\n# using a function: rounding numbers\nround(3.14)\n\n[1] 3\n\n\nIn this case, round is the function, and 3.14 is the number (data) being manipulated by the funcion. A word followed by parentheses is a common format for functions in R.\n\nSyntax refers to the rules that dictate how combinations of words and symbols are interpreted in a language (either programming or human).\n\nAdditional options for modifying functions are called arguments, and are included with the data between parentheses. For the round function, a common modification would be the number of decimal points output. You can change this detail by adding a comma and then additional argument:\n\n# using a function with more arguments\nround(3.14, digits = 1)\n\n[1] 3.1\n\n\nIf you would like to learn more about how this function works, you can go to the bottom righthand panel and click on the Help tab. Enter the name of a function into the search box and hit Enter. Alternatively, execute the following in your console:\n?round\nThis is a shortcut for performing the same task in the panel described above.\nR help documentation tends to be formatted very consistently. At the very top, you’ll see the name of the function. Below that, a short title indicates the purpose of the function, along with a more verbose “Description”. “Usage” tells you how to use the function in code, and “Arguments” details each of the optiond in “Usage”. The rest of the subheadings should be self-explanatory.\nIn the example above, there is no label associated with 3.14. In reality, 3.14 represents x, so the command can actually be written as round(x = 3.14, digits = 1). Even if not explicitly stated, the computer assumes that 3.14 represents x if the number is the first thing that appears after the opening parenthesis.\nIf you define both arguments explicitly, you can switch the order in which they appear:\n\n# can switch order of arguments\nround(digits = 1, x = 3.14)\n\n[1] 3.1\n\n\nIf you remove the labels (round(1, 3.14)), the answer is different, because R is assuming you mean round(x = 1, digits = 3.14).\n\nYou may notice that boxes pop up as you type. These represent RStudio’s attempts to guess what you’re typing and share additional options.\n\n\nChallenge-hist\nWhat does the function hist do? What are its main arguments? How did you determine this?"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#assigning-objects",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#assigning-objects",
    "title": "Functions and objects",
    "section": "Assigning objects",
    "text": "Assigning objects\nSo far, we’ve been performing tasks with R that require us to input the data manually. One of the strengths of using a programming language is the ability to assign data to objects, or variables.\n\nObjects in R are referred to as variables in other programming languages. We’ll use these terms synonymously for this course, though in other contexts there may be differences between them. Please see the R documentation on objects for more information.\n\nLike in math, a variable is a word used to represent a value (in this case, a number):\n\n# assigning value to an object\nweight_kg &lt;- 55\n\nIn the code above, &lt;- is the assignment operator: it instructs R to recognize weight_kg as representing the value 55. You can think of this code as referencing “55 goes into weight_kg.”\nAfter executing the code above, you’ll see the object appear in the Environment panel on the upper right hand side of the RStudio screen. The name of the object will appear on the left, with the value assigned to it on the right.\nThe name you assign to objects can be arbitrary, but we recommend using names that are relatively short and meaningful in the context of the values they represent. It’s useful to also know other general limitations on object names:\n\ncase sensitive\ncannot start with numbers\navoid other common words in R (e.g., function names, like mean)\navoid dots (underscores are a good alternative, such as the example above)\n\nExtra information on object names is available in the tidyverse style guide.\nNow that the object has been assigned, we can reference that object by executing its name:\n\n# recall object\nweight_kg\n\n[1] 55\n\n\nThus, the value weight_kg represents is printed to the Console.\nWe can also perform operations on an object:\n\n# multiple an object (convert kg to lb)\n2.2 * weight_kg\n\n[1] 121\n\n\nIn that case, the answer is printed to the Console. You can also assign the output to a new object:\n\n# assign weight conversion to object\nweight_lb &lt;- 2.2 * weight_kg\n\nAfter executing that line of code, you’ll see weight_lb appear in the Environment panel, too.\nNow let’s explore what happens if we assign a value to an existing object name:\n\n# reassign new value to an object\nweight_kg &lt;- 100\n\nNote that the value assigned to weight_kg as it appears in the Environment panel changes after executing the code above.\nHas the value assigned to weight_lb also changed? You might expect this would be the case, since this value is derived from weight_kg. However, weight_kg remains the same as previously assigned. If you want the value for weight_kg to reflect the new value for weight_kg, you will need to again execute weight_lb &lt;- 2.2 * weight_kg. This should help you understand an important concept in writing code: the order in which you execute lines of code matters! In the context of the material we cover in this class, we’ll continue saving code in scripts so we have a record of both the relevant commands and the appropriate order for execution.\n\nYou can think of the names of objects like sticky notes. You have the option to place the sticky note (name) on any value you choose. You can pick up the sticky note and place it on another value, but you need to explicitly tell R when you want values assigned to certain objects.\n\nAt this point in the lesson, it’s common to have accidentally created an object with a typo in the name. If this has happened to you, it’s useful to know how to remove the object to keep your environment up to date. Here, we’ll practice removing an object with something everyone has available:\n\n# remove object\nremove(weight_lb) \n\nThis removes the specified object from the environment, which you can confirm by its absence in the Environment panel. You can also abbreviate this command to rm(weight_lb).\n\nYou can clear the entire environment using the button at the top of the Environment panel with a picture of a broom. This may seem extreme, but don’t worry! We can re-create all the work we’ve already done by executing each line of code again.\n\n\nChallenge-values\nFor the code chunk below, what is the value of each item at each step?\n\n\nmass &lt;- 47.5            # mass?\nwidth  &lt;- 122             # width?\nmass &lt;- mass * 2.0      # mass?\nwidth  &lt;- width - 20        # width?\nmass_index &lt;- mass/width  # mass_index?"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#vectors",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#vectors",
    "title": "Functions and objects",
    "section": "Vectors",
    "text": "Vectors\nSo far, we’ve worked with objects containing a single value. For most research purposes, however, it’s more realistic to work with a collection of values. We can do that in R by creating a vector with multiple values:\n\n# assign vector\nages &lt;- c(50, 55, 60, 65) \n# recall vector\nages\n\n[1] 50 55 60 65\n\n\nThe c function used above stands for “combine,” meaning all of the values in parentheses after it are included in the object. This is reflected in the Console, where recalling the value shows all four values, and the Environment window, where multiple values are shown on the right side.\nWe can use functions to ask basic questions about our vector, including:\n\n# how many things are in object?\nlength(ages)\n\n[1] 4\n\n# what type of object?\nclass(ages)\n\n[1] \"numeric\"\n\n# get overview of object\nstr(ages)\n\n num [1:4] 50 55 60 65\n\n\nIn the code above, we learn that there are four items (values) in our vector, and that the vector is composed of numeric data. str stands for “structure”, and shows us a general overview of the data, including a preview of the first few values (or all the values, as is the case in our small vector).\nEven more useful is the ability to use functions to perform more complex tasks for us, such as statistical summaries:\n\n# performing functions with vectors\nmean(ages)\n\n[1] 57.5\n\nrange(ages)\n\n[1] 50 65\n\n\nAlthough we’ve focused on numbers as data so far, it’s also possible for data to be words instead:\n\n# vector of body parts\norgans &lt;- c(\"lung\", \"prostate\", \"breast\")\n\nIn this case, each word is encased in quotation marks, indicating these are character data, rather than object names.\n\nChallenge-organs\nPlease answer the following questions about organs: - How many values are in organs? - What type of data is organs? - How can you see an overview of organs?\n\nWe’ve seen data as numbers and letters so far. In fact, R has all of the following basic data types:\n\ncharacter: sometimes referred to as string data, tend to be surrounded by quotes\nnumeric: real or decimal numbers, sometimes referred to as “double”\ninteger: a subset of numeric in which numbers are stored as integers\nlogical: Boolean data (TRUE and FALSE)\ncomplex: complex numbers with real and imaginary parts (e.g., 1 + 4i)\nraw: bytes of data (machine readable, but not human readable)\n\nThe three data types listed in bold above are the focus of this class. R automatically interprets the type as you enter data. Most data analysis activities will not require you to understand specific details of the other data types.\n\nChallenge-dtypes\nR tends to handle interpreting data types in the background of most operations. The following code is designed to cause some unexpected results in R. What is unusual about each of the following objects?\n\n\nnum_char &lt;- c(1, 2, 3, \"a\")\nnum_logical &lt;- c(1, 2, 3, TRUE)\nchar_logical &lt;- c(\"a\", \"b\", \"c\", TRUE)\ntricky &lt;- c(1, 2, 3, \"4\")"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#manipulating-vectors",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#manipulating-vectors",
    "title": "Functions and objects",
    "section": "Manipulating vectors",
    "text": "Manipulating vectors\nIn the section above, we learned to create and assess vectors, and use functions to calculate statistics across the values. We can also modify a vector after it’s been created:\n\n# add a value to end of vector\nages &lt;- c(ages, 90) \n\nThe example above uses the same combine (c) function as when we initially created the vector. We can also use it to add values to the beginning of the vector:\n\n# add value at the beginning\nages &lt;- c(30, ages)\n\nIf we wanted to extract, or subset, a portion of a vector:\n\n# extracting second value\norgans[2] \n\n[1] \"prostate\"\n\n\nIn general, square brackets ([ ]) in R refer to a part of an object. The number 2 indicates the second value in the vector.\n\nThe index position of a value is the number associated with its location in a collection. In the example above, note that R indexes (or counts) starting with 1. This is different from many other programming languages, like Python, which use 0-based indexing.\n\nIn R, a minus sign (-) can be used to negate a value’s position, which excludes that value from the output:\n\n# excluding second value\norgans[-2] \n\n[1] \"lung\"   \"breast\"\n\n\nYou may be tempted to try extracting multiple values at a time by separating the numbers with commas (e.g., organs[2,3]). This will result in a rather cryptic error, which we’ll talk more about next time. For now, remember that you can use the combine function to indicate multiple values for subsetting:\n\n# extracting first and third values\norgans[c(1, 3)] \n\n[1] \"lung\"   \"breast\"\n\n\nWe’ll switch back to our numerical ages object to explore another common need when subsetting: extracting values based on a condition (or criteria). For numerical data, we’re often interested in extracting data that are in a certain range of values. It is tempting to try something like:\n\nages &gt; 60 \n\n[1] FALSE FALSE FALSE FALSE  TRUE  TRUE\n\n\nThe result, however, is less than satisfying: you receive either TRUE or FALSE for each data point, depending on whether it meets the condition or not.\nWhile that information isn’t quite what we expected, we can combine it with the subsetting syntax we learned earlier:\n\n# extracts values which meet condition\nages[ages &gt; 60] \n\n[1] 65 90\n\n\nIf we read the code above from the inside out (a common strategy for R), the code above identifies which values meet the criteria, and the square brackets are used to extract this from the original vector.\nIf you want to extract items exactly equal to a specific value, you need to use two equal signs:\n\n# extracts values numerically equivalent values\nages[ages == 60]\n\n[1] 60\n\n\nYou can think of this as a way to differentiate mathematical equivalency from specification of parameters for arguments (such as digits = 1 for round(), as we learned earlier). R also allows you to use &lt;= and &gt;=.\nFinally, it’s common to need to combine conditions while subsetting. For example, you may be interested in only values between 50 and 60:\n\n# ages less than 50 OR greater than 60\nages[ages &lt; 50 | ages &gt; 60]\n\n[1] 30 65 90\n\n\nIn the code above, the vertical pipe | is interpreted to mean “or,” so each data point can belong to either the category on the left of the pipe, the category on the right, or both. In other words, the vertical pipe means any single value being evaluated must meet one or both conditions.\nYou can also combine conditions with &, but this means any single value must meet both conditions:\n\n# ages greater than 50 OR less than 60\nages[ages &gt; 50 & ages &lt; 60]\n\n[1] 55\n\n\n\nBe careful when thinking about human language as opposed to programming languges. When speaking, we is reasonable to say “extract all values below 50 and above 60.” While this makes sense in context, it is mathematically impossible for a value to be both less than 50 AND greater than 60.\n\n\nChallenge-compare\nWhy does the following code return the answer it\n\n\n\"four\" &gt; \"five\"\n\n[1] TRUE"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#missing-data",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#missing-data",
    "title": "Functions and objects",
    "section": "Missing data",
    "text": "Missing data\nMost of the data we encounter has missing data. Programming languages interpret and handle missing data in different ways, so it’s worth taking time to dig into how R approaches this issue.\nFirst, we’ll create a new vector some values indicated as missing data:\n\n# create a vector with missing data\nheights &lt;- c(2, 4, 4, NA, 6)\n\nIn the vector above, NA represents a value where data are missing. You may notice NA is not encased in quotation marks. This is because R interprets that set of characters specifically as missing data.\nNext, let’s investigate how this vector responds to use in functions:\n\n# calculate mean and max on vector with missing data\nmean(heights)\n\n[1] NA\n\nmax(heights)\n\n[1] NA\n\n\nThe answer isn’t very satisfying; we’re told the answer is missing data because of the presence of a single missing value in the vector. This is a slightly frustrating default behavior for some common statistical functions in R, but we can add an argument to ignore missing data and calculate across the remaining values:\n\n# add argument to remove NA\nmean(heights, na.rm = TRUE)\n\n[1] 4\n\nmax(heights, na.rm = TRUE)\n\n[1] 6\n\n\nIn the code above, the na.rm parameter controls whether missing data are removed. The default (which you can also reference in the help documentation) is for missing values to be included (na.rm = FALSE). By switching to na.rm = TRUE, we’re instructing R to remove missing data.\nThe example above retains missing values in the dataset while performing calculations. There are certainly cases in which you may want to specifically filter out the missing data from your dataset.\nThe function is.na allows you to ask whether elements in a dataset are missing:\n\n# identify elements which are missing data\nis.na(heights)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\nIf a resulting value is TRUE, the value is missing. If FALSE, the data point is present. We can invert the resulting logical data using an exclamation point:\n\n# reverse the TRUE/FALSE\n!is.na(heights)\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE\n\n\nThis means missing data are now listed as FALSE, with data present as TRUE.\nAs with the conditional statements we learned earlier, we can combine these results with our square bracket subsetting syntax to extract only values that are present in the dataset:\n\n# extract elements which are not missing values\nheights[!is.na(heights)]\n\n[1] 2 4 4 6\n\n\nAlternatively, you can use a function specifically designed for excluding (omitting) missing data:\n\n# remove incomplete cases\nna.omit(heights) \n\n[1] 2 4 4 6\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n\nYou may notice that this output looks slightly different than the previous example. This is because na.omit includes output about attributes, or information about the data. The output vectors are the same for the last two code examples, even though the way they appear in the Console seems different.\n\nIf you aren’t sure how to interpret the output in your console, sometimes it helps to assign the output to an object. You can then inspect the data type, structure, etc to ensure you’re getting the answer you expected.\n\n\nChallenge-analyze\nComplete the following tasks after executing the code chunk below. (Note: there are multiple solutions): - Remove NAs - Calculate the median - Identify how many elements in the vector are greater than 67 inches - Visualize the data as a histogram (hint: function hist)\n\n\n# create vector\nmore_heights &lt;- c(63, 69, 60, 65, NA, 68, 61, 70, 61, 59, 64, 69, 63, 63, NA, 72, 65, 64, 70, 63, 65)"
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#wrapping-up",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#wrapping-up",
    "title": "Functions and objects",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this session, we spent some time getting to know the RStudio interface for writing and running R code, explored the basic principles of R syntax for functions and object assignment, and worked with vectors to understand how R handles missing data.\nIn the next session, we’ll learn to import spreadsheet-style data that are more similar to what you’d like handle for a research project, and practice accessing different portions of the data.\nWhen you are done working in RStudio, you should save any changes to your R script. When you close RStudio, you will see a pop-up box asking if you want to save your workspace image. We do not recommend saving your project in this way, as it creates extra (hidden) files on your computer that can be unwieldy in size and inadvertently retain sensitive data (if you’re working with PHI or other private data). If you’ve saved your R script, you can recreate all the work you’ve accomplished. For more information on this topic, please review this explanation. If you would like to prevent this box from popping up in the future, we recommend:\n\nGo to Tools -&gt; Global Options (Global means for all projects; you can also change this for each project using Project Options)\nIn the drop-down menu next to Save workspace to ~/.Rdata on exit select Never.\n\nIf you need to reopen your project after closing RStudio, you should use the File -&gt; Open Project and navigate to the location of your project directory. Alternatively, using your operating system’s file browser, double click on the r_intro.Rrpoj file."
  },
  {
    "objectID": "resources/computing/intro_to_r/1_functions_and_objects.html#extra-exercises",
    "href": "resources/computing/intro_to_r/1_functions_and_objects.html#extra-exercises",
    "title": "Functions and objects",
    "section": "Extra exercises",
    "text": "Extra exercises\n\nChallenge-objects\n\nCreate an object called agge that contains your age in years\nReassign the object to a new object called age (e.g., correct the typo)\nRemove the previous object from your environment\nCalculate your age in days\n\n\n\nChallenge-char\nThe following vector represents the number of vacation days possessed by various employees:\n\nvacation_days &lt;- c(5, 7, 20, 1, 0, 0, 12, 4, 2, 2, 2, 4, 5, 6, 7, 10, 4)\n\n\nHow many employees are represented in the vector?\nHow many employees have at least one work week’s worth of vacation available to them?"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html",
    "title": "Data visualization in R",
    "section": "",
    "text": "So far in this course, we have:\n\nlearned basic R syntax, including working with objects and functions\nimported data into R for manipulation with base R methods\nloaded tidyverse and used its data science tools to manipulate and filter data\n\nThis last material continues our explorations of tidyverse with a specific focus on data visualization. After completing this material, you should be able to use ggplot2 in R to:\n\ncreate and modify scatterplots and boxplots\nrepresent time series data as line plots\nsplit figures into multiple panels\ncustomize your plots"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#objectives",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#objectives",
    "title": "Data visualization in R",
    "section": "",
    "text": "So far in this course, we have:\n\nlearned basic R syntax, including working with objects and functions\nimported data into R for manipulation with base R methods\nloaded tidyverse and used its data science tools to manipulate and filter data\n\nThis last material continues our explorations of tidyverse with a specific focus on data visualization. After completing this material, you should be able to use ggplot2 in R to:\n\ncreate and modify scatterplots and boxplots\nrepresent time series data as line plots\nsplit figures into multiple panels\ncustomize your plots"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#getting-set-up",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#getting-set-up",
    "title": "Data visualization in R",
    "section": "Getting set up",
    "text": "Getting set up\nSince we are continuing to work with data in tidyverse, we need to make sure all of our data and packages are available for use.\nOpen your project in RStudio. Create a new script called class4.R, add a title, and enter the following code with comments:\n\n# load library\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# read in first filtered data from last class\nbirth_reduced &lt;- read_csv(\"data/birth_reduced.csv\")\n\nRows: 4169 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): primary_diagnosis, tumor_stage, vital_status, morphology, state, t...\ndbl  (8): age_at_diagnosis, days_to_death, days_to_birth, days_to_last_follo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# read in second filtered data from last class\nsmoke_complete &lt;- read_csv(\"data/smoke_complete.csv\")\n\nRows: 1152 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): primary_diagnosis, tumor_stage, vital_status, morphology, state, t...\ndbl  (8): age_at_diagnosis, days_to_death, days_to_birth, days_to_last_follo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you have trouble accessing your data and see an error indicating the file is not found, it is likely one of the following problems:\n\nCheck to make sure your project is open in RStudio. You should see the path to your project directory (e.g., ~/Desktop/introR) appear at the top of the console (above the window showing output). If this doesn’t appear, you should save your script in your project directory, then go to File -&gt; Open Project. Navigate to the location of your project directory and open the folder, then try to reexecute your code.\nMake sure you have the two datasets (birth_reduced.csv and smoke_complete.csv) in your data directory. Please reference the materials from class 3 to filter the original clinical dataset and export these data.\n\nOnce your data are imported appropriately, we can create a quick plot:\n\n# simple plot from base R from the smoke_complete dataset\nplot(x=smoke_complete$age_at_diagnosis, y=smoke_complete$cigarettes_per_day)\n\n\n\n\nThis plot is from base R. It gives you a general idea about the data, but isn’t very aesthetically pleasing. Our work today will focus on developing more refined plots using ggplot2, which is part of the tidyverse."
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#intro-to-ggplot2-and-scatterplots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#intro-to-ggplot2-and-scatterplots",
    "title": "Data visualization in R",
    "section": "Intro to ggplot2 and scatterplots",
    "text": "Intro to ggplot2 and scatterplots\nThere are three steps to creating a ggplot. We’ll start with a scatterplot, which is used to compare quantitative (continuous) variables.\n\nbind data: create a new plot with a designated dataset\n\n\n# basic ggplot\nggplot(data = smoke_complete) # bind data to plot\n\n\n\n\nThe last line of code creates an empty plot, since we didn’t include any instructions for how to present the data.\n\nspecify the aesthetic: maps the data to axes on a plot\n\n\n# basic ggplot\nggplot(data = smoke_complete, aes(x = age_at_diagnosis, \n                           y = cigarettes_per_day)) # specify aesthetics (axes)\n\n\n\n\nThis adds labels to the axis, but no data appear because we haven’t specified how they should be represented\n\nadd layers: visual representation of plot, including ways through which data are represented (geometries or shapes) and themes (anything not the data, like fonts)\n\n\nggplot(data = smoke_complete,\n       mapping = aes(x = age_at_diagnosis, y = cigarettes_per_day)) + \n  geom_point() # add a layer of geometry\n\n\n\n\nThe plus sign (+) is used here to connect parts of ggplot code together. The line breaks and indentation used here represents the convention for ggplot, which makes the code more readible and easy to modify.\nIn the code above, note that we don’t need to include the labels for data = and mapping =. It’s also common to include the mapping (aes) in the geom, which allows for more flexibility in customizing (we’ll get to this later!).\n\nggplot(smoke_complete) + \n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day)) \n\n\n\n\nThis plot is identical to the previous plot, despite the differences in code."
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#customizing-plots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#customizing-plots",
    "title": "Data visualization in R",
    "section": "Customizing plots",
    "text": "Customizing plots\nNow that we have the data generally displayed the way we’d like, we can start to customize a plot.\n\n# add transparency with alpha\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day), alpha = 0.1)\n\n\n\n\nTransparency is useful to help see the distribution of data, especially when points are overlapping.\n\n# change color of points\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day), \n             alpha = 0.1, color = \"green\")\n\n\n\n\nFor more information on colors available, look here.\nWe can also color points based on another (usually categorical) variable:\n\n# plot disease by color\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, \n                 color = disease), \n             alpha = 0.1)\n\n\n\n\nNote the location of color= with the other aesthetics, as well as the lack of quotation marks around disease.\nColoring by a variable automatically adds a legend as well.\nWe can also change the general appearance of the plot (background colors and fonts):\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease), alpha = 0.1) +\n  theme_bw() # change background theme\n\n\n\n\nThis adds another layer to our plot representing a black and white theme. A complete list of pre-set themes is available here, and we’ll cover ways to customize our own themes later in this lesson.\nWhile the axes are currently sufficient, they aren’t particularly attractive. We can add a title and replace the axis labels using labs:\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease), alpha = 0.1) +\n  labs(title = \"Age at diagnosis vs cigarettes per day\", # title\n       x=\"age (days)\", # x axis label\n       y=\"cigarettes per day\") +# y axis label\n  theme_bw()\n\n\n\n\nAnother common feature to customize involves the orientation and appearance of fonts. While this can be controlled by default themes like theme_bw), you can also control different parts independently. For example, we can make a dramatic modification to all text in the plot:\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  theme(text = element_text(size = 16)) # increase all font size\n\n\n\n\nAlternatively, you can alter only one specific type of text:\n\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5)) # rotate and adjust x axis text\n\n\n\n\nThis rotates and adjusts the horizontal and vertical arrangement of the labels on only the x axis. Of course, you can also modify other text (y axis, axis labels, legend).\nAfter you’re satisfied with a plot, it’s likely you’d want to share it with other people or include in a manuscript or report.\n\n# create directory for output\ndir.create(\"figures\")\n\n\n# save plot to file\nggsave(\"figures/awesomePlot.jpg\", width = 10, height = 10, dpi = 300)\n\nThis automatically saves the last plot for which code was executed. You can view your figures/ directory to see the exported jpeg file. This command interprets the file format for export using the file suffix you specify. The other arguments dictate the size (width and height) and resolution (dpi).\n\nChallenge-scatterplot\nCreate a scatterplot showing age at diagnosis vs years smoked with points colored by gender and appropriate axis labels."
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#box-and-whisker-plots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#box-and-whisker-plots",
    "title": "Data visualization in R",
    "section": "Box and whisker plots",
    "text": "Box and whisker plots\nBox and whisker plots compare the distribution of a quantitative variable among categories.\n\n# creating a box and whisker plot\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day))\n\n\n\n\nThe main differences from the scatterplots we created earlier are the geom type and the variables plotted.\nWe can change the color similarly to scatterplots:\n\n# adding color\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day), color = \"tomato\")\n\n\n\n\nIt seems weird to change the color of the entire box, though. A better option would be to add colored points to a black box and whisker plot:\n\n# adding colored points to black box and whisker plot\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day)) +\n  geom_jitter(aes(x = vital_status, y = cigarettes_per_day), alpha = 0.3, color = \"blue\")\n\n\n\n\nJitter references a method of randomly offsetting points slightly to allow them to be seen and interpreted more easily.\nThis method, however, effectively duplicates some data points, since all points are shown with jitter and the boxplot shows outliers. You can use an option in geom_boxplot to suppress plotting of outliers:\n\n# boxplot with both boxes and points\nggplot(smoke_complete) +\n  geom_boxplot(aes(x = vital_status, y = cigarettes_per_day), outlier.shape = NA) +\n  geom_jitter(aes(x = vital_status, y = cigarettes_per_day), alpha = 0.3, color = \"blue\")\n\n\n\n\n\nChallenge-comments\nWrite code comments for each of the following lines of code. What is the advantage of writing code like this?\n\n\nmy_plot &lt;- ggplot(smoke_complete, aes(x = vital_status, y = cigarettes_per_day)) \nmy_plot +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(alpha = 0.2, color = \"purple\")\n\n\n\n\n\nChallenge-order\nDoes the order of layers in the last plot matter? What happens if jitter is coded before boxplot?"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#time-series-data-as-line-plots",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#time-series-data-as-line-plots",
    "title": "Data visualization in R",
    "section": "Time series data as line plots",
    "text": "Time series data as line plots\nSo far we’ve been able to work with the data as it appears in our filtered dataset. Now that we’re moving on to time series plots (changes in variables over time), we need to manipulate the data. We’ll also be working with the birth_reduced dataset, which we created last class (primarily by removing all missing data for year of birth). We’d like to plot the number of individuals in the dataset born by year, so we need to first count our observations based on both disease and year of birth:\n\n# count number of observations for each disease by year of birth\nyearly_counts &lt;- birth_reduced %&gt;%\n  count(year_of_birth, disease) \n\nWe can plot these data as a single line:\n\n# plot all counts by year\nggplot(yearly_counts) +\n  geom_line(aes(x = year_of_birth, y = n))\n\n\n\n\nHere, n represents the number of patients born in each year, from the count table created above. The result isn’t very satisfying, because we also grouped by disease. We can improve this by plotting each disease on a separate line, which is more appropriate when there are multiple data points per year:\n\n# plot one line per cancer type\nggplot(yearly_counts) +\n  geom_line(aes(x = year_of_birth, y = n, \n                group = disease))\n\n\n\n\nMoreover, we can color each line individually:\n\n# color each line per cancer type\nggplot(yearly_counts) +\n  geom_line(aes(x = year_of_birth, y = n, color = disease))\n\n\n\n\nNote that you don’t have to include a separate argument for group = disease because grouping is assumed by color = disease.\n\nChallenge-line\nCreate a line plot for year of birth and number of patients with lines representing each gender. Hint: you’ll need to manipulate the birth_reduced dataset first.\n\n\nChallenge-dash\nHow do you show differences in lines using dashes/dots instead of color?"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#faceting",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#faceting",
    "title": "Data visualization in R",
    "section": "Faceting",
    "text": "Faceting\nSo far we’ve been working on building single plots, which can show us two main variables (for the x and y axes) and additional variables using color (and potentially size/shape/etc). Scientific visualizations often need to compare among categories (e.g., control vs various treatments), which is generally clearer if those categories are presented in separate panels. ggplot provides this capacity through faceting.\nLet’s revisit the scatterplot we initially created, plotting age at diagnosis by cigarettes per day, with points colored by disease. We add an additional layer to create facets, or separate panels, for a given variable (in this case, the same variable being used to color points):\n\n# use previous scatterplot, but separate panels by disease\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_wrap(vars(disease)) # wraps panels to make a square/rectangular plot\n\n\n\n\nvars is used for faceting in the same way that aes() is used for mapping: it is used to specify the variable to form facet groups.\nfacet_wrap determines how many rows and columns of panels are needed to create the most square-shaped final plot possible. This becomes useful when there are many more categories:\n\n# add a variable by leaving color but changing panels to other categorical data\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_wrap(vars(tumor_stage))\n\n\n\n\nIn this case, we’re now visualizing an additional variable (tumor stage), in addition to the original three (age at diagnosis, cigarettes per day, and disease).\nIf you want to control the specific layout of panels, you can use facet_grid instead of facet_wrap:\n\n# scatterplots with panels for vital status in one row\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_grid(rows = vars(vital_status)) \n\n\n\n\nThis method can also plot panels in columns.\nWe may want to show interactions between two categorical variables, by arranging panels into rows according to one variable and columns according to another:\n\n# add another variable using faceting\nggplot(smoke_complete) +\n  geom_point(aes(x = age_at_diagnosis, y = cigarettes_per_day, color = disease)) +\n  facet_grid(rows = vars(vital_status), cols = vars(disease)) # arrange plots via variables in rows, columns\n\n\n\n\nDon’t forget to look at the help documentation (e.g., ?facet_grid) to learn more about additional ways to customize your plots!\n\nChallenge-panels\nAlter your last challenge plot of (birth year by number of patients) to show each gender in separate panels.\n\n\nChallenge-axis\nHow do you change axis formatting, like tick marks and lines? Hint: You may want to use Google!"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#wrapping-up",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#wrapping-up",
    "title": "Data visualization in R",
    "section": "Wrapping up",
    "text": "Wrapping up\nThis material introduced you to ggplot as a tool for data visualization, allowing you to now create publication-quality images using R code. Combined with our previous explorations of the basic principles of R syntax, importing and extracting data with base R, and manipulating data using tidyverse, you should be equipped to continue learning about R on your own and developing code to meet your research needs.\nIf you are interested in learning more about ggplot: - Documentation for all ggplot features is available here. - RStudio also publishes a ggplot cheat sheet that is really handy!"
  },
  {
    "objectID": "resources/computing/intro_to_r/4_data_visualisation.html#extra-exercises",
    "href": "resources/computing/intro_to_r/4_data_visualisation.html#extra-exercises",
    "title": "Data visualization in R",
    "section": "Extra exercises",
    "text": "Extra exercises\n\nChallenge-improve\nImprove one of the plots previously created today, by changing thickness of lines, name of legend, or color palette http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/"
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html",
    "title": "Good practices in R",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction"
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html#writing-clean-and-readable-code",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html#writing-clean-and-readable-code",
    "title": "Good practices in R",
    "section": "Writing Clean and Readable Code",
    "text": "Writing Clean and Readable Code\nExample in R:\n# Good: Use meaningful variable names and comments\npopulation_size &lt;- 1000  # Number of individuals in the population\nsample_size &lt;- 100       # Number of individuals in the sample\n\n# Avoid: Using unclear or abbreviated variable names\nps &lt;- 1000\nss &lt;- 100"
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html#using-comments-and-documentation",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html#using-comments-and-documentation",
    "title": "Good practices in R",
    "section": "Using Comments and Documentation",
    "text": "Using Comments and Documentation\n# Good: Add comments to explain complex operations or logic\ncalculate_mean &lt;- function(data) {\n    # Calculate the sum of elements in the data\n    total &lt;- sum(data)\n    \n    # Calculate the mean by dividing the total by the number of elements\n    mean_value &lt;- total / length(data)\n    \n    return(mean_value)\n}\n\n# Avoid: Lack of comments and explanation\ncalc_mean &lt;- function(d) {\n    t &lt;- sum(d)\n    m &lt;- t / length(d)\n    return(m)\n}"
  },
  {
    "objectID": "resources/computing/intro_to_r/5_good_practice_r.html#avoiding-global-variables-and-code-dependencies",
    "href": "resources/computing/intro_to_r/5_good_practice_r.html#avoiding-global-variables-and-code-dependencies",
    "title": "Good practices in R",
    "section": "Avoiding Global Variables and Code Dependencies",
    "text": "Avoiding Global Variables and Code Dependencies\n# Good: Use local variables within functions to avoid global scope pollution\ncalculate_variance &lt;- function(data) {\n    n &lt;- length(data)\n    mean_value &lt;- sum(data) / n\n    \n    # Calculate the variance using local variables\n    variance &lt;- sum((data - mean_value)^2) / (n - 1)\n    \n    return(variance)\n}\n\n# Avoid: Using global variables in functions\nmean_value &lt;- 0\ncalc_variance &lt;- function(data) {\n    n &lt;- length(data)\n    mean_value &lt;&lt;- sum(data) / n\n    \n    # Calculate the variance using global variables\n    variance &lt;- sum((data - mean_value)^2) / (n - 1)\n    \n    return(variance)\n}"
  },
  {
    "objectID": "resources/computing/computing-cheatsheets.html",
    "href": "resources/computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references."
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html",
    "href": "resources/computing/intro_to_python/good_practice_python.html",
    "title": "Good practices in Python",
    "section": "",
    "text": "Important\n\n\n\n🏗 Under construction"
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html#writing-clean-and-readable-code",
    "href": "resources/computing/intro_to_python/good_practice_python.html#writing-clean-and-readable-code",
    "title": "Good practices in Python",
    "section": "Writing Clean and Readable Code",
    "text": "Writing Clean and Readable Code\n# Good: Use meaningful variable names and comments\npopulation_size = 1000  # Number of individuals in the population\nsample_size = 100       # Number of individuals in the sample\n\n# Avoid: Using unclear or abbreviated variable names\nps = 1000\nss = 100"
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html#using-comments-and-documentation",
    "href": "resources/computing/intro_to_python/good_practice_python.html#using-comments-and-documentation",
    "title": "Good practices in Python",
    "section": "Using Comments and Documentation",
    "text": "Using Comments and Documentation\n# Good: Add comments to explain complex operations or logic\ndef calculate_mean(data):\n    # Calculate the sum of elements in the data\n    total = sum(data)\n    \n    # Calculate the mean by dividing the total by the number of elements\n    mean_value = total / len(data)\n    \n    return mean_value\n\n# Avoid: Lack of comments and explanation\ndef calc_mean(d):\n    t = sum(d)\n    m = t / len(d)\n    return m"
  },
  {
    "objectID": "resources/computing/intro_to_python/good_practice_python.html#avoiding-global-variables-and-code-dependencies",
    "href": "resources/computing/intro_to_python/good_practice_python.html#avoiding-global-variables-and-code-dependencies",
    "title": "Good practices in Python",
    "section": "Avoiding Global Variables and Code Dependencies",
    "text": "Avoiding Global Variables and Code Dependencies\n# Good: Use local variables within functions to avoid global scope pollution\ndef calculate_variance(data):\n    n = len(data)\n    mean_value = sum(data) / n\n    \n    # Calculate the variance using local variables\n    variance = sum((data - mean_value)**2) / (n - 1)\n    \n    return variance\n\n# Avoid: Using global variables in functions\nmean_value = 0\ndef calc_variance(data):\n    n = len(data)\n    global mean_value\n    mean_value = sum(data) / n\n    \n    # Calculate the variance using global variables\n    variance = sum((data - mean_value)**2) / (n - 1)\n    \n    return variance"
  },
  {
    "objectID": "resources/tips/tips_coding.html",
    "href": "resources/tips/tips_coding.html",
    "title": "Good practices for coding",
    "section": "",
    "text": "What will happen when you show your work to someone else if you don’t follow a guide style. (From Xkcd: code quality)"
  },
  {
    "objectID": "resources/tips/tips_coding.html#why-should-you-care",
    "href": "resources/tips/tips_coding.html#why-should-you-care",
    "title": "Good practices for coding",
    "section": "",
    "text": "What will happen when you show your work to someone else if you don’t follow a guide style. (From Xkcd: code quality)"
  },
  {
    "objectID": "resources/tips/tips_coding.html#clear-naming-of-variables-and-functions",
    "href": "resources/tips/tips_coding.html#clear-naming-of-variables-and-functions",
    "title": "Good practices for coding",
    "section": "Clear naming of variables and functions",
    "text": "Clear naming of variables and functions\n\nBe explicit in your naming. If you name a variable my_variable, don’t name another one my_var or my_var_. If you name a function my_function, don’t name another one my_func or my_func_.\nNames should be self-explanatory. If you need to add a comment to explain what a variable or a function does, it means that you should change its name. For example, my_variable is a bad name, but number_of_samples is a good name. df, df2, … are bad names, but raw_data, ìmputed_data, … are good names."
  },
  {
    "objectID": "resources/tips/tips_coding.html#consistency",
    "href": "resources/tips/tips_coding.html#consistency",
    "title": "Good practices for coding",
    "section": "Consistency",
    "text": "Consistency\n\nBe consistent in your style. If you start a project, try to follow the style of the project. If you join a project, try to follow the style of the project.\nBe consistent in your naming. If you name a variable my_variable, don’t name another one myVariable or myVariable_. If you name a function my_function, don’t name another one myFunction or myFunction_.\nBe consistent in your formatting. If you use 2 spaces for indentation, don’t use 4 spaces for indentation. If you use 2 spaces for indentation, don’t use tabs for indentation.\n\nAll of this will make your code easier to read and understand."
  },
  {
    "objectID": "resources/tips/tips_coding.html#code-style-guidelines",
    "href": "resources/tips/tips_coding.html#code-style-guidelines",
    "title": "Good practices for coding",
    "section": "Code Style Guidelines",
    "text": "Code Style Guidelines\nConsistency in coding style is essential for making the code more readable and understandable by others. Adopting a standard coding style across your team helps avoid confusion and reduces the time spent on code reviews. For each language (Julia, R, and Python), there are widely accepted coding style guidelines:\n\n\nJulia\n\nJulia official style guide\nmore opinionated SciML Style Guide\n\n\n\n\nPython\n\nPEP 8\n\n\n\n\nR\n\ntidyverse style guide\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere exists tools to help you check your code style and correct the basic mistakes. They are called linters. For example, in Julia, you can use JuliaFormatter.jl, in Python black and in R styler."
  },
  {
    "objectID": "resources/tips/virtual_environments.html",
    "href": "resources/tips/virtual_environments.html",
    "title": "Virtual environments",
    "section": "",
    "text": "Package managers typically maintain a database of software dependencies and version information to prevent software mismatches and missing prerequisites. When package versions collide, this can lead to problems ranging from error messages and frustration to silent bugs and unexpected code behavior !"
  },
  {
    "objectID": "resources/tips/virtual_environments.html#why-cant-i-just-install-the-packages-i-need",
    "href": "resources/tips/virtual_environments.html#why-cant-i-just-install-the-packages-i-need",
    "title": "Virtual environments",
    "section": "Why can’t I just install the packages I need ?",
    "text": "Why can’t I just install the packages I need ?\nInstead of installing everything globally and risking conflicts, virtual environments give you separate spaces for each project. This means no more worrying about messing up your setup! Plus, package managers make installing, updating, and removing packages a breeze, saving you time and hassle. You can easily share your projects with classmates and reproduce your work on any machine.\n\n\n\n\n\n\nNote\n\n\n\nThis is a very brief introduction to virtual environments and package managers. For more details, please see the documentation of the package manager you use.\n\nPython virtual environment tutorial\nJulia package manager documentation\nR renv documentation"
  },
  {
    "objectID": "resources/tips/virtual_environments.html#benefits",
    "href": "resources/tips/virtual_environments.html#benefits",
    "title": "Virtual environments",
    "section": "Benefits",
    "text": "Benefits\nHere are some examples of why using virtual environments and package managers can be incredibly useful for scientific computing:\n\nProject Isolation: Let’s say you’re working on two different projects—one in R and another in Python. Each project requires different versions of certain packages. By using virtual environments, you can create separate environments for each project, ensuring that the specific package versions needed for each don’t interfere with one another.\nReproducibility: With virtual environments, you can easily share your projects with classmates or professors, ensuring that they can replicate your exact setup without any compatibility issues. This enhances the reproducibility of your work and allows others to verify your results.\nDependency Management: Sometimes, a package may rely on a specific version of another package to work correctly. Package managers handle these dependencies automatically, saving you the headache of figuring out and managing dependencies manually.\nExperimentation: Working on a new statistical model and want to test different libraries or versions? With virtual environments, you can create a sandbox to experiment freely without worrying about affecting your main setup.\nCollaboration: When collaborating with classmates or researchers, having consistent environments through virtual environments ensures that everyone is on the same page. It prevents conflicts arising from different package versions and improves overall productivity.\nSystem Cleanliness: Installing packages globally can clutter your system, making it difficult to manage and potentially leading to conflicts between different software. Virtual environments keep your system clean and organized.\nVersion Control: Using virtual environments makes it easier to integrate your projects with version control systems like Git. You can include the configuration files for your virtual environment in the repository, making it simpler for others to work on the project.\nEfficient Updates: Package managers allow you to update packages quickly and efficiently. You can easily check for updates, install the latest versions, and keep your project up-to-date with the latest features and bug fixes.\n\nBy embracing virtual environments and package managers, you’ll have a smoother, more organized, and productive workflow, making your research and analysis process much more enjoyable and effective."
  },
  {
    "objectID": "resources/tips/virtual_environments.html#downsides",
    "href": "resources/tips/virtual_environments.html#downsides",
    "title": "Virtual environments",
    "section": "Downsides",
    "text": "Downsides\nYou will have to run a few commands everytime you start a new project. This is a small price to pay for the benefits you get. (You may also need to activate the virtual environment everytime you start a new shell session, but this can be automated)."
  },
  {
    "objectID": "resources/tips/virtual_environments.html#how",
    "href": "resources/tips/virtual_environments.html#how",
    "title": "Virtual environments",
    "section": "How ?",
    "text": "How ?\nrenv for R package management, venv, conda or others for Python package management. Julia has this feature built in using Pkg.\n\nCreating a virtual environment\n\n\n\nIn terminal\npython3.6 -m venv my_env \nsource my_env/bin/activate\n\n\n\nIn Julia repl\nusing Pkg\nPkg.activate(\"my_env\")\n\n\n\nIn R console\nrenv::init()\n\n\n\n\n\nAdding packages to the virtual environment (already activated)\n\n\n\nIn terminal\npip install numpy\n\n\n\nIn Julia repl\nPkg.add(Plots)\n\n\n\nIn R console\nrenv::install(\"tidyverse\")\n\n\n\n\n\nRecreating the virtual environment from a file (after creating the environment)\n\n\n\nIn terminal\npip install -r requirements.txt\n\n\n\nIn Julia repl\nPkg.instantiate()\n\n\n\nIn R console\nrenv::restore()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPython is the only one that has a specific command to create a file with the list of packages.\npip freeze &gt; requirements.txt\nFor R and Julia, the file is created automatically when you add a package to the environment and updates automatically when you add or remove packages. (For Renv you may need to run renv::snapshot() to update the file sometimes).\n\n\nFor more details on the commands or the OS specificity please see the documentation of the package manager you are using:\n\nrenv renv wignet\nJulia Julia pkg\nPython Python venv and pip, conda\n\n\nStep by step tutorial\nPython | Julia | R\n\nPython Virtual Environments (venv):\nVirtual environments in Python enable you to create isolated environments for each project. Here’s how to use venv:\n\nOpen your terminal or command prompt.\nNavigate to your project’s directory.\nCreate a new virtual environment:\npython -m venv my_project_env\nActivate the virtual environment:\n\nOn Windows:\n\nmy_project_env\\Scripts\\activate\n\nOn macOS/Linux:\n\nsource my_project_env/bin/activate\nInstall packages within the virtual environment:\npip install package_name\nDeactivate the virtual environment when you’re done:\ndeactivate\n\n\n\nJulia Package Manager (Pkg):\nJulia’s Pkg allows you to manage and install packages effortlessly. Here’s how to use Pkg:\n\nOpen the Julia REPL (Read-Eval-Print Loop).\nTo enter package management mode, type ].\nCreate a new environment and activate it:\nactivate my_project_env\nInstall packages within the environment:\nadd package_name\nUpdate packages:\nupdate\nTo exit package management mode, press Ctrl + C or type exit().\n\n\n\nR Package Manager (renv):\nIn R, renv provides a similar functionality to Python’s venv and Julia’s Pkg. Here’s how to use renv:\n\nOpen your R console or RStudio.\nInstall the renv package (if not already installed):\ninstall.packages(\"renv\")\nInitialize renv for your project:\nrenv::init()\nInstall packages within the renv environment:\ninstall.packages(\"package_name\")\nRestore the project’s environment to remove any packages that aren’t listed in the lockfile:\nrenv::restore()\nor update the lockfile to include any new packages:\nrenv::snapshot()\nDeactivate the renv environment (optional):\nrenv::deactivate()\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\ninitialization\nactivate\ndeactivate\nadd package\n\n\n\n\nrenv\nrenv::init()\nrenv:activate()\nrenv::deactivate()\nrenv::install()\n\n\nvenv\npython -m venv {name}\nsource {name}/bin/activate\ndeactivate\npip install ...\n\n\nJulia\n] activate {name}\n] activate {name}\n] activate\n] add ...\n\n\n\n\n\n\n\n\n\n\n\n\nfiles to share\nto recreate\n\n\n\n\nR and renv\nrenv.lock\nrenv::restore()\n\n\nPython and venv\nrequirements.txt\npip install -r requirements.txt\n\n\nJulia\nProject.toml\n] instantiate"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 517: Statistical computation and visualisation",
    "section": "",
    "text": "This page contains an outline of the topics, exercises (ex.), and assignments (ae.) for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester. See the overview page for more information about the course.\n\n\n\n\n\n\nTip\n\n\n\nIt can be useful to check the available tutorials to get ready for the assignments quickly. You can find them under the resources section on your left. Please check the FAQ page for common questions and answers.\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n    \n      WEEK\n      DATE\n      TOPIC\n      RESOURCES\n      AE\n      EX\n      DUE\n    \n  \n  \n    1\nFri, Sep 22\nSoftware and Data Considerations\n\n📖 linear regression\n\n📖 ae-01\n\n⌨️ ex-01\n\n\n    2\nFri, Sep 29\nGraphics and Visualization\n\n\n💻 mini project\n\n\n\n    \nSun, Oct 1\n\n\n\n\n📖 ae-01\n\n    3\nFri, Oct 6\nNonparametric Regression\n\n🖥️ kde interactive  from notes\n\n📖 ae-02\n\n\n\n    \nSun, Oct 8\n\n\n\n\n\n    4\nFri, Oct 13\n\n\n\n\n\n    \nSun, Oct 15\n\n\n\n\n📖 ae-02\n\n    5\nFri, Oct 20\n\n\n\n\n\n    \nSun, Oct 22\n\n\n\n\n💻 mini project\n\n    6\nFri, Oct 27\n\n\n\n\n\n    \nSun, Oct 29\n\n\n\n\n\n    7\nFri, Nov 3\n\n\n\n\n\n    \nSun, Nov 5\n\n\n\n\n\n    8\nFri, Nov 10\n\n\n\n\n\n    \nSun, Nov 12\n\n\n\n\n\n    9\nFri, Nov 17\n\n\n\n\n\n    \nSun, Nov 19\n\n\n\n\n\n    10\nFri, Nov 24\n\n\n\n\n\n    \nSun, Nov 26\n\n\n\n\n\n    11\nFri, Dec 1\n\n\n\n\n\n    \nSun, Dec 3\n\n\n\n\n\n    12\nFri, Dec 8\n\n\n\n\n\n    \nSun, Dec 10\n\n\n\n\n\n    13\nFri, Dec 15\n\n\n\n\n\n    \nSun, Dec 17\n\n\n\n\n\n    14\nFri, Dec 22\n\n\n\n\n\n    \nSun, Dec 24\n\n\n\n\n💻 project"
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Important\n\n\n\nDue date: 11:59pm on Sunday, 1 October 2023."
  },
  {
    "objectID": "assignments/assignment-01.html#today",
    "href": "assignments/assignment-01.html#today",
    "title": "Assignment 1",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ncreate a github account and familiarise yourself with git and GitHub\naccept the first assignment and submit your work\nmake sure GitHub Classroom works as expected and you can submit your work."
  },
  {
    "objectID": "assignments/assignment-01.html#assignment",
    "href": "assignments/assignment-01.html#assignment",
    "title": "Assignment 1",
    "section": "Assignment",
    "text": "Assignment\nThe goal of this assignment is for you to fit a linear model. For a refresher on linear models, see the ressource for this week. Your task will be to code a function that fits a linear model to a given dataset. You will need to fill in the blacks in the code located in your personal repository ae-1-YOUR_GITHUB_USERNAME.\nHere is how to proceed:\n\nWe expect you to know the basis of progamming in either R, Julia or Python. Please read the introduction to the language of your choice (R, Julia, or Python) if it is not the case.\n\n\nRead the README.md file in your assignment repository and follow the instructions there."
  },
  {
    "objectID": "exercises/exercise-01.html",
    "href": "exercises/exercise-01.html",
    "title": "Exercise 1: Peeking",
    "section": "",
    "text": "The severity depends on the number of peeks the researcher takes at the data and on the number of observations added between peeks.\nExercise: The “Example of Peeking” below is an example of a small simulation study, checking whether a designed test strategy respects the nominal level \\(\\alpha = 0.05\\) or not. Incorporate further levels of peeking and see how it affects the nominal significance level.\n\npeeking &lt;- function(a,b=10){\n  x &lt;- rnorm(25)\n  Tstat &lt;- mean(x)/sd(x)*sqrt(length(x))\n  if(abs(Tstat) &gt; qt(0.975,length(x)-1)){\n    return(Tstat)\n  }else{\n    x &lt;- append(x, rnorm(b))\n    Tstat &lt;- mean(x)/sd(x)*sqrt(length(x))\n    return(Tstat)\n  }\n}\nset.seed(517)\nTstats &lt;- sapply(1:10000,peeking)\nmean(I(abs(Tstats) &gt; qnorm(0.975)))\n\n[1] 0.0851"
  },
  {
    "objectID": "ae/ae20.html",
    "href": "ae/ae20.html",
    "title": "Hypothesis tests and confidence intervals",
    "section": "",
    "text": "Exam 2 Thursday\n\nno lab Friday, no TA office hours Friday/Monday"
  },
  {
    "objectID": "ae/ae20.html#bulletin",
    "href": "ae/ae20.html#bulletin",
    "title": "Hypothesis tests and confidence intervals",
    "section": "",
    "text": "Exam 2 Thursday\n\nno lab Friday, no TA office hours Friday/Monday"
  },
  {
    "objectID": "ae/ae20.html#today",
    "href": "ae/ae20.html#today",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ndefine type I and type II error\ncompare hypothesis tests with confidence intervals\npractice conducting hypothesis tests"
  },
  {
    "objectID": "ae/ae20.html#getting-started",
    "href": "ae/ae20.html#getting-started",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae20.qmd\",\ndestfile = \"ae20.qmd\")"
  },
  {
    "objectID": "ae/ae20.html#load-packages",
    "href": "ae/ae20.html#load-packages",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)"
  },
  {
    "objectID": "ae/ae20.html#practice",
    "href": "ae/ae20.html#practice",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Practice",
    "text": "Practice\n\nLoad data\nThe stent30 data set comes from the openintro package and is from a study conducted in 2011 on the effects of arterial stents as a therapy for stroke patients. See the original publication:\nChimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Med- ical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993- 1003. doi: 10.1056/NEJMoa1105335.\nor check ?stent30 for more information.\n\ndata(stent30)\n\n\nglimpse(stent30)\n\nRows: 451\nColumns: 2\n$ group   &lt;fct&gt; treatment, treatment, treatment, treatment, treatment, treatme…\n$ outcome &lt;fct&gt; stroke, stroke, stroke, stroke, stroke, stroke, stroke, stroke…\n\n\n\nExercise 1\nDo stents affect stroke outcome in patients?\n\nWrite the null and alternative hypothesis. Report the observed statistic.\nSimulate under the null and visualize the null distribution.\nCompute and report the p-value, compare to \\(\\alpha = 0.05\\) and make a conclusion with appropriate context"
  },
  {
    "objectID": "ae/ae20.html#confidence-intervals-and-hypothesis-tests",
    "href": "ae/ae20.html#confidence-intervals-and-hypothesis-tests",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Confidence intervals and hypothesis tests",
    "text": "Confidence intervals and hypothesis tests\nHere we revisit the data from the first three seasons of NC Courage games (2017-2019).\n\ncourage = read_csv(\"https://sta101-fa22.netlify.app/static/labs/data/courage.csv\")\n\n\nglimpse(courage)\n\nRows: 78\nColumns: 10\n$ game_id     &lt;chr&gt; \"washington-spirit-vs-north-carolina-courage-2017-04-15\", …\n$ game_date   &lt;chr&gt; \"4/15/2017\", \"4/22/2017\", \"4/29/2017\", \"5/7/2017\", \"5/14/2…\n$ game_number &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ home_team   &lt;chr&gt; \"WAS\", \"NC\", \"NC\", \"BOS\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NC\", …\n$ away_team   &lt;chr&gt; \"NC\", \"POR\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NJ\", \"NC\", \"KC\", \"…\n$ opponent    &lt;chr&gt; \"WAS\", \"POR\", \"ORL\", \"BOS\", \"ORL\", \"CHI\", \"NJ\", \"CHI\", \"KC…\n$ home_pts    &lt;dbl&gt; 0, 1, 3, 0, 3, 1, 2, 3, 2, 3, 0, 0, 2, 1, 1, 0, 1, 2, 2, 2…\n$ away_pts    &lt;dbl&gt; 1, 0, 1, 1, 1, 3, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 2, 0, 3, 1…\n$ result      &lt;chr&gt; \"win\", \"win\", \"win\", \"win\", \"loss\", \"loss\", \"win\", \"loss\",…\n$ season      &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017…\n\n\nDo National Women’s Soccer League (NWSL) teams have a home-field advantage? We’ll answer this question in a few separate ways.\nHypothesis testing framework: does NC Courage score a significantly different number of points (on average) away than at home?\n\nExercise 2\n\nCreate a new column location that tells you whether the courage are “home” or “away”\nCreate a new column pts that always reports the Courage points scored in a game.\nSave your result as a new data frame titled courage2.\n\n\n# code here\n\n\n\nExercise 3\nTo answer the question does NC Courage score a significantly different number of points (on average) away than at home?\n\nWrite the null and alternative hypothesis. Report the observed statistic.\nSimulate under the null and visualize the null distribution.\nCompute and report the p-value, compare to \\(\\alpha = 0.05\\) and make a conclusion with appropriate context\n\n\n# code here\n\n\n\nExercise 4\n\nReport the mean difference between away and home games and report a 95% bootstrap confidence interval. Use set.seed(3) and reps=5000 Interpret your interval in context.\n\n\n# code here\n\n\n\nExercise 5\nIs there a better way we could investigate whether or not the Courage have a home-field advantage? Why?"
  },
  {
    "objectID": "ae/ae20.html#notes",
    "href": "ae/ae20.html#notes",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Notes",
    "text": "Notes\n\nType 1 and Type 2 Errors\n\n\n\nTruth\nReject the null\nFail to reject the null\n\n\n\n\n\\(H_0\\) is true\nType 1 error\n✔️\n\n\n\\(H_A\\) is true\n✔️\nType 2 error\n\n\n\nThe significance level, \\(\\alpha\\), is the probability of a type 1 error. In some contexts, a type 1 error may be referred to as a “false positive” and a type 2 error as a “false negative”.\nIntuitively, by considering extremes, one can see a trade-off exists between type 1 and type 2 error.\n\nIf \\(\\alpha = 0\\), then the p-value stands no chance of being smaller than \\(\\alpha\\) and we always fail to reject the null. This makes type 1 errors impossible.\n\nSimilarly, if \\(\\alpha = 1\\), then all p-values will be smaller than \\(\\alpha\\) and type 2 errors will become impossible, because we will always reject the null.\n\\(\\beta\\) is used to denote the probability of a type 2 error.\nThe power of a test is \\(1 - \\beta\\), which is the probability that your test rejects the null hypothesis when the null hypothesis is false."
  },
  {
    "objectID": "ae/ae20.html#why-its-important-to-be-careful-with-interpretation",
    "href": "ae/ae20.html#why-its-important-to-be-careful-with-interpretation",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Why it’s important to be careful with interpretation",
    "text": "Why it’s important to be careful with interpretation\n\n(And why hypothesis tests don’t tell the whole story)\nThe data for this example comes from Confounding and Simpson’s paradox1 by Julious and Mullee.\nThe data examines 901 individuals with diabetes and includes the following variables\n\ninsulin_dep: whether or not the patient has insulin dependent or non-insulin dependent diabetes\nage: whether or not the individual is less than 40 years old\nsurvival: whether or not the individual survived the length of the study\n\n\ndiabetes = read_csv(\"https://sta101.github.io/static/appex/data/diabetes.csv\")\n\nFlex Aisher thinks people with insulin dependent diabetes actually survive longer than those without insulin dependence. Flex wants to formally test his hypothesis.\nLet \\(p_{d}\\) be the probability of insulin dependent survival and \\(p_{i}\\) be the probability of insulin independent survival.\n\\[\nH_0: p_{d} - p_{i} = 0\n\\]\n\\[\nH_A: p_{d} - p_{i} &gt; 0\n\\]\nAt first glance the data seem to back up his claim…\n\n\nExercise 6\nCompute the probability of survival and death for diabetic individuals with and without insulin dependence.\n\n#  code here\n\n\n\nExercise 7\nIs Flex’s claim significant at the \\(\\alpha = 0.05\\) level? Perform a hypothesis test and report your results.\n\n# code here\n\n\n\nExercise 8\nIs the aggregate data misleading? Use the code chunk below to investigate further.\n\n# code here"
  },
  {
    "objectID": "ae/ae8.html",
    "href": "ae/ae8.html",
    "title": "Linear regression III",
    "section": "",
    "text": "Lab 3 due tonight\nLooking towards next week, please fill out this optional form to request group members (from your lab) to work on the projects with.\nExam 1 released tonight and due Monday\n\nno TA office hours Friday/Monday\nask questions early"
  },
  {
    "objectID": "ae/ae8.html#bulletin",
    "href": "ae/ae8.html#bulletin",
    "title": "Linear regression III",
    "section": "",
    "text": "Lab 3 due tonight\nLooking towards next week, please fill out this optional form to request group members (from your lab) to work on the projects with.\nExam 1 released tonight and due Monday\n\nno TA office hours Friday/Monday\nask questions early"
  },
  {
    "objectID": "ae/ae8.html#warm-up",
    "href": "ae/ae8.html#warm-up",
    "title": "Linear regression III",
    "section": "Warm-up",
    "text": "Warm-up\nCheck you understanding! Answer the following…\n\nTo “fit” a linear model means…[fill in the blank]\nIs \\(y = \\beta_0 + \\beta_1 \\log(x_1)+ \\beta_2 x_2^2 + \\epsilon\\) a linear model? Why or why not?"
  },
  {
    "objectID": "ae/ae8.html#today",
    "href": "ae/ae8.html#today",
    "title": "Linear regression III",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nfit and interpret models with categorical predictors\nfit and interpret models with interactive effects"
  },
  {
    "objectID": "ae/ae8.html#getting-started",
    "href": "ae/ae8.html#getting-started",
    "title": "Linear regression III",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae8.qmd\",\ndestfile = \"ae8.qmd\")"
  },
  {
    "objectID": "ae/ae8.html#load-packages-and-data",
    "href": "ae/ae8.html#load-packages-and-data",
    "title": "Linear regression III",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\nToday we will return to our Palmer penguins data set\n\ndata(penguins)\n\nUse ?penguins or click here for more info."
  },
  {
    "objectID": "ae/ae8.html#notes",
    "href": "ae/ae8.html#notes",
    "title": "Linear regression III",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "ae/ae8.html#main-effects",
    "href": "ae/ae8.html#main-effects",
    "title": "Linear regression III",
    "section": "Main effects",
    "text": "Main effects\nUp until now, we’ve seen models that look like this:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\epsilon\n\\]\nHere’s an example:\n\\(y\\): body mass (g)\n\\(x_1\\): bill length (mm)\n\\(x_2\\): 1 if island Dream, 0 otherwise\n\\(x_3\\): 1 if island Torgersen, 0 otherwise\n\\[\ny = \\beta_0 + {\\beta_1} x_1 + {\\beta_2} x_2 + {\\beta_3} x_3 + \\epsilon\n\\]\nNotice that\n\nWe have a categorical predictor island that takes three values: Dream, Torgersen, and Biscoe.\nDespite taking three values, there are only two island variables in the model. One for Dream and one for Torgersen. Biscoe island is considered the default. This always occurs when we have a categorical variable – one category is considered the default.\nBill length only impacts body mass via the term \\(\\beta_1 x_1\\).\n\\(x_2\\) and \\(x_3\\) can be thought of as turning on or off a constant.\n\nLet’s visualize the main effects model below.\n\n\n\n\n\nWe can fit the “main effects” model above with our standard procedure:\n\nmain_fit = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ bill_length_mm + island, data = penguins)\n\n  main_fit %&gt;%\n  tidy()\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)       1226.     243.        5.04 7.58e- 7\n2 bill_length_mm      77.1      5.31     14.5  1.66e-37\n3 islandDream       -919.      58.6     -15.7  5.15e-42\n4 islandTorgersen   -523.      85.5      -6.12 2.64e- 9\n\n\nIf we want to know how bill length relates to body mass for penguins on island Biscoe, we plug in \\(0\\) for \\(x_2\\) and \\(x_3\\) and write the resulting model. If we repeat as appropriate for each island, the result is 3 separate fitted models:\nBiscoe:\n\\[\n\\hat{y} = 1225.8 + 77.1 x_1\n\\]\nDream:\n\\[\n\\hat{y} = 1225.8 + 77.1 x_1 - 919.1\n\\]\nTorgersen:\n\\[\n\\hat{y} = 1225.8 + 77.1 x_1 -523.3\n\\]\nNotice that in each case, the slope associated with bill length (\\(x_1\\)) is the same.\n\nInteraction effects\nInteraction effect models contain products of predictors, e.g.\n\\[\ny = {\\beta_0} + {\\beta_1} x_1 + {\\beta_2} x_2 + {\\beta_3} x_3 +  {\\beta_4} x_1 x_2 + {\\beta_5} x_1 x_3 + \\epsilon\n\\]\nHere we have an interaction between bill length and island (\\(\\beta_4 x_1 x_2\\) and \\(\\beta_5 x_1 x_3\\)).\nTake-away idea: \\(x_1\\) is related to \\(y\\) but the relationship changes depending on \\(x_2\\) and \\(x_3\\).\nThe simplest scenario is one of “group membership”. In other words, knowing the group your measurement belongs to affects the relationship between \\(x_1\\) and \\(y\\).\nHere, we see bill length (\\(x_1\\)) show up multiple times in our linear model paired with islands. In other words, the relationship between bill length and body mass depends on the island a penguin is from.\nWe fit this interaction model using the code below:\n\ninteraction_fit = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ bill_length_mm * island, data = penguins)\n\n  interaction_fit %&gt;%\n  tidy()\n\n# A tibble: 6 × 5\n  term                           estimate std.error statistic  p.value\n  &lt;chr&gt;                             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                     -1726.     292.       -5.91 8.43e- 9\n2 bill_length_mm                    142.       6.42     22.2  9.14e-68\n3 islandDream                      4479.     395.       11.3  2.03e-25\n4 islandTorgersen                  2871.     778.        3.69 2.60e- 4\n5 bill_length_mm:islandDream       -121.       8.77    -13.8  1.93e-34\n6 bill_length_mm:islandTorgersen    -76.6     19.5      -3.92 1.07e- 4\n\n\n\n\n\n\n\nInterpreting interactions can be difficult, especially without writing things down. To make it easier, we will compare the implied linear models:\nPlug in 0 for islandDream (\\(x_2\\)) and 0 for islandTorgersen (\\(x_3\\)) to get the linear model for islandBiscoe penguins\nPlug in 1 for islandDream (\\(x_2\\)) and 0 for islandTorgersen (\\(x_3\\)) to get the linear model for islandDream penguins\nPlug in 0 for islandDream (\\(x_2\\)) and 1 for islandTorgersen (\\(x_3\\)) to get the linear model for islandTorgersen penguins\n\nBiscoe fitted model:\n\n\\[\n\\hat{y} = -1726.0+ 142.3 x_1\n\\]\n\nDream fitted model:\n\n\\[\n\\hat{y} = -1726.0 + 142.3 x_1 + 4478.7 -120.6 x_1\n\\]\nCombine terms:\n\\[\n\\hat{y} = 2752.7 + 21.7 x_1\n\\]\n\nExercise 1\nWrite out the fitted model for Torgersen island below.\n\nTorgersen model: \\[\n\\hat{y} = [\\text{write here}]\n\\]\n\n\n\n\nInterpreting\nNow we can interpret the interaction model by comparing bill length slopes between islands.\n\nFor a unit increase in bill length of a penguin from the island Dream, how much do we expect the body mass to increase?\n\n\nExercise 2\n\nYou measured the bill length of a penguin from island Biscoe and a penguin from island Torgersen a year ago. You re-measure them today and find the bill length of each one grew by exactly 2 mm. How much mass do you expect each penguin to have gained?\n\n\n\nExercise 3\nAre the intercepts meaningful?\n\n\nExercise 4\nIs the relationship between Body mass (g) and Bill depth (mm) positive or negative? Create a convincing argument from the data.\n\n\nExercise 5\nBuild a linear model of body mass using bill depth and one other predictor of your choosing (hint: see previous exercise!)\n\nWrite out a linear model with both predictors and fit the model.\nFit the linear model\nDo you prefer this model to the interaction effects model from a previous exercise? Why?"
  },
  {
    "objectID": "ae/ae14.html",
    "href": "ae/ae14.html",
    "title": "Likelihoods",
    "section": "",
    "text": "Lab 05 due Thursday\nFinal project released"
  },
  {
    "objectID": "ae/ae14.html#bulletin",
    "href": "ae/ae14.html#bulletin",
    "title": "Likelihoods",
    "section": "",
    "text": "Lab 05 due Thursday\nFinal project released"
  },
  {
    "objectID": "ae/ae14.html#today",
    "href": "ae/ae14.html#today",
    "title": "Likelihoods",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nbe able to define a likelihood function\nunderstand the intuition behind likelihood-based inference"
  },
  {
    "objectID": "ae/ae14.html#getting-started",
    "href": "ae/ae14.html#getting-started",
    "title": "Likelihoods",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae14.qmd\",\ndestfile = \"ae14.qmd\")"
  },
  {
    "objectID": "ae/ae14.html#load-packages",
    "href": "ae/ae14.html#load-packages",
    "title": "Likelihoods",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\ncoin_flips = read_csv(\n  \"https://sta101-fa22.netlify.app/static/appex/data/coin_flips_LE.csv\"\n  )"
  },
  {
    "objectID": "ae/ae14.html#the-likelihood-function",
    "href": "ae/ae14.html#the-likelihood-function",
    "title": "Likelihoods",
    "section": "The likelihood function",
    "text": "The likelihood function\n\\[\nL(\\theta | x) = f(x | \\theta)\n\\]\nThe likelihood of parameter(s) \\(\\theta\\) given the data \\(x\\) is equivalent to the density of sample \\(x\\) given the parameter(s) \\(\\theta\\).\n\nIf \\(x\\) is a discrete random variable, the likelihood function is the probability of the data given \\(\\theta\\).\n\n\nExample 1\n\\[\nX \\sim \\text{Binomial}(k, p)\n\\]\nwhere \\(k\\) is the number of trials and \\(p\\) is the probability of a success. The parameters of the distribution are \\(k\\) and \\(p\\).\n\n\n\n\n\n\nNote\n\n\n\nRecall: you can find the formula for the binomial density function in the documentation. See ?dbinom\n\n\n\nExercise 1\nImagine we flip a coin 10 times and observe 7 heads and 3 tails. All together we have:\n\n\\(k\\): __\n\\(x\\): __\n\nThere is some true \\(p\\) for this coin. What is your best guess (or estimate), \\(\\hat{p}\\)?\n\nnum_success = coin_flips %&gt;%\n  summarize(total = sum(result)) %&gt;%\n  pull(total)\n\nlikelihood = function(p) {\n  return(dbinom(x = num_success, size = 10, prob = p))\n}\n\nggplot() +\nxlim(0, 1) +\n  geom_function(fun = likelihood) +\n  theme_bw() + \n  labs(x = \"p\", y = \"Likelihood\",\n       title = \"Likelihood of 7/10 coin flips landing heads as a function of p\") \n\n\n\n\nIn the example above, \\(p\\) is the parameter we are interested in, so we would write \\(L(p | x)\\). If we maximize \\(L(p | x)\\), we obtain the maximum likelihood estimate \\(\\hat{p}\\). The maximum likelihood estimate is the value of the parameter that maximizes the likelihood function."
  },
  {
    "objectID": "ae/ae14.html#example-2",
    "href": "ae/ae14.html#example-2",
    "title": "Likelihoods",
    "section": "Example 2",
    "text": "Example 2\nLikelihood-based inference works with continuous random variables as well.\n\\[\nX \\sim N(\\mu, \\sigma)\n\\] where \\(\\mu\\) is the mean (location) and \\(\\sigma\\) is the standard deviation (scale). \\(\\mu\\) and \\(\\sigma\\) are the parameters of the distribution.\n\nExercise 2\nImagine resting heart rates in this class are normally distributed with unknown mean \\(\\mu\\) and a standard deviation of 5 beats per minute. You randomly sample 1 person in the class and find their heart rate is 72 beats per minute. What is your best guess for the mean, \\(\\hat{\\mu}\\)?\n[answer here]\nYou randomly sample two additional people and find their resting heart rate is 65 and 75 beats per minute. What is your new best guess \\(\\hat{\\mu}\\) given these three data points?\n[answer here]\nUse geom_vline() to add your guesses to the plots below.\n\nlikelihoodNormal1 = function(mu) {\n  return(dnorm(x = 72 , mean = mu, sd = 5))\n}\n\nlikelihoodNormal2 = function(mu) {\n  return(dnorm(x = 65 , mean = mu, sd = 5))\n}\n\nlikelihoodNormal3 = function(mu) {\n  return(dnorm(x = 75 , mean = mu, sd = 5))\n}\n\n\n\nggplot() +\nxlim(50, 90) +\n  geom_function(fun = likelihoodNormal1) +\n  theme_bw() +\n  labs(x = \"Mu\",\n       y = \"Likelihood\",\n       title = \"Likelihood of mu given 1 data point\")\n\n\n\n\nIt’s a similar story for each of the other two data points. If we want to find the likelihood of some parameter under three independent observations, we can multiply our likelihoods together.\n\ncombinedLikelihood = function(mu) {\n  return(likelihoodNormal1(mu) * \n           likelihoodNormal2(mu) * \n           likelihoodNormal3(mu))\n}\n\nggplot() +\nxlim(50, 90) +\n  geom_function(fun = combinedLikelihood) +\n  theme_bw() +\n  labs(x = \"Mu\",\n       y = \"Likelihood\",\n       title = \"Likelihood of mu given 3 data points\")\n\n\n\n\nWhat do you notice about the shape of the function as the number of data points increases?\n\n\n\n\n\n\nNote\n\n\n\nAs the number of data points increases, the product of several likelihood functions gets very small very quickly. For this reason we often work with the log-likelihood. It is a fact that if \\(x &lt; y\\) then \\(\\log(x) &lt; \\log(y)\\). This property is known as monotonicity. Because of this, the maximum of the log-likelihood will be the same as the maximum of the likelihood function.\n\n\n\n\nExample 3\nIn all of the examples above, the findings were obvious upon inspection. How do you compute the likelihood in, e.g. a regression setting? Remember that \\(\\text{AIC} = 2k - 2 \\log \\text{likelihood}\\).\nA simple model:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\nA common assumption:\n\\[\n\\epsilon \\sim N(0, \\sigma)\n\\]\nFrom this assumption:\n\\[\ny \\sim N(\\beta_0 + \\beta_1x_1, \\sigma)\n\\]\n\nExercise 3\nAssume \\(\\sigma = 2\\) is known and \\(\\beta_0 = 1\\). You observe the data point, \\((x,y) = (1, 6.5)\\). What is your best estimate \\(\\hat{\\beta_1}\\)?\n\nregLikelihood = function(beta1) {\n  return(dnorm(x = 6.5, mean = (1 + (beta1*1)), sd = 2))\n}\n\nggplot() +\nxlim(-1, 12) +\n  geom_function(fun = regLikelihood) +\n  theme_bw() +\n  labs(x = \"beta1\",\n       y = \"Likelihood\",\n       title = \"Likelihood of beta1 given 1 data points\")\n\n\n\n\nIn the code above, why do we write x = 6.5 when \\(6.5\\) is the observed value of \\(y\\)?\n[answer here]\nYou observe two more data points, \\((2, 10)\\) and \\((5,26)\\). What is your best guess of \\(\\hat{\\beta_1}\\) based on the three points you observed?\n\nregLikelihood2 = function(beta1) {\n  return(dnorm(x = 10, mean = (1 + (beta1*2)), sd = 2))\n}\n\nregLikelihood3 = function(beta1) {\n  return(dnorm(x = 26, mean = (1 + (beta1*5)), sd = 2))\n}\n\ncombinedRegLikelihood = function(beta1){ \n  return(log(regLikelihood(beta1)) + log(regLikelihood2(beta1)) +\n           log(regLikelihood3(beta1)))\n  }\n\n\nggplot() +\nxlim(-2, 12) +\n  geom_function(fun = combinedRegLikelihood) +\n  theme_bw() +\n  labs(x = \"beta1\",\n       y = \"log-likelihood\",\n       title = \"Likelihood of beta1 given 3 data points\")"
  },
  {
    "objectID": "ae/ae21.html",
    "href": "ae/ae21.html",
    "title": "Ethics in Statistics and Data Science",
    "section": "",
    "text": "Lab 09\nThis Friday is last lab before peer-review in two weeks"
  },
  {
    "objectID": "ae/ae21.html#bulletin",
    "href": "ae/ae21.html#bulletin",
    "title": "Ethics in Statistics and Data Science",
    "section": "",
    "text": "Lab 09\nThis Friday is last lab before peer-review in two weeks"
  },
  {
    "objectID": "ae/ae21.html#today",
    "href": "ae/ae21.html#today",
    "title": "Ethics in Statistics and Data Science",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ncritically examine graphics, models and results\ndiscuss data privacy and redundancy\nanalyze a real data example of Simpson’s paradox"
  },
  {
    "objectID": "ae/ae21.html#getting-started",
    "href": "ae/ae21.html#getting-started",
    "title": "Ethics in Statistics and Data Science",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae21.qmd\",\ndestfile = \"ae21.qmd\")"
  },
  {
    "objectID": "ae/ae21.html#load-packages",
    "href": "ae/ae21.html#load-packages",
    "title": "Ethics in Statistics and Data Science",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nGuidelines for Discussion\n\nListen respectfully. Listen actively and with an ear to understanding others’ views.\nCriticize ideas, not individuals.\nCommit to learning, not debating. Comment in order to share information, not to persuade.\nAvoid blame, speculation, and inflammatory language.\nAvoid assumptions about any member of the class or generalizations about social groups."
  },
  {
    "objectID": "ae/ae21.html#data-representation",
    "href": "ae/ae21.html#data-representation",
    "title": "Ethics in Statistics and Data Science",
    "section": "Data Representation",
    "text": "Data Representation\n\nMisleading Data Visualizations1\nBrexit\n\n\n\nBrexit\n\n\n\nWhat is the graph trying to show?\nWhy is this graph misleading?\nHow can you improve this graph?\n\nSpurious Correlations2\n\n\n\nA Spurious Correlation\n\n\n\nWhat is the graph trying to show?\nWhy is this graph misleading?"
  },
  {
    "objectID": "ae/ae21.html#statistical-modeling",
    "href": "ae/ae21.html#statistical-modeling",
    "title": "Ethics in Statistics and Data Science",
    "section": "Statistical modeling",
    "text": "Statistical modeling\nRead, with a critical eye, page 2 and table 1 from Physician–patient racial concordance and disparities inbirthing mortality for newborns and chat with your neighbor."
  },
  {
    "objectID": "ae/ae21.html#data-privacy",
    "href": "ae/ae21.html#data-privacy",
    "title": "Ethics in Statistics and Data Science",
    "section": "Data privacy",
    "text": "Data privacy\n\nWeb scraping3\nA data analyst received permission to post a data set that was scraped from a social media site. The full data set included name, screen name, email address, geographic location, IP (Internet protocol) address, demographic profiles, and preferences for relationships. The analyst removes name and email address from the data set in effort to deidentify it.\n\nWhy might it be problematic to post this data set publicly?\nHow can you store the full dataset in a safe and ethical way?\nYou want to make the data available so your analysis is transparent and reproducible. How can you modify the full data set to make the data available in an ethical way?\n\n\n\nRedundancy\n\nslides\n\n\n\nAdditional readings\n\nWhy pokemon go’s plan to 3d scan the world is dangerous\nHow companies learn your secrets\n\n\n\nDiscussion questions\n\n“Simpson’s paradox”, where conclusions drawn from analyzing subgroups differ from conclusions drawn when the groups are combined. Can you demonstrate Simpson’s Paradox with the data below? 4\nFor further reading see Bickel, Peter J., Eugene A. Hammel, and J. William O’Connell. “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation.” Science 187.4175 (1975): 398-404.\n\n\nberk = read_csv(\"https://sta101.github.io/static/appex/data/BerkeleyAdmissionsData.csv\")\n\nRows: 7 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Dept\ndbl (4): MaleYes, MaleNo, FemaleYes, FemaleNo\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nberk\n\n# A tibble: 7 × 5\n  Dept  MaleYes MaleNo FemaleYes FemaleNo\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 A         512    313        89       19\n2 B         313    207        17        8\n3 C         120    205       202      391\n4 D         138    279       131      244\n5 E          53    138        94      299\n6 F          22    351        24      317\n7 All      1158   1493       557     1278"
  },
  {
    "objectID": "ae/ae24.html",
    "href": "ae/ae24.html",
    "title": "Forensic genetic analysis",
    "section": "",
    "text": "Wednesday office hours moved to Thursday after class\nLab 09 due Thursday. Draft peer-report due Friday (in lab review)\ncourse evaluations open. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project"
  },
  {
    "objectID": "ae/ae24.html#bulletin",
    "href": "ae/ae24.html#bulletin",
    "title": "Forensic genetic analysis",
    "section": "",
    "text": "Wednesday office hours moved to Thursday after class\nLab 09 due Thursday. Draft peer-report due Friday (in lab review)\ncourse evaluations open. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project"
  },
  {
    "objectID": "ae/ae24.html#getting-started",
    "href": "ae/ae24.html#getting-started",
    "title": "Forensic genetic analysis",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae24.qmd\",\ndestfile = \"ae24.qmd\")"
  },
  {
    "objectID": "ae/ae24.html#load-packages",
    "href": "ae/ae24.html#load-packages",
    "title": "Forensic genetic analysis",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "ae/ae24.html#background",
    "href": "ae/ae24.html#background",
    "title": "Forensic genetic analysis",
    "section": "Background",
    "text": "Background\nDNA evidence is sometimes used in court. In this AE, you will learn about the case of Dr. Schmidt from Lafayette, Louisiana, who was accused of infecting his former lover with HIV through a contaminated blood sample of one of his patients. Read more about this court case here and here."
  },
  {
    "objectID": "ae/ae24.html#today",
    "href": "ae/ae24.html#today",
    "title": "Forensic genetic analysis",
    "section": "Today",
    "text": "Today\nBy the end of today you will\n\nbe able to critical think about the use of DNA evidence and statistics in court\nanalyze non-numerical data rigorously"
  },
  {
    "objectID": "ae/ae24.html#explore-the-data",
    "href": "ae/ae24.html#explore-the-data",
    "title": "Forensic genetic analysis",
    "section": "Explore the data",
    "text": "Explore the data\n\ndf_HIV = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/HIV.csv\")\n\ndf_HIV contains several observations of HIV genomes. Within this data set are two samples with special ids: sample1 and sample2.\nFor the purpose of this exercise, you might imagine sample1 is associated with the HIV sampled from the plaintiff while sample2 belongs to that of the defendant’s patient.\nFundamentally, we are interested in whether or not sample1 and sample2 are closely related.\n\nExercise 1\n\nhow many observations are present in the data set?\nwhat are the observational units?\nhow many bases does the first DNA sequence contain? Hint: use the R function nchar."
  },
  {
    "objectID": "ae/ae24.html#extracting-a-sub-sequence-of-dna",
    "href": "ae/ae24.html#extracting-a-sub-sequence-of-dna",
    "title": "Forensic genetic analysis",
    "section": "Extracting a sub-sequence of DNA",
    "text": "Extracting a sub-sequence of DNA\nFor computational speed, we will have to work with shorter sub-sequences of DNA.\nThe function str_sub from the package stringr in the Tidyverse can be used to extract a sub-string from a character vector.\n\nExercise 2\n\nhow many arguments does the function str_sub take? What does each argument do?\nuse str_sub to extract (i) the first four letters of the words statistics, (ii) the sub-string between the third and the seventh letters, and (iii) the last four letters\n\n\nterm = \"statistics\"\n\nLet’s use str_sub to extract the first 500 bases from the DNA sequences using the template below.\n\nHIV = df_HIV %&gt;% \n  mutate(dna_short = ___) %&gt;%\n  select(-dna)\n\n\nuse nchar to verify that each sub-sequence dna_short contains 500 bases. Hint: nchar is vectorized, meaning if given a vector input, it will return a vector output."
  },
  {
    "objectID": "ae/ae24.html#computing-the-pairwise-distances-between-the-dna-sequence",
    "href": "ae/ae24.html#computing-the-pairwise-distances-between-the-dna-sequence",
    "title": "Forensic genetic analysis",
    "section": "Computing the pairwise distances between the DNA sequence",
    "text": "Computing the pairwise distances between the DNA sequence\nNow that the data have been prepared, we will establish how similar/different each DNA sequence is to the others. To accomplish this, given two DNA sequences we will count the number of bases for which they differ. The rationale for this step is the following. If two DNA differ on many bases, it means that they have evolved separately for a while and had the time to undergo numerous mutations. On the other hand, if they only differ on a few bases, it means that the two sequences have only recently began to evolve separately.\nThe following code creates a data frame where each row corresponds to a pair of DNA sequences.\n\nd_pairs &lt;- combn(HIV$person_id, 2) %&gt;%\n  t() %&gt;% # go from wide matrix (2 rows) to long matrix (2 columns)\n  as_tibble() %&gt;%\n  rename(id1 = V1, id2 = V2) %&gt;%\n  left_join(HIV, by = c(\"id1\" = \"person_id\")) %&gt;% # add dna for person 1\n  rename(dna1 = dna_short) %&gt;%\n  left_join(HIV, by = c(\"id2\" = \"person_id\")) %&gt;% # add dna for person 1\n  rename(dna2 = dna_short)\n\nQuestion: What are the dimensions of d_pairs?\n\nHamming distance\nTo measure the distance between two sequences, we first consider the Hamming distance\n\\[\nd(i,j) = \\sum_{k=1}^n 1\\{\\text{dna}_{ik} \\neq \\text{dna}_{jk}\\}\n\\]\nwhich counts the number of elements that are different between two sequences. Here \\(d(i,j)\\) denotes the Hamming distance between sequences \\(i\\) and \\(j\\), and \\(1\\{\\text{dna}_{ik} \\neq \\text{dna}_{jk}\\}\\) is equal to \\(1\\) is the \\(k\\)-th element of sequence \\(i\\) is different from that of sequence \\(j\\).\nThe following code computes the Hamming distance between each pair of sequences. We first construct function compute_hamming which computes the Hamming distance between two DNA sequences. We then apply this function to each row of the d_pairs data frame.\n\ncompute_hamming &lt;- function(dna1, dna2) {\n  \n  dna1_split &lt;- str_split(dna1, pattern = \"\", simplify = TRUE)\n  dna2_split &lt;- str_split(dna2, pattern = \"\", simplify = TRUE)\n  \n  hamming_distance &lt;- sum(dna1_split != dna2_split)\n  return(hamming_distance)\n}\n\nd_hamming &lt;- d_pairs %&gt;%\n  mutate(\n    distance_ham = list(dna1, dna2) %&gt;% pmap_dbl(compute_hamming)\n  )\n\n\nExercise 3\n\nMake a histogram of the Hamming distances and describe the distribution.\n\n\nFind the 10 pairs of DNA sequences that are the closest to the plaintiff’s sequence in terms of Hamming distance.\n\n\nWhich sequence is most closely related to the plaintiff’s sequence?\nHow many differences are there between the DNA sequence of the plaintiff and that of the defendant’s patient?\nCan you identify a shortcoming of the Hamming distance? Does a large Hamming distance necessarily imply that the two DNA sequences are very different?\n\n\n\n\nAn alternative measure of distance for DNA sequences\nLet us now consider an alternative measure of distance for DNA sequences.\n\nd_biology &lt;- d_pairs %&gt;%\n  mutate(\n    distance_bio = list(dna1, dna2) %&gt;% pmap_dbl(adist)\n    )\n\n\nExercise 4\n\nAgain, make a histogram of the new distance and find the 10 DNA sequences closest to the plaintiff’s.\n\n\nWhich of the two measures do you find more adequate for these data?\n\n\n\nExercise 5\n\nImagine that you are a juror in this court case, would you find this piece of evidence convincing? Would you like to have more information? If so, what additional information would you need? Now, imagine that you are a judge; do you think that this piece of evidence should be admitted to court? Why?"
  },
  {
    "objectID": "ae/ae2.html",
    "href": "ae/ae2.html",
    "title": "Intro to R and EDA",
    "section": "",
    "text": "first lab tomorrow, due following Thursday at 11:59pm on Gradescope\nbe sure to complete prepare material (on the schedule) before each class"
  },
  {
    "objectID": "ae/ae2.html#bulletin",
    "href": "ae/ae2.html#bulletin",
    "title": "Intro to R and EDA",
    "section": "",
    "text": "first lab tomorrow, due following Thursday at 11:59pm on Gradescope\nbe sure to complete prepare material (on the schedule) before each class"
  },
  {
    "objectID": "ae/ae2.html#today",
    "href": "ae/ae2.html#today",
    "title": "Intro to R and EDA",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\npractice using glimpse(), names(), nrow(), ncol(), count()\ndefine and compute various statistics\nbegin to gain familiarity with ggplot"
  },
  {
    "objectID": "ae/ae2.html#getting-started",
    "href": "ae/ae2.html#getting-started",
    "title": "Intro to R and EDA",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae2.qmd\",\ndestfile = \"ae2.qmd\")\n\nA note on navigating RStudio\n\nSource code vs visual editor\ncode chunks and narrative\nfile system"
  },
  {
    "objectID": "ae/ae2.html#load-packages",
    "href": "ae/ae2.html#load-packages",
    "title": "Intro to R and EDA",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\n\n\nA package within a package…\n\nThe tidyverse package contains the dplyr and ggplot packages we will use today. ggplot contains functions for plotting data while dplyr contains tools to wrangle, manipulate and summarize data frames."
  },
  {
    "objectID": "ae/ae2.html#load-data",
    "href": "ae/ae2.html#load-data",
    "title": "Intro to R and EDA",
    "section": "Load data",
    "text": "Load data\n\nflint = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/flint.csv\")\n\n\nData dictionary\n\nid: sample ID number (identifies the home)\nzip: ZIP code in Flint of the sample’s location\nward: ward in Flint of the sample’s location\ndraw: which time point the water was sampled from\nlead: lead content in parts per billion"
  },
  {
    "objectID": "ae/ae2.html#goal",
    "href": "ae/ae2.html#goal",
    "title": "Intro to R and EDA",
    "section": "Goal",
    "text": "Goal\nWe want to learn about the population using a sample.\nIn the case we want to learn about the lead content in all of Flint, MI homes but only have available water readings from a sample of homes (our data set).\n\nExercise 1:\nLook at the data, how many observations are there? How many variables?\n[answer here]"
  },
  {
    "objectID": "ae/ae2.html#count",
    "href": "ae/ae2.html#count",
    "title": "Intro to R and EDA",
    "section": "Count",
    "text": "Count\nLet’s count() to find the number of different time points water was sampled.\n\ncount(flint, draw)\n\n# A tibble: 3 × 2\n  draw       n\n  &lt;chr&gt;  &lt;int&gt;\n1 first    271\n2 second   271\n3 third    271\n\n\nHow many unique homes are in the data set?\n\nflint %&gt;%\n  count(id)\n\n# A tibble: 269 × 2\n      id     n\n   &lt;dbl&gt; &lt;int&gt;\n 1     1     3\n 2     2     3\n 3     4     3\n 4     5     3\n 5     6     3\n 6     7     3\n 7     8     3\n 8     9     3\n 9    12     3\n10    13     3\n# ℹ 259 more rows\n\n\n\nA note on pipes %&gt;%"
  },
  {
    "objectID": "ae/ae2.html#exercise-2",
    "href": "ae/ae2.html#exercise-2",
    "title": "Intro to R and EDA",
    "section": "Exercise 2",
    "text": "Exercise 2\nFill in the code to see how many samples were taken from each zip code. Uncomment the lines (i.e. remove the # before running the code)\n\n# flint %&gt;% \n # count(______)\n\nWhich ZIP code had the most samples drawn?"
  },
  {
    "objectID": "ae/ae2.html#statistics",
    "href": "ae/ae2.html#statistics",
    "title": "Intro to R and EDA",
    "section": "Statistics",
    "text": "Statistics\nWhat is a statistic? It’s any mathematical function of the data. Sometimes, a statistic is referred to as “sample statistic” since you compute it from a sample (the data) and not the entire population.\n\nmeasure of central tendency:\n\nmean\nmedian\nmode\n\n\n\nmeasures of spread:\n\nvariance\nstandard deviation\nrange\nquartiles\ninter-quartile range (IQR)\n\n\n\norder statistics:\n\nquantiles\nminimum (0 percentile)\nmedian (50th percentile)\nmaximum (100 percentile)\n\n… and any other arbitrary function of the data you can come up with!\n\n\nExercise 3:\nCome up with your own statistic and write it in the narrative here.\nTo access a column of the data, we’ll use data$column.\nLet’s compute each of these statistics for lead ppb in R.\n\n# code here"
  },
  {
    "objectID": "ae/ae2.html#plotting",
    "href": "ae/ae2.html#plotting",
    "title": "Intro to R and EDA",
    "section": "Plotting",
    "text": "Plotting\nLet’s take a look at the distribution of lead content in homes in Flint, MI.\n\nflint %&gt;% # data\n  ggplot(aes(x = lead)) + # columns we want to look at\n  geom_histogram() # geometry of the visualization\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can make this plot look nicer by adjusting the number of bins and/or the x-axis.\n\nflint %&gt;% # data\n  ggplot(aes(x = lead)) + # columns we want to look at\n  geom_histogram(bins = 50) + # geometry of the visualization\n  xlim(0, 100) # limit the x-axis to a certain range\n\n\n\n\nLet’s visualize some of our summary statistics on the plot.\n\nExercise 4:\nUn-comment the code below and fill in the blank with the mean.\n\nflint %&gt;% \n  ggplot(aes(x = lead)) + \n  geom_histogram(bins = 50) + \n  xlim(0,100) #+\n  #geom_vline(xintercept = __, color = 'red')\n\nAdd another geom_vline with the median. Use a separate color."
  },
  {
    "objectID": "ae/ae2.html#box-plots",
    "href": "ae/ae2.html#box-plots",
    "title": "Intro to R and EDA",
    "section": "Box plots",
    "text": "Box plots\nLet’s make some plots, where we will focus on zip codes 48503, 48504, 48505, 48506, and 48507. We will restrict our attention to samples with lead values less than 1,000 ppb.\n\nflint_focus &lt;- flint %&gt;% \n  filter(zip %in% 48503:48507, lead &lt; 1000)\n\nBelow are side-by-side box plots for the three flushing times in each of the five zip codes we considered. Add x and y labels; add a title by inserting title = \"title_name\" inside the labs() function.\n\nggplot(data = flint_focus, aes(x = factor(zip), y = lead)) +\n  geom_boxplot(aes(fill = factor(draw))) +\n  labs(x = \"--------\", y = \"--------\", fill = \"Flushing time\") +\n  scale_fill_discrete(breaks = c(\"first\", \"second\", \"third\"),\n                      labels = c(\"0 (sec)\", \"45 (sec)\", \"120 (sec)\")) +\n  coord_flip() +\n  theme_bw()\n\n\n\n\nAdd labels for x, y, a title, and subtitle to the code below to update the corresponding plot.\n\nggplot(data = flint_focus, aes(x = factor(zip), y = lead)) +\n  geom_boxplot(aes(fill = factor(draw))) + \n  labs(x = \"--------\", y = \"--------\", fill = \"Flushing time\",\n       subtitle = \"--------\") +\n  scale_fill_discrete(breaks = c(\"first\", \"second\", \"third\"),\n                      labels = c(\"0 (sec)\", \"45 (sec)\", \"120 (sec)\")) +\n  coord_flip(ylim = c(0, 50)) +\n  theme_bw()\n\n\n\n\nWhat is the difference between the two plots? What are the advantages and disadvantages to each plot?"
  },
  {
    "objectID": "ae/ae2.html#references",
    "href": "ae/ae2.html#references",
    "title": "Intro to R and EDA",
    "section": "References",
    "text": "References\n\nLangkjaer-Bain, R. (2017). The murky tale of Flint’s deceptive water data. Significance, 14: 16-21.\nKelsey J. Pieper, Rebekah Martin, Min Tang, LeeAnne Walters, Jeffrey Parks, Siddhartha Roy, Christina Devine, and Marc A. Edwards Environmental Science & Technology 2018 52 (15), 8124-8132 DOI: 10.1021/acs.est.8b00791"
  },
  {
    "objectID": "ae/ae26.html",
    "href": "ae/ae26.html",
    "title": "Correlation and Covariance",
    "section": "",
    "text": "Final project due date updated (see announcement)\nCourse evaluations. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project.\nIf \\(&gt;80\\%\\) TA evals, another 0.5 points."
  },
  {
    "objectID": "ae/ae26.html#bulletin",
    "href": "ae/ae26.html#bulletin",
    "title": "Correlation and Covariance",
    "section": "",
    "text": "Final project due date updated (see announcement)\nCourse evaluations. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project.\nIf \\(&gt;80\\%\\) TA evals, another 0.5 points."
  },
  {
    "objectID": "ae/ae26.html#getting-started",
    "href": "ae/ae26.html#getting-started",
    "title": "Correlation and Covariance",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae26.qmd\",\ndestfile = \"ae26.qmd\")"
  },
  {
    "objectID": "ae/ae26.html#load-packages",
    "href": "ae/ae26.html#load-packages",
    "title": "Correlation and Covariance",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ellipse)"
  },
  {
    "objectID": "ae/ae26.html#covariance-and-correlation",
    "href": "ae/ae26.html#covariance-and-correlation",
    "title": "Correlation and Covariance",
    "section": "Covariance and correlation",
    "text": "Covariance and correlation\nEmpirical covariance between two variables\n\\[\nCov(X,Y) = \\frac{1}{n-1} \\sum_{i = 1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n\\] Later in this application exercise we write \\(\\sigma_{xy}^2\\) as the covariance between x and y.\nEmpirical correlation between two variables,\n\\[\np_{XY} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\n\\] where \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviation of X and Y respectively.\n\nExample\n\nlibrary(tidyverse)\nx = c(1, 2, 3, 4, 5)\ny = c(0.5, 3, 2.2, 5, 5.5)\n\ndf = data.frame(x, y)\nfit1 = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x, data = df)\nr2 = glance(fit1)$r.squared\n\ncat(\"Correlation between x and y: \", cor(x,y),\n    \"\\nCovariance between x and y: \", cov(x,y),\n    \"\\nStandard deviation of x: \", sd(x),\n    \"\\nStandard deviation of y: \", sd(y),\n    \"\\nR squared: \", r2\n    )\n\nCorrelation between x and y:  0.9243906 \nCovariance between x and y:  3 \nStandard deviation of x:  1.581139 \nStandard deviation of y:  2.052559 \nR squared:  0.854498\n\n\n\n\nGuess that correlation!"
  },
  {
    "objectID": "ae/ae26.html#data",
    "href": "ae/ae26.html#data",
    "title": "Correlation and Covariance",
    "section": "Data",
    "text": "Data\nHawks is a subset of a data set by the same name in the Stat2Data package. Today we will focus on the following measurements of 891 hawks:\n\nSpecies: CH = cooper’s, RT = red-tailed, SS = sharp-shinned\nWeight: body weight in grams\nWing: length in mm of primary wing feather from tip to wrist it attaches to\nCulmen: length in mm of the upper bill from the tip to where it bumps into the fleshy part of the bird\nHallux: length in mm of the killing talon\n\n\nHawks = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/Hawks.csv\")\n\n\nglimpse(Hawks)\n\nRows: 891\nColumns: 5\n$ Species &lt;chr&gt; \"RT\", \"RT\", \"CH\", \"SS\", \"RT\", \"RT\", \"RT\", \"RT\", \"RT\", \"RT\", \"R…\n$ Weight  &lt;dbl&gt; 920, 990, 470, 170, 1090, 960, 855, 1210, 1120, 1010, 1010, 11…\n$ Wing    &lt;dbl&gt; 385, 381, 265, 205, 412, 370, 375, 412, 405, 393, 371, 390, 41…\n$ Culmen  &lt;dbl&gt; 25.7, 26.7, 18.7, 12.5, 28.5, 25.3, 27.2, 29.3, 26.0, 26.3, 25…\n$ Hallux  &lt;dbl&gt; 30.1, 31.3, 23.5, 14.3, 32.2, 30.1, 30.0, 31.3, 30.2, 30.8, 29…"
  },
  {
    "objectID": "ae/ae26.html#examples",
    "href": "ae/ae26.html#examples",
    "title": "Correlation and Covariance",
    "section": "Examples",
    "text": "Examples\n\nTwo measurements\nLet’s look at weight and wing length.\n\nHawks %&gt;%\n  ggplot(aes(x = Weight, y = Wing)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n# Standardize data\nHawks2 = Hawks %&gt;%\n  mutate(sWeight = (Weight - mean(Weight)) / sd(Weight),\n         sWing = (Wing - mean(Wing)) / sd(Wing))\n\nHawks2 %&gt;%\n  ggplot(aes(x = sWeight, y = sWing)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\nHow can we describe the variability of the data?\n\n# Covariance matrix\ncovMatrix1 = Hawks %&gt;%\n  select(Weight, Wing) %&gt;%\n  cov()\n\ncovMatrix1\n\n          Weight      Wing\nWeight 214310.57 41247.975\nWing    41247.97  9085.273\n\ncovMatrix2 = Hawks2 %&gt;%\n  select(sWeight, sWing) %&gt;%\n  cov()\n\ncovMatrix2\n\n          sWeight     sWing\nsWeight 1.0000000 0.9347852\nsWing   0.9347852 1.0000000\n\n\nCovariance matrix \\(\\Sigma\\) collects variances and covariances together,\n\\[\n\\Sigma = \\begin{pmatrix}\\sigma_x^2 & \\sigma_{xy}^2\\\\\\ \\sigma_{xy}^2 & \\sigma_y^2\\end{pmatrix}\n\\] How can we visualize the covariance matrix above?\n\noffline example of matrix multiplication\n\nThe “matrix inverse” helps. The inverse of \\(\\Sigma\\) is denoted \\(\\Sigma^{-1}\\). The property of the inverse is:\n\\[\n\\Sigma^{-1} \\Sigma = \\begin{pmatrix}1 & 0\\\\\\ 0 & 1 \\end{pmatrix}\n\\]\n\\[\nz^T \\Sigma^{-1} z = c^2\n\\]\nwhere \\(z = (x, y)\\) and \\(\\Sigma^{-1} = \\begin{pmatrix}s_x^2 & s_{xy}^2\\\\\\ s_{xy}^2 & s_y^2\\end{pmatrix}\\). Where have we seen this before? Hint: ?pnorm or see multivariate normal\n\\[\n\\begin{pmatrix} x & y\\end{pmatrix}\n\\begin{pmatrix}s_x^2 & s_{xy}^2\\\\\\ s_{xy}^2 & s_y^2\\end{pmatrix}\n\\begin{pmatrix} x\\\\\\ y \\end{pmatrix}\n= c^2\n\\]\n\\[\n(x s_x^2 + y s_{xy}^2 \\ \\ \\ \\ x s_{xy}^2 + y s_y^2) \\begin{pmatrix} x\\\\\\ y \\end{pmatrix} = c^2\n\\]\n\\[\nx^2 s_x^2 + 2x y \\cdot s_{xy}^2 + y^2 s_y^2 = c^2\n\\] This is the equation of an ellipse.\n\n# Grab the points (x,y) that satisfy the equation above\nellipsePoints = data.frame(ellipse(covMatrix2))\n\nHawks2 %&gt;%\n  ggplot(aes(x = sWeight, y = sWing, color = Species)) +\n  geom_point() +\n  theme_bw() +\n  geom_point(aes(x = sWeight, y = sWing), data = ellipsePoints, color = 'steelblue')\n\n\n\n\nSet \\(c^2 = 6\\):\nTo make sure the function ellipse above is doing what we expect:\n\nFirst we get \\(\\Sigma^{-1}\\):\n\n\nsolve(covMatrix2)\n\n          sWeight     sWing\nsWeight  7.925401 -7.408548\nsWing   -7.408548  7.925401\n\n\nNext, we manually solve the quadratic equation using the quadratic formula:\n\ngetCoordinate = function(y, s1, s2, s12) {\n  A = s1\n  B = 2*y*s12\n  C = (y*s2) - 6\n  p1 = -1*B\n  p2 = sqrt(B^2 - (4*A*C))\n  p3 = 2*A\n  \n  coord1 = (p1 + p2) / p3\n  coord2 = (p1 - p2) / p3\n  return(c(coord1, coord2))\n}\n\ngetCoordinate(1, 7.925401, 7.925401, -7.408548)\n\n[1] 1.7290667 0.1405038\n\n\nThe axes of the ellipse provide the most informative directions to measure the data. In \\(n\\)-dimensions, where we have a \\(n\\)-dimensional ellipsoid, it can be useful to look at \\(p&lt;n\\) axes. The largest \\(p\\) axes are called the “principal components”.\n\nExample of “principle component analysis” in action: genes mirror geography within Europe and an associated news article discussing the work."
  },
  {
    "objectID": "ae/ae1.html",
    "href": "ae/ae1.html",
    "title": "Welcome to R",
    "section": "",
    "text": "By the end of today you will…\n\nbegin to know your way around RStudio\nbe able to define package, data frame, variable, function, argument\nuse the function glimpse()"
  },
  {
    "objectID": "ae/ae1.html#today",
    "href": "ae/ae1.html#today",
    "title": "Welcome to R",
    "section": "",
    "text": "By the end of today you will…\n\nbegin to know your way around RStudio\nbe able to define package, data frame, variable, function, argument\nuse the function glimpse()"
  },
  {
    "objectID": "ae/ae1.html#getting-started",
    "href": "ae/ae1.html#getting-started",
    "title": "Welcome to R",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae1.qmd\",\ndestfile = \"ae1.qmd\")"
  },
  {
    "objectID": "ae/ae1.html#r-as-a-calculator",
    "href": "ae/ae1.html#r-as-a-calculator",
    "title": "Welcome to R",
    "section": "R as a calculator",
    "text": "R as a calculator\n\nUse R as a calculator by typing the following into the console:\n\n\n5 * 5 + 10\n\nx = 3\nx + x^2\n\nx = 1:10\nx * 7\nIn the last couple examples we save some value as the object “x”.\nWe can “print” x to the screen by typing the name of the object (“x”) in the console or in a code chunk."
  },
  {
    "objectID": "ae/ae1.html#tour-of-rstudio",
    "href": "ae/ae1.html#tour-of-rstudio",
    "title": "Welcome to R",
    "section": "Tour of RStudio",
    "text": "Tour of RStudio\n\nenvironment\nR functions\nloading and viewing a data frame"
  },
  {
    "objectID": "ae/ae1.html#load-a-package",
    "href": "ae/ae1.html#load-a-package",
    "title": "Welcome to R",
    "section": "Load a package",
    "text": "Load a package\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "ae/ae1.html#load-data",
    "href": "ae/ae1.html#load-data",
    "title": "Welcome to R",
    "section": "Load data",
    "text": "Load data\n\nroster = read_csv(\"https://sta101.github.io/static/appex/data/sample-roster.csv\")\nsurvey = read_csv(\"https://sta101.github.io/static/appex/data/sample-survey.csv\")\n\nQuestion: What objects store the data in the code chunk above? Can you print them to the screen?\nCreate a new code chunk with CMD+OPTION+I (mac) or CTRL+ALT+I (windows/linux)\nSo far we’ve already seen two functions. library and read_csv. Functions in R are attached to parentheses and take an input, aka an argument, and often (but not always) return an output. To learn more about a function, you can check the documentation with ?, e.g. ?library."
  },
  {
    "objectID": "ae/ae1.html#demos",
    "href": "ae/ae1.html#demos",
    "title": "Welcome to R",
    "section": "Demos",
    "text": "Demos\nLet’s glimpse the data frame.\n\nglimpse(survey)\n\nRows: 12\nColumns: 5\n$ name                 &lt;chr&gt; \"A\", \"Appa\", \"Bumi\", \"Soka\", \"Katara\", \"Suki\", \"Z…\n$ email                &lt;chr&gt; \"the-last-Rbender@duke.edu\", \"yip-yip-appa@duke.e…\n$ bender               &lt;chr&gt; \"Airbender\", \"Airbender\", \"Earthbender\", \"None\", …\n$ previous_programming &lt;chr&gt; \"No\", \"No\", \"No\", \"Somewhat\", \"Yes\", \"Yes\", \"Yes\"…\n$ cat_dog              &lt;chr&gt; \"dog\", \"cat\", \"cat\", \"dog\", \"dog\", \"cat\", \"cat\", …\n\n\nTo look at all of it, we can use view()\n\nview(survey)\n\nView the roster data in the console\nTerminology: “columns” of a dataframe are called variables whereas “rows” are observations.\nQuestion: How many variables are in the data frame survey? How many observations? What about the data frame roster?\nWhy must I input a specific email format?\n\nroster %&gt;% \n  left_join(survey, by = \"email\")"
  },
  {
    "objectID": "ae/ae12.html",
    "href": "ae/ae12.html",
    "title": "Probability I",
    "section": "",
    "text": "Lab 04 due Friday October 7\nRegression project due Friday October 14"
  },
  {
    "objectID": "ae/ae12.html#bulletin",
    "href": "ae/ae12.html#bulletin",
    "title": "Probability I",
    "section": "",
    "text": "Lab 04 due Friday October 7\nRegression project due Friday October 14"
  },
  {
    "objectID": "ae/ae12.html#today",
    "href": "ae/ae12.html#today",
    "title": "Probability I",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nbe able to define random variables, probability, and distributions\nbe able to estimate probabilites from data\nsimulate from a binomial distribution"
  },
  {
    "objectID": "ae/ae12.html#getting-started",
    "href": "ae/ae12.html#getting-started",
    "title": "Probability I",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae12.qmd\",\ndestfile = \"ae12.qmd\")"
  },
  {
    "objectID": "ae/ae12.html#load-packages-and-data",
    "href": "ae/ae12.html#load-packages-and-data",
    "title": "Probability I",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fivethirtyeight)\n\n\ndata(bob_ross)\nglimpse(bob_ross)\n\nRows: 403\nColumns: 71\n$ episode            &lt;chr&gt; \"S01E01\", \"S01E02\", \"S01E03\", \"S01E04\", \"S01E05\", \"…\n$ season             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, …\n$ episode_num        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 2, 3,…\n$ title              &lt;chr&gt; \"A WALK IN THE WOODS\", \"MT. MCKINLEY\", \"EBONY SUNSE…\n$ apple_frame        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ aurora_borealis    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ barn               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ beach              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ boat               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ bridge             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ building           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ bushes             &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, …\n$ cabin              &lt;int&gt; 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ cactus             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ circle_frame       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cirrus             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ cliff              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ clouds             &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, …\n$ conifer            &lt;int&gt; 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, …\n$ cumulus            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, …\n$ deciduous          &lt;int&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, …\n$ diane_andre        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dock               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ double_oval_frame  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ farm               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fence              &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fire               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ florida_frame      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ flowers            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fog                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ framed             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ grass              &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, …\n$ guest              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ half_circle_frame  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ half_oval_frame    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ hills              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lake               &lt;int&gt; 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, …\n$ lakes              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lighthouse         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ mill               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ moon               &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ mountain           &lt;int&gt; 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, …\n$ mountains          &lt;int&gt; 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, …\n$ night              &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ocean              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ oval_frame         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ palm_trees         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ path               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ person             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ portrait           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ rectangle_3d_frame &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ rectangular_frame  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ river              &lt;int&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ rocks              &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ seashell_frame     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ snow               &lt;int&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, …\n$ snowy_mountain     &lt;int&gt; 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, …\n$ split_frame        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ steve_ross         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ structure          &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ sun                &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, …\n$ tomb_frame         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ tree               &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ trees              &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ triple_frame       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ waterfall          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ waves              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ windmill           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ window_frame       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ winter             &lt;int&gt; 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ wood_framed        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "ae/ae12.html#re-cap-from-prepare",
    "href": "ae/ae12.html#re-cap-from-prepare",
    "title": "Probability I",
    "section": "Re-cap from prepare",
    "text": "Re-cap from prepare\n\nWarm-up\n\nThere are 403 episodes of Bob Ross. Assume exactly 1 painting is painted in each episode. Pretend that before each episode, Bob Ross flips a coin to decide whether or not to paint a sunrise. If it lands heads, a sunrise is in the painting and if it’s tails then he does not paint a sunrise. Let \\(X\\) be the number of episodes a sunrise is featured in the painting. What is the sample space of this random experiment?\nWhat is an event?\nLet A be the event \\(X&gt;10\\) and let B be the event \\(X&lt;3\\). Are A and B disjoint?\n\n\n\nProbability\nA probability is the long-run frequency of an event. In other words, the proportion of times we would see an event occur if we could repeat an experiment an infinite number of times. Probabilities take values between 0 and 1 inclusive.\n\n\n\n\n\n\nNote\n\n\n\nIf A and B are two disjoint events, then the probability of A or B occurring is equal to the probability of A plus the probability of B. More concisely, Pr(A or B) = Pr(A) + Pr(B).\n\n\n\nMore definitions\nLet A and B be two events.\n\nMarginal probability: The probability an event occurs regardless of values of the other event\n\nP(A)\nExample: What’s the probability that, in a randomly selected episode of Bob Ross, the painting features clouds?\n\nJoint probability: The probability two or more events simultaneously occur\n\nExample: What’s the probability that, in a randomly selected episode of Bob Ross, the painting features clouds and mountains?\nP(A and B)\n\nConditional probability: The probability an event occurs given the other has occurred\n\nP(A|B) or P(B|A)\nExample: What is the probability that a Bob Ross painting features clouds if it was randomly selected from season 1?\nP(A|B) = P(A and B) / P(B)\n\nIndependent events: Knowing one event has occurred does not lead to any change in the probability we assign to another event.\n\nP(A|B) = P(A) or P(B|A) = P(B)\nExample: The probability a painting features lakes is independent of whether or not it features rivers. More concisely, P(lakes | rivers) = P(lakes)\n\n\n\n\nExercise 1\nApproximate the probability of each example above using the bob_ross data set. For the last example (independence) validate or refute the claim using the data set.\n\n# code here"
  },
  {
    "objectID": "ae/ae12.html#data-generative-processes",
    "href": "ae/ae12.html#data-generative-processes",
    "title": "Probability I",
    "section": "Data generative processes",
    "text": "Data generative processes\nAs statisticians, we often want to model the process that generates data. For example, although Bob Ross probably does not flip a coin to decide what to paint, it might be a useful model for describing the data.\nTo formalize this concept, we will embrace two new concepts: random variables and distributions.\n\nRandom variables\nYou may not have realized it, but we’ve already seen random variables. A random variable is a function that maps an observed outcome to a number.\nFor example, when Bob Ross paints a tree and we label it “1” or does not paint a tree and we label it “0”, we are defining a random variable!\n\n\n\n\n\n\nNote\n\n\n\nRandom variables that only take the values 0 and 1 have a special name. They are called indicator random variables because they are thought of as indicating whether or not an event occurs.\n\n\nRandom variables have distributions…"
  },
  {
    "objectID": "ae/ae12.html#distributions",
    "href": "ae/ae12.html#distributions",
    "title": "Probability I",
    "section": "Distributions",
    "text": "Distributions\n\nBinomial distribution\nThe binomial distribution models the number of success in a series of independent and identical binary trials and is defined by two parameters:\n\n\\(k\\), the total number of trials,\n\\(p\\), the probability of a success in an individual trial.\n\nThe sample space of a binomial random variable is \\(\\{0, 1, \\ldots, k \\}\\). In words, there could be up to \\(k\\) success in an binomial experiment.\n\nExample\nYou toss a fair coin 10 times. Let A be the event there is at least one head. What is the probability of A?\nHere \\(k\\) is _ and \\(p\\) is _.\nrbinom() arguments:\n\nsize is the number of trials, aka the number of coin flips in 1 experiment\nprob is the probability of a success\nn defines how many times to repeat the entire experiment\n\n\nset.seed(100) # sets random seed to ensure we get same result\n\nN = 1000 # total number of experiments of 10 coin flips\n\ncoin_flips = data.frame(num_heads = rbinom(n = N, size = 10, prob = 0.5))\n\ncoin_flips %&gt;%\n  filter(num_heads &gt;=1) %&gt;%\n  nrow() / N\n\n[1] 0.998\n\n\n\ncoin_flips %&gt;%\n  ggplot(aes(x = num_heads)) +\n  geom_histogram(bins = 35) +\n  theme_bw() +\n  labs(x = \"Number of heads\",\n       y  = \"Count\",\n       title = \"Distribution of the total # heads in 10 fair coin flips\") +\n  scale_x_continuous(breaks = 0:10)\n\n\n\n\n\n\nExercise 2\nSuppose Bob Ross paintings feature a mountain with probability \\(0.7\\). (You might imagine that before Bob Ross paints, he flips an unfair coin that has a 70% chance of landing heads. If the coin lands on heads, he paints a mountain if it lands on tails he does not.) Given that there are 403 episodes of Bob Ross, what is the probability that at least 150 paintings feature a mountain?\nTo help you setup your simulation, set \\(N = 2000\\). What is \\(k\\)? What is \\(p\\)?\nTo ensure we get the same answer, use the seed provided below.\n\nset.seed(100)\n# code here\n\n\nAs a follow-up question, what is the probability that at least 100 paintings but not more than 200 feature a mountain?\n\n\n\n\nMathematics of the binomial distribution\n\\[\nX \\sim \\text{Binomial}(k, p)\n\\]\n“X has a binomial distribution with parameters k and p”\nWhat this means to us is:\n\nwe can simulate the distribution of X using the rbinom code from above\nwe can compute the probability X equals any specific value explicitly with the equation below\n\n\\[\nP(X = m) = {k \\choose m} (p)^{m}(1-p)^{k-m}\n\\] If we want to know, for example, the exact probability Bob Ross paints at least 150 paintings with a mountain, we would need to add up \\(P(X = 150) + P(X = 151) + P(X = 152) + \\ldots + P(X = 402) + P(X = 403)\\).\n\nExample\nWhat’s the probability that exactly 1 coin lands heads when we flip a fair coin 10 times?\n\n# P(X = m) \n\n# k choose m\n# k = 10, m = 1\n# fair coin means p = 0.5\n\nchoose(10, 1) * (0.5)^1 * (1 - 0.5)^(10 - 1)\n\n[1] 0.009765625\n\n\n\n\nExercise 3\nWhat’s the probability at least 1 coin lands heads? Use the equation above to compute. Hint: \\(P(X &gt;= 1) = 1 - P(X = 0)\\).\n\n# code here\n\n\n\nExercise 4 (before next class)\nPlay with the “random variables” lesson here. Describe what you observe."
  },
  {
    "objectID": "ae/ae17.html",
    "href": "ae/ae17.html",
    "title": "CLT and Confidence Intervals",
    "section": "",
    "text": "Lab 6 due tonight\nProject proposal due Friday (tomorrow)"
  },
  {
    "objectID": "ae/ae17.html#bulletin",
    "href": "ae/ae17.html#bulletin",
    "title": "CLT and Confidence Intervals",
    "section": "",
    "text": "Lab 6 due tonight\nProject proposal due Friday (tomorrow)"
  },
  {
    "objectID": "ae/ae17.html#today",
    "href": "ae/ae17.html#today",
    "title": "CLT and Confidence Intervals",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nUse CLT to construct confidence intervals"
  },
  {
    "objectID": "ae/ae17.html#getting-started",
    "href": "ae/ae17.html#getting-started",
    "title": "CLT and Confidence Intervals",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae17.qmd\",\ndestfile = \"ae17.qmd\")"
  },
  {
    "objectID": "ae/ae17.html#load-packages",
    "href": "ae/ae17.html#load-packages",
    "title": "CLT and Confidence Intervals",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae17.html#load-data-pokemon",
    "href": "ae/ae17.html#load-data-pokemon",
    "title": "CLT and Confidence Intervals",
    "section": "Load Data: Pokemon",
    "text": "Load Data: Pokemon\nWe will be using the pokemon data set, which contains information about 42 randomly selected Pokemon (from all generations). You may load in the data set with the following code:\n\npokemon = read_csv(\"https://sta101.github.io/static/appex/data/pokemon.csv\")\n\nIn this analysis, we will use CLT-based inference to draw conclusions about the mean height among all Pokemon species.\n\nExercise 1\nLet’s start by looking at the distribution of height_m, the typical height in meters for a Pokemon species, using a visualization and summary statistics.\n\nggplot(data = pokemon, aes(x = height_m)) +\n  geom_histogram(binwidth = 0.25, fill = \"steelblue\", color = \"black\") + \n  labs(x = \"Height (in meters)\", \n       y = \"Distributon of Pokemon heights\")\n\n\n\n\n\npokemon %&gt;%\n  summarise(mean_height = mean(height_m), \n            sd_height = sd(height_m), \n            n_pokemon = n())\n\n# A tibble: 1 × 3\n  mean_height sd_height n_pokemon\n        &lt;dbl&gt;     &lt;dbl&gt;     &lt;int&gt;\n1       0.929     0.497        42\n\n\nIn the previous lecture we were given the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), of the population. That is unrealistic in practice (if we knew \\(\\mu\\) and \\(\\sigma\\), we wouldn’t need to do statistical inference!).\nToday we will start on using the Central Limit Theorem to draw conclusions about the \\(\\mu\\), the mean height in the population of Pokemon.\n\nWhat is the point estimate for \\(\\mu\\), i.e., the “best guess” for the mean height of all Pokemon?\nWhat is the point estimate for \\(\\sigma\\), i.e., the “best guess” for the standard deviation of the distribution of Pokemon heights?\n\n\n\nExercise 2\nBefore moving forward, let’s check the conditions required to apply the Central Limit Theorem. Are the following conditions met:\n\nIndependence?\nSample size/distribution?\n\n\n\nCentral limit theorem\nRemember, when the independence and sample size assumptions are met, the central limit theorem states\n\\[\n\\bar{x} \\sim N(\\mu, \\sigma / \\sqrt{n})\n\\] If we know \\(\\sigma\\), we can construct a symmetric confidence interval for the true mean easily using qnorm().\nFor example, if the true standard deviation in pokemon height is 0.4 meters, then to construct a 95% confidence interval:\n\nxbar = pokemon %&gt;%\n  summarize(xbar = mean(height_m)) %&gt;%\n  pull(xbar)\n\nqnorm(c(0.025, 0.975), mean = xbar, sd = 0.4)\n\n[1] 0.1445858 1.7125570\n\n\nThis can be equivalently expressed\n\nzscore = qnorm(0.025)\nxbar + zscore*0.4\n\n[1] 0.1445858\n\nxbar - zscore*0.4\n\n[1] 1.712557\n\n\nwhere we use the fact that we can write any normal distribution as a linear combination of a standard normal. For example,\nif \\(X \\sim N(0.928, .4)\\), then \\(X = .4Z + 0.928\\) where \\(Z\\) is standard normal, in other words \\(Z \\sim N(0, 1)\\).\nIn general, the confidence interval can be written as\n\\[\n\\bar{x} \\pm z^* \\times \\sigma\n\\] where \\(z^*\\) is the quantile of a standard normal distribution associated with our level of confidence.\nWhat about when we don’t know \\(\\sigma\\)?\n\n\nPractical confidence intervals\nWe don’t know the true population mean \\(\\mu\\) and standard deviation \\(\\sigma\\), how do we use CLT to construct a confidence interval?\nWe approximate \\(\\mu\\) by \\(\\bar{x}\\) and \\(\\sigma\\) by the same standard deviation \\(s\\). However \\(s\\) may be smaller than \\(\\sigma\\) and our confidence interval could be too narrow, for example, run the code below to compute the standard deviation of three draws from a standard normal.\n\nset.seed(6)\nsamples = rnorm(3, mean = 0, sd = 1)\nsd(samples)\n\n[1] 0.7543284\n\n\nThis was just for 1 random seed. If you remove the seed and repeat the simulation, you will find that \\(s\\) is sometimes above and sometimes below the true standard deviation.\nTo account for this uncertainty, we will use a distribution with thicker tails. This sampling distribution is called a t-distribution.\n\nggplot(data = data.frame(x = c(0 - 1*3, 0 + 1*3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),\n                color = \"black\") +\n  stat_function(fun = dt, args = list(df = 3),\n                color = \"red\",lty = 2) + theme_bw() +\n  labs(title = \"Black solid line = normal, Red dotted line = t-distribution\")\n\n\n\n\nThe t-distribution has a bell shape but the extra thick tails help us correct for the variability introduced by using \\(s\\) instead of \\(\\sigma\\).\nThe t-distribution, like the standard normal, is always centered at zero. Therefore, the t-distribution has only a single parameter: degrees of freedom. The degrees of freedom describes the precise form of the bell-shaped t-distribution. In general, we’ll use a t-distribution with \\(df=n−1\\) to model the sample mean when the sample size is \\(n\\).\nWe can use qt and pt to find quantiles and probabilities respectively under the t-distribution.\n\n\nConfidence interval\nTo construct our practical confidence interval (where we don’t know \\(\\sigma\\)) we use the t-distribution:\n\\[\n\\bar{x} \\pm t^*_{n-1} \\times \\frac{s}{\\sqrt{n}}\n\\]\n\nExercise 3\n\nCalculate the 95% confidence interval for pokemon height using the t-distribution.\n\n\n# code here\n\nHow does this compare to a 95% bootstrap confidence interval?\n\n# code here"
  },
  {
    "objectID": "ae/ae22.html",
    "href": "ae/ae22.html",
    "title": "Regression + Inference",
    "section": "",
    "text": "Draft final project report due Friday December 2\nThis Friday is last lab before peer-review in two weeks"
  },
  {
    "objectID": "ae/ae22.html#bulletin",
    "href": "ae/ae22.html#bulletin",
    "title": "Regression + Inference",
    "section": "",
    "text": "Draft final project report due Friday December 2\nThis Friday is last lab before peer-review in two weeks"
  },
  {
    "objectID": "ae/ae22.html#today",
    "href": "ae/ae22.html#today",
    "title": "Regression + Inference",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nunderstand regression hypothesis testing\ninterpret p-values in a regression framework"
  },
  {
    "objectID": "ae/ae22.html#getting-started",
    "href": "ae/ae22.html#getting-started",
    "title": "Regression + Inference",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae22.qmd\",\ndestfile = \"ae22.qmd\")"
  },
  {
    "objectID": "ae/ae22.html#load-packages",
    "href": "ae/ae22.html#load-packages",
    "title": "Regression + Inference",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(viridis)"
  },
  {
    "objectID": "ae/ae22.html#load-data",
    "href": "ae/ae22.html#load-data",
    "title": "Regression + Inference",
    "section": "Load data",
    "text": "Load data\n\nSeoul_Bikes = read_csv(\"https://sta101.github.io/static/appex/data/Seoul_Bikes.csv\") \nSeoul_Calendar = read_csv(\"https://sta101.github.io/static/appex/data/Seoul_Calendar.csv\")\n\nbikes = left_join(Seoul_Bikes, Seoul_Calendar)\n\nThis data set was originally analyzed in two studies1 of predicting bike-rental usage in Seoul, South Korea. For this lecture, the data was sourced from UCI Machine Learning Repository.\nCode book:\n\nDate: the date\nrented_bikes: total number of bikes rented on a given day\ntemp_c: mean daily temperature (Celsius)\nhumidity_pct: mean daily humidity\nwind_speed: mean daily windspeed\nsnowfall_cm: mean daily snowfall (in cm)\nseason: the season\nholiday: whether or not the day is a holiday\n\n\nglimpse(bikes)\n\nRows: 365\nColumns: 8\n$ Date         &lt;chr&gt; \"1/1/18\", \"1/10/18\", \"1/11/18\", \"1/12/17\", \"1/2/18\", \"1/3…\n$ rented_bikes &lt;dbl&gt; 4290, 27909, 22964, 9539, 5377, 5132, 17388, 26820, 31928…\n$ temp_c       &lt;dbl&gt; -1.2833333, 15.4375000, 8.3458333, -2.4541667, -3.8666667…\n$ humidity_pct &lt;dbl&gt; 39.33333, 54.25000, 54.16667, 45.87500, 44.00000, 64.2083…\n$ wind_speed   &lt;dbl&gt; 1.4541667, 2.8250000, 1.2708333, 1.5375000, 1.6083333, 3.…\n$ snowfall_cm  &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.9041667, 0.…\n$ season       &lt;chr&gt; \"Winter\", \"Autumn\", \"Autumn\", \"Winter\", \"Winter\", \"Spring…\n$ holiday      &lt;chr&gt; \"Holiday\", \"No Holiday\", \"No Holiday\", \"No Holiday\", \"No …"
  },
  {
    "objectID": "ae/ae22.html#notes",
    "href": "ae/ae22.html#notes",
    "title": "Regression + Inference",
    "section": "Notes",
    "text": "Notes\n\nHypothesis testing in a regression framework\n\nbikes %&gt;%\n  ggplot(aes(x = temp_c, y = rented_bikes, color = holiday)) +\n  geom_point(alpha = 0.5) +\n  theme_bw() +\n  labs(x = \"Temperature\", y = \"Rented No. Bikes\", color = \"Holiday?\",\n       title = \"Rented Bicycles in Seoul\") +\n  scale_color_manual(values = c(\"red\", \"#00539B\")) +\n  geom_smooth(method = 'lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nExercise 1\nWrite the full model equation to match the figure above to predict the number of bikes rented on a given day based on the temperature outside and whether or not the day is a holiday.\n\\[\n\\text{Your equation here}\n\\]\n\n\nExercise 2\nFit the model above and examine the tidied output. What are the p-values associated with each predictor?\n\n# code here\n\nThe p-value output is associated with a typical hypothesis test… but what’s the null and alternative?\nThe main idea is that if a predictor (e.g. holiday) does not help us explain bike rental numbers then its associated \\(\\beta\\) might as well be 0. Within the framework of hypothesis testing:\n\\(H_0\\): \\(\\beta_{\\text{holiday}} = 0\\)\n\\(H_A:\\) \\(\\beta_{\\text{holiday}} \\neq 0\\)\nFor OLS regression, our test statistic is\n\\[\nT = \\frac{\\hat{\\beta} - 0}{\\text{SE}_{\\hat{\\beta}}} \\sim t_{n - 4}\n\\] We want to see if our observed statistic, \\(T\\), falls far in the tail under the null.\nR takes care of much of this behind the scenes with the tidy output and reports a p-value for each \\(\\beta\\) by default.\n\n\nExercise 3\nCalculate the p-value associated with \\(\\beta_{holiday}\\) manually using the equation above. Note: in a regression setting, the degrees of freedom is the number of observations minus the number of \\(\\beta\\)s.\nCompare the p-value to one reported in the tidy output.\n\n# code here\n\n\nIs \\(\\beta_{\\text{holiday}}\\) significant at the \\(\\alpha = 0.05\\) level? State your conclusion.\nLooking at the tidied output, are any of the \\(\\beta\\)s not significant at the \\(\\alpha = 0.05\\) level?\nChange the model from an interaction effects to a main effects model. What do you notice?"
  },
  {
    "objectID": "ae/ae3.html",
    "href": "ae/ae3.html",
    "title": "Exploratory Data Analysis I",
    "section": "",
    "text": "Lab 1 due Thursday at 11:59pm on gradescope\nHow to render to pdf directly"
  },
  {
    "objectID": "ae/ae3.html#bulletin",
    "href": "ae/ae3.html#bulletin",
    "title": "Exploratory Data Analysis I",
    "section": "",
    "text": "Lab 1 due Thursday at 11:59pm on gradescope\nHow to render to pdf directly"
  },
  {
    "objectID": "ae/ae3.html#today",
    "href": "ae/ae3.html#today",
    "title": "Exploratory Data Analysis I",
    "section": "Today",
    "text": "Today\nWe’ll begin today by visiting the last figure of ae2.\nBy the end of today you will…\n\nharness the power of filter() using logic\ncreate and interpret scatter plots, bar plots, stacked bar plots, facet plots and be able to look up and use other ggplot geometries"
  },
  {
    "objectID": "ae/ae3.html#getting-started",
    "href": "ae/ae3.html#getting-started",
    "title": "Exploratory Data Analysis I",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae3.qmd\",\ndestfile = \"ae3.qmd\")"
  },
  {
    "objectID": "ae/ae3.html#load-packages",
    "href": "ae/ae3.html#load-packages",
    "title": "Exploratory Data Analysis I",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(viridis) # we'll use to customize colors"
  },
  {
    "objectID": "ae/ae3.html#load-data",
    "href": "ae/ae3.html#load-data",
    "title": "Exploratory Data Analysis I",
    "section": "Load data",
    "text": "Load data\n\ndata(penguins)\n\nType ?palmerpenguins to learn more about this package. Or better yet, check it out here."
  },
  {
    "objectID": "ae/ae3.html#logic-in-r",
    "href": "ae/ae3.html#logic-in-r",
    "title": "Exploratory Data Analysis I",
    "section": "Logic in R",
    "text": "Logic in R\nThe table of logical operators below will be helpful as you work with filtering.\n\n\n\noperator\ndefinition\n\n\n\n\n&lt;\nis less than?\n\n\n&lt;=\nis less than or equal to?\n\n\n&gt;\nis greater than?\n\n\n&gt;=\nis greater than or equal to?\n\n\n==\nis exactly equal to?\n\n\n!=\nis not equal to?\n\n\nis.na(x)\nis x NA?\n\n\n!is.na(x)\nis x not NA?\n\n\nx %in% y\nis x in y?\n\n\n!(x %in% y)\nis x not in y?\n\n\nx & y\nis x AND y?\n\n\nx \\| y\nis x OR y?\n\n\n!x\nis not x?\n\n\n\nThe above operations return TRUE (1) or FALSE (0).\n\nExamples\nHow many penguins have flipper length &gt; 200 mm?\n\npenguins %&gt;%\n  filter(flipper_length_mm &gt; 200)\n\n# A tibble: 148 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Dream               35.7          18                 202        3550\n 2 Adelie  Dream               41.1          18.1               205        4300\n 3 Adelie  Dream               40.8          18.9               208        4300\n 4 Adelie  Biscoe              41            20                 203        4725\n 5 Adelie  Torgersen           41.4          18.5               202        3875\n 6 Adelie  Torgersen           44.1          18                 210        4000\n 7 Adelie  Dream               41.5          18.5               201        4000\n 8 Gentoo  Biscoe              46.1          13.2               211        4500\n 9 Gentoo  Biscoe              50            16.3               230        5700\n10 Gentoo  Biscoe              48.7          14.1               210        4450\n# ℹ 138 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nWe could also pipe into nrow() to quickly grab the number of rows. Try it!\n\nHow many female penguins have flipper length &gt; 200 mm?\n\npenguins %&gt;%\n  filter(flipper_length_mm &gt; 200 & (sex == \"female\"))\n\n# A tibble: 60 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Dream            35.7          18                 202        3550\n 2 Gentoo  Biscoe           46.1          13.2               211        4500\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           46.5          13.5               210        4550\n 5 Gentoo  Biscoe           45.4          14.6               211        4800\n 6 Gentoo  Biscoe           43.3          13.4               209        4400\n 7 Gentoo  Biscoe           40.9          13.7               214        4650\n 8 Gentoo  Biscoe           45.5          13.7               214        4650\n 9 Gentoo  Biscoe           45.8          14.6               210        4200\n10 Gentoo  Biscoe           42            13.5               210        4150\n# ℹ 50 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nFor how many penguins was flipper length not measured (i.e. reported as NA)?\n\npenguins %&gt;%\n  filter(is.na(flipper_length_mm))\n\n# A tibble: 2 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen             NA            NA                NA          NA\n2 Gentoo  Biscoe                NA            NA                NA          NA\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nHow many penguins are of species Adelie or Chinstrap?\n\npenguins %&gt;%\n  filter(species %in% c(\"Adelie\", \"Chinstrap\"))\n\n# A tibble: 220 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 210 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nExercise 1:\nDouble check the Adelie and Chinstrap total using count().\n\n# code here\n\n\n\nExercise 2:\nWhat is the mean bill length of Adelie penguins? Hint: filter and then use summarize() as seen in lab 1.\n\n# code here\n\n\n\nExercise 3:\nHow many penguins have a bill length greater than 40 mm or a bill depth less than 15 mm?\n\n# code here\n\n\n\nExercise 4:\nWhat proportion of penguins are from the island Torgersen?\n\n# code here"
  },
  {
    "objectID": "ae/ae3.html#plots",
    "href": "ae/ae3.html#plots",
    "title": "Exploratory Data Analysis I",
    "section": "Plots",
    "text": "Plots\nThe procedure used to construct plots can be summarized using the code below.\n\nggplot(data = [data set], \n       mapping = aes(x = [x-variable], y = [y-variable])) +\n   geom_xxx() +\n   geom_xxx() + \n  other options\n\n\nExample: bar plot\n\nggplot(data = penguins, \n       mapping = aes(x = species)) +\n  geom_bar() +\n  labs(x = \"Species\", y = \"Count\", title = \"Palmer penguin species\")\n\n\n\n\n\n\nExample: stacked bar plot\n\npenguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\nggplot(mapping = aes(x = species, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Species\", y = \"Count\", title = \"Palmer penguin species\")\n\n\n\n\n\ntry with and without position = \"fill\"\n\n\n\nAesthetics\nAn aesthetic is a visual property in your plot that is derived from the data.\n\nshape\ncolor\nsize\nalpha (transparency)\n\nWe can map a variable in our data set to a color, a size, a transparency, and so on. The aesthetics that can be used with each geom_ can be found in the documentation.\nHere we are going to use the viridis package, which has more color-blind accessible colors. scale_color_viridis specifies which colors you want to use. You can learn more about the options here.\nOther sources that can be helpful in devising accessible color schemes include Color Brewer, the Wes Anderson package, and the cividis package.\nThis visualization shows a scatterplot of bill length (x variable) and flipper length (y variable). Using the viridis function, we make points for male penguins purple and female penguins yellow. We also add axes labels and a title.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = flipper_length_mm,\n                     color = sex)) + \n   geom_point() + \n   labs(title = \"Flipper length vs bill length\",\n        x = \"Bill length (mm)\", y = \"Flipper length (mm)\") + \n        scale_color_viridis(discrete=TRUE, option = \"D\", name=\"Sex\")\n\nWarning: Removed 11 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nExercise 5:\nCan you remove the NAs from the above visualization?\nQuestion: What will the visualization look like below? Write your answer down before running the code.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = flipper_length_mm,\n                     shape = sex)) + \n   geom_point() + \n   labs(title = \"Flipper length vs bill length\",\n        x = \"Bill length (mm)\", y = \"Flipper length (mm)\") + \n        scale_color_viridis(discrete=TRUE, option = \"D\", name=\"Sex\")\n\n\n\n\nFaceting\nWe can use smaller plots to display different subsets of the data using faceting. This is helpful to examine conditional relationships.\n\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm, flipper_length_mm, color = island)) +\n  geom_point() +\n  facet_wrap(~ species) +\n  labs(x = \"Bill length (mm)\", y = \"Flipper length (mm)\", color = \"Island\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm, flipper_length_mm, color = species)) +\n  geom_point() +\n  facet_wrap(~ island) +\n  labs(x = \"Bill length (mm)\", y = \"Flipper length (mm)\", color = \"Island\") +\n  scale_color_viridis(discrete = TRUE)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "ae/ae3.html#ggplot-activity",
    "href": "ae/ae3.html#ggplot-activity",
    "title": "Exploratory Data Analysis I",
    "section": "ggplot activity",
    "text": "ggplot activity\n\n# code here"
  },
  {
    "objectID": "ae/ae3.html#additional-resources",
    "href": "ae/ae3.html#additional-resources",
    "title": "Exploratory Data Analysis I",
    "section": "Additional resources",
    "text": "Additional resources\n\nFind more ggplot geometries at https://ggplot2.tidyverse.org/reference/"
  },
  {
    "objectID": "ae/ae18.html",
    "href": "ae/ae18.html",
    "title": "Intro to hypothesis tests",
    "section": "",
    "text": "Lab 7 due Thursday"
  },
  {
    "objectID": "ae/ae18.html#bulletin",
    "href": "ae/ae18.html#bulletin",
    "title": "Intro to hypothesis tests",
    "section": "",
    "text": "Lab 7 due Thursday"
  },
  {
    "objectID": "ae/ae18.html#today",
    "href": "ae/ae18.html#today",
    "title": "Intro to hypothesis tests",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nBe familiar with the terms “p-value”, “null-distribution”, “null hypothesis”, “alternative hypothesis”\nCompute a p-value"
  },
  {
    "objectID": "ae/ae18.html#getting-started",
    "href": "ae/ae18.html#getting-started",
    "title": "Intro to hypothesis tests",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae18.qmd\",\ndestfile = \"ae18.qmd\")"
  },
  {
    "objectID": "ae/ae18.html#load-packages",
    "href": "ae/ae18.html#load-packages",
    "title": "Intro to hypothesis tests",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae18.html#introduction-to-hypothesis-testing",
    "href": "ae/ae18.html#introduction-to-hypothesis-testing",
    "title": "Intro to hypothesis tests",
    "section": "Introduction to hypothesis testing",
    "text": "Introduction to hypothesis testing\n\nIs this a fair coin?\nWe’ll record 1 if the coin is “Heads” and 0 if the coin lands “Tails”.\n\n# coin_flips = c()\n\nIf the coin is fair, what is the probability of seeing the outcome we saw? To answer this question we’ll setup a statistical model:\n\\[\n\\text{\\# heads} \\sim Binomial(n, p)\n\\]\nwhere \\(p\\) is the probability of a heads and \\(n\\) is the total number of coin flips.\n\nExercise 2\n\nIf the coin is fair, what would \\(p\\) be?\nUsing R, flip a fair coin 6 times and count the number of heads. Next, repeat this experiment 1000 times and count the proportion of times you observe 6 heads.\n\n\n# code here\n\n\nWhat is the probability of observing 6 heads in 6 coin flips?"
  },
  {
    "objectID": "ae/ae18.html#hypothesis-testing-framework",
    "href": "ae/ae18.html#hypothesis-testing-framework",
    "title": "Intro to hypothesis tests",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\nYou may not have realized it but you just performed a hypothesis test!\nYou setup a null hypothesis: \\(H_0\\). The null hypothesis is a hypothesis you set up and then try and knock down. Conceptually, it’s the “nothing special is going on” hypothesis. Formally, the null hypothesis makes a claim or assumption about a population parameter.\nIn this case, it was the assumption that the coin is fair. Mathematically, we write this:\n\\[\nH_0: p = 0.5\n\\]\nConceptually, \\(p\\) is the probability of flipping a head if we flipped the coin infinitely many times.\nSince we computed the probability of observing all heads, we were fundamentally interested in if \\(p &gt; 0.5\\). This is our alternative hypothesis, \\(H_A\\). In mathematical notation, we write\n\\[\nH_A: p &gt; 0.5\n\\]\nNext, we simulated under the null. This means that we simulated what the coin flips would have looked like if the null was true. In this context, this means we simulated as if the coin was fair.\nFinally, we check to see where our actual observed data places under the null distribution. If it’s way out in the tail, we reject the null. If its not way out in the tail, we fail to reject the null.\nHow can we make “way out in the tail” more precise? Well, it’s arbitrary and context-dependent. In some contexts it is popular to use a cut-off of \\(0.05\\). This cutoff is called “the significance level” and is also known as \\(\\alpha\\).\n\nExercise 3\nAssume we continue flipping our coin for a total of 30 coin flips and observe 23 heads and 7 tails. What is the probability of seeing 23 or more heads if the coin is fair?\n\n# code here"
  },
  {
    "objectID": "ae/ae18.html#p-values",
    "href": "ae/ae18.html#p-values",
    "title": "Intro to hypothesis tests",
    "section": "p-values",
    "text": "p-values\nYou might not realize it, but you just computed a p-value… again!\nA p-value is a probability. It’s the tail probability associated with your alternative hypothesis.\nThe alternative hypothesis must always relate to the null. Here we had three options:\n\n\\(H_A: p &lt; 0.5\\), the coin is biased to land tails\n\\(H_A: p &gt; 0.5\\), the coin is biased to land heads\n\\(H_A: p \\neq 0.5\\), the coin is biased\n\nLet’s look offline at what each one would like here.\n\nExercise 4\nCompute the p-value associated with each of the alternative hypotheses above.\n\n# code here\n\nMake a conclusion based on a significance level of 0.05. In other words,\n\nif p &lt; 0.05, reject the null.\nif p &gt; 0.05, we fail to reject the null.\n\nAs you can see our conclusion depends on our alternative hypothesis. For this reason, it is important to set up an alternative hypothesis before looking at the data.\n\n\nRecap\nWe were interested in whether or not a coin was fair. We let \\(p\\) be the probability of landing heads. Fundamentally, we were interested in whether or not \\(p = 0.5\\). This was our null hypothesis:\n\\[\nH_0: p = .5\n\\]\nand our alternative, was that the coin was biased heads: \\[\nH_A: p &gt; 0.5\n\\]\nIn one example our data consisted of 30 coin flips and 23 heads. The proportion of heads that we observed, \\(\\hat{p} = 23/30 = .77\\).\nDo these 30 coin flips give us enough evidence to reject the null in favor of the alternative?\nTo answer this question, we computed the p-value: \\(Pr(\\hat{p} \\geq .77 | H_0 \\text{ true})\\). In words, the probability that our statistic of interest, (\\(\\hat{p}\\)), is greater than or equal to what we saw given that the null is true.\nNotice that the p-value is defined by three things:\n\nour observed statistic (0.77)\nthe null hypothesis (\\(H_0\\))\nthe alternative hypothesis (\\(H_A\\)), this tells us the direction (\\(&gt;=\\)) to shade.\n\nWe compared the p-value to some pre-defined cutoff, \\(\\alpha\\). In our example we set our cutoff at \\(\\alpha = 0.05\\). If p-value \\(&lt; \\alpha\\), we reject the null. If p-value \\(&gt; \\alpha\\), we fail to reject the null.\n\n\nThe tidy way\n\ncoin_flips = data.frame(one_flip = sample(c(rep(\"H\",23), rep(\"T\", 7)), size = 30))\n\nglimpse(coin_flips)\n\nRows: 30\nColumns: 1\n$ one_flip &lt;chr&gt; \"H\", \"H\", \"H\", \"T\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"T\", \"H\", \"…\n\n\n\nset.seed(2022)\nnull_dist = \n  coin_flips %&gt;% \n  specify(response = one_flip, success = \"H\") %&gt;%\n  hypothesize(null = \"point\", p = 0.5) %&gt;%\n  generate(reps = 10000, type = \"draw\") %&gt;%\n  calculate(stat = \"prop\")\n\nobs_stat = 23/30\n\nvisualize(null_dist) +\n  shade_p_value(obs_stat, direction = \"right\")\n\n\n\nnull_dist %&gt;%\nget_p_value(obs_stat, direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0027\n\n\np-value of \\(0.0027 &lt; 0.05\\). We reject the null hypothesis that the coin is fair."
  },
  {
    "objectID": "ae/ae9.html",
    "href": "ae/ae9.html",
    "title": "Model Selection",
    "section": "",
    "text": "Please take the first five minutes of class to fill out this survey\nMy Wednesday office hours are now zoom only for the rest of the semester. Monday will remain hybrid."
  },
  {
    "objectID": "ae/ae9.html#bulletin",
    "href": "ae/ae9.html#bulletin",
    "title": "Model Selection",
    "section": "",
    "text": "Please take the first five minutes of class to fill out this survey\nMy Wednesday office hours are now zoom only for the rest of the semester. Monday will remain hybrid."
  },
  {
    "objectID": "ae/ae9.html#today",
    "href": "ae/ae9.html#today",
    "title": "Model Selection",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nselect between linear models with different numbers of predictors"
  },
  {
    "objectID": "ae/ae9.html#getting-started",
    "href": "ae/ae9.html#getting-started",
    "title": "Model Selection",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae9.qmd\",\ndestfile = \"ae9.qmd\")"
  },
  {
    "objectID": "ae/ae9.html#load-packages-and-data",
    "href": "ae/ae9.html#load-packages-and-data",
    "title": "Model Selection",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae9.html#notes",
    "href": "ae/ae9.html#notes",
    "title": "Model Selection",
    "section": "Notes",
    "text": "Notes\n\nThe problem with \\(R^2\\)\n\\(R^2\\) tell us the proportion of variability in the data our model explains. If we add predictors to our model, we will always improve \\(R^2\\) (regardless of whether the predictor is good or not).\nTo see this…\n\noffline example\ntake away: a line can go through any two points, a plane can go through any three points, etc. In general an \\(n\\) dimensional object can go through \\(n\\) points.\n\nFor this reason, \\(R^2\\) is not a good way to select between two models that have a different number of predictors. Instead, we prefer to use Akaike Information Criterion (AIC).\n\n\nAIC\n\\[\n\\text{AIC} = 2k - 2 \\log (\\text{likelihood})\n\\]\nwhere \\(k\\) is the number of estimated parameters (\\(\\beta\\)s) in the model. Notice this will be 1 + the number of predictors. and \\(\\hat{L}\\) is “likelihood” of the data given the fitted model.\nWe’ll return to the idea of a likelihood later in the semester. For now, it suffices to know that the likelihood is a measure of how well a given model fits the data. Specifically, higher likelihoods imply better fits. Since the AIC score has a negative in front of the log likelihood, lower scores are better fits. However, \\(k\\) penalizes adding new predictors to the model.\nTake-away: lower AIC is better fit.\nYou can find AIC using glance(fitted-model). (Assuming you named your fitted model fitted-model)"
  },
  {
    "objectID": "ae/ae9.html#building-a-model",
    "href": "ae/ae9.html#building-a-model",
    "title": "Model Selection",
    "section": "Building a model",
    "text": "Building a model\nScenario: you have an outcome \\(y\\) you want to predict. You have several variables you’ve measured that you could use as predictors in your linear model. Each predictor is expensive to collect future measurements of. You want your model to only include the most useful predictors.\n\nBackward elimination\nBackward elimination starts with the full model (the model that includes all potential predictor variables). Variables are eliminated one-at-a-time from the model until we cannot improve the model any further.1\nProcedure:\n\nStart with a model that has all predictors we consider and compute the AIC.\nNext fit every possible model with 1 less predictor.\nCompare AIC scores to select the best model with 1 less predictor.\nRepeat steps 2 and 3 until you score the model with no predictors.\nCompare AIC among all tested models to select the best model.\n\n\n\nForward selection\nForward selection is the reverse of the backward elimination technique. Instead, of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that improve the model any further.\nProcedure:\n\nStart with a model that has no predictors.\nNext fit every possible model with 1 additional predictor and score each model.\nCompare AIC scores to select the best model with 1 additional predictor.\nRepeat steps 2 and 3 until you score the model with all available predictors.\nCompare AIC among all tested models to select the best model."
  },
  {
    "objectID": "ae/ae9.html#example",
    "href": "ae/ae9.html#example",
    "title": "Model Selection",
    "section": "Example",
    "text": "Example\n\nExercise\n\nWill forward selection and backward elimination always yield the same model? Type your answer below before running any code.\nNext, see if you are right in using the data set below.\n\nSolution below\n\ntest_df = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/test_df.csv\")\n\nRows: 20 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): y, x1, x2, x3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIn the following two examples, we will use stepwise selection to build a main effects model.\nPerform 1 step of forward selection. What variable will be in the final forward selection model?\n\nAnswer: \\(x_1\\)\n\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x1, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 93.08637\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x2, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 131.8917\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x3, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 152.9043\n\n\nNext, perform 1 step of backward elimination. Which variable will not be in the final backward elimination model?\n\nAnswer: \\(x_1\\)\n\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x2 + x3, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 35.25949\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x1 + x3, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 94.2536\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x1 + x2, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 87.1686"
  }
]