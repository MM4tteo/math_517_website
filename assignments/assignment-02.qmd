---
title: "Assignment 2"
subtitle: "Bandwidth selection in Kernel Density Estimation"
date: 10-06-2023
format: 
  html:
    code-fold: true
    code-tools: true
editor: visual
number-sections: true
---
::: callout-important
**Due date: 11:59pm on Sunday, 15 October 2023.**
:::

::: callout-note
Use the corresponding invite link in [this google doc](https://docs.google.com/document/d/1XTO7obXEr2Yxllxk48wnZ5e49ptPGMsMyl40KEZDGRg/edit?usp=sharing) (accessible with your EPFL account) to accept the assignment. Then go to the [course GitHub organization](https://github.com/MATH-517) and locate the repo titled `ae-2-YOUR_GITHUB_USERNAME` to get started.
:::


## Recap: Kernel Density Estimation

Remember  that the Kernel Density Estimation (KDE) of $f$ based on $X_1,\ldots,X_N$ is
$$\widehat{f}(x) = \frac{1}{n h_n} \sum_{i=1}^n K\left(\frac{X_i - x}{h_n} \right),$$
where the \textbf{kernel} $K(\cdot)$ satisfies:

:::: {.columns}

::: {.column width="40%"}
1. $K(x) \geq 0$ for all $x \in \mathbb{R}$
2. $K(- x) = K(x)$ for all $x \in \mathbb{R}$
3. $\int_\mathbb{R} K(x) d x = 1$
:::

::: {.column width="10%"}
<!-- empty column to create gap -->
:::

::: {.column width="40%"}

4. $\lim_{|x| \to \infty} |x| K(x) = 0$
5. $\sup_x |K(x)| < \infty.$
:::

::::


```{r, fig.dim=c(10,2.5)}
#| fig-cap: "Impact of the bandwidth on KDE, where the Gaussian kernel is in black, the Epanechnikov kernel in blue and the rectangular kernel in red."
plot_kdes <- function(bw){
  plot(density(faithful$eruptions, kernel="gaussian", bw=bw),
       main=paste("bandwidth = ",bw,sep=""), xlab="time [min]")
  lines(density(faithful$eruptions, kernel="epanechnikov", bw=bw), col=4)
  lines(density(faithful$eruptions, kernel="rectangular", bw=bw), col=2)
}
par(mfrow=c(1,4), mar = c(3.2, 3, 1.6, 0.2))
plot_kdes(0.75); plot_kdes(0.5); plot_kdes(0.25); plot_kdes(0.1)
```

where the kernels are defined as:

| Kernel Name                | Formula                                               |
|----------------------------|-------------------------------------------------------|
| Epanechnikov               | $K(x) \propto (1-x^2) \mathbb{1}_{[|x| \leq 1]}$      |
| Gaussian                   | $K(x) \propto \exp(-x^2/2)$                           |
| Rectangular                | $K(x)=\frac{1}{2} 1_{\{-1<x<1\}}$              |

## Task

We saw in the lecture that the choice of the bandwidth $h$ is crucial for the performance of the KDE. In this assignment we will explore this in more detail with a simulation study. Specifically:

* repeat the following $200$ times:
  1. generate $N=100$ samples from a Gaussian mixture [^1] (see @sec-GaussianMixture)
  2. perform density estimation, i.e., obtain $\widehat{f}$, for
     - Gaussian, Epanechnikov, and rectangular kernels
     - bandwidth values $h = 0.1,0.15,0.2,0.25,\ldots,0.9$
  3. calculate the error measure $\| f - \widehat{f} \|_2$
* report your findings as a single (well commented) figure [^2]


## Gaussian mixture {#sec-GaussianMixture}

Let $X^{(1)}, \dots, X^{(N)}$ be i.i.d. random variables each with pdf $$ f_{\boldsymbol{\theta}}(x) = (1-\tau) \ \varphi_{\mu_1, \sigma_1}\left(x \right) + \tau \ \varphi_{\mu_2, \sigma_2}\left(x \right) $$ where $\boldsymbol{\theta} = (\tau, \mu_1,\mu_2,\sigma_1^2, \sigma_2^2)^\top$, with

-   $\varphi_{\mu, \sigma}$ is the pdf of a Gaussian with mean $\mu$ and standard deviation $\sigma$,
-   $\mu_1, \mu_2$ and $\sigma_1^2, \sigma_2^2$ are the means and variances of the mixture components, and
-   $\tau \in (0,1)$ is the mixing proportion



$$
  \ell_{obs}(\boldsymbol{\theta}) = \sum_{n=1}^N \log \left\lbrace (1-\tau)\, \varphi_{\mu_1, \sigma_1}\left(X_n\right) {\boldsymbol{+}} \tau\, \varphi_{\mu_2, \sigma_2}\left(X_n\right) \right\rbrace
$$ **Trick**: add latent i.i.d. indicators $Z^{(n)} \sim \operatorname{Bernoulli}(\tau)$ such that $X^{(n)} \mid Z^{(n)} = 0 \sim N(\mu_1, \sigma_1^2)$ and $X^{(n)} \mid Z^{(n)} = 1 \sim N(\mu_2, \sigma_2^2)$.

Given $Z^{(n)} = z^{(n)}$, $n=1, \dots, N$, the joint likelihood can be written as $$
  L_{comp}(\boldsymbol{\theta}) = (1-\tau)^{N_1} \tau^{N_2} \prod_{n=1}^{N} \varphi_{\mu_1, \sigma_1}\left( X^{(n)}\right)^{(1-Z^{(n)})}  \varphi_{\mu_2, \sigma_2}\left( X^{(n)}\right)^{Z^{(n)}}
$$ with $N_2 = \sum_{n=1}^{N} Z^{(n)}$ and $N_1 = N - N_2$.

The two functions below allow for random number generation and density evaluation for the Gaussian mixture distribution 

```{r}
#| code-fold: show

# random generation for a mixture of two normal distributions
rmixnorm <- function(N, mu1, mu2, sigma1, sigma2, tau){
  ind <- I(runif(N) > tau)
  X <- rep(0,N)
  X[ind] <- rnorm(sum(ind), mu1, sigma1)
  X[!ind] <- rnorm(sum(!ind), mu2, sigma2)
  return(X)
}

# density evaluation for a mixture of two normal distributions
dmixnorm <- function(x, mu1, mu2, sigma1, sigma2, tau){
  y <- (1-tau)*dnorm(x,mu1,sigma1) + tau*dnorm(x,mu2,sigma2)
  return(y)
}
```

A sample call is below.


```{r}
library(ggplot2)

N <- 300
mu1 <- 3
mu2 <- 0
sigma1 <- 0.5
sigma2 <- 1
tau <- 0.6

X <- rmixnorm(N, mu1, mu2, sigma1, sigma2, tau)
x <- seq(-3, 6, by = 0.01)
fx <- dmixnorm(x, mu1, mu2, sigma1, sigma2, tau)

ggplot() +
    theme_bw() +
    aes(X, after_stat(density)) +
    geom_histogram(colour = "#999999", fill = "#999999", binwidth = 0.1) + 
    geom_line(aes(x, fx), colour = "#E69F00", linewidth = 1) + 
    labs(
    title = "Gaussian mixture density and histogram of 300 samples",
    subtitle = "generated from a Gaussian mixture with tau = 0.6")    
```

Similar functions will be provided for Python and Julia in the assignment repository.


[^1]: The Gaussian mixture parameters can be chosen as you like, but make sure to explore different settings.
[^2]: Remember the [lecture of week 2](../lectures/02_Exploration.pdf)
