{
  "hash": "446c87166b4abb24450d5c0e8fd5f409",
  "result": {
    "markdown": "---\ntitle: \"Assignment 2\"\nsubtitle: \"Bandwidth selection in Kernel Density Estimation\"\ndate: 10-06-2023\nformat: \n  html:\n    code-fold: true\n    code-tools: true\neditor: visual\nnumber-sections: true\n---\n\n::: callout-important\n**Due date: 11:59pm on Sunday, 15 October 2023.**\n:::\n\n::: callout-note\nUse the corresponding invite link in [this google doc](https://docs.google.com/document/d/1XTO7obXEr2Yxllxk48wnZ5e49ptPGMsMyl40KEZDGRg/edit?usp=sharing) (accessible with your EPFL account) to accept the assignment. Then go to the [course GitHub organization](https://github.com/MATH-517) and locate the repo titled `ae-2-YOUR_GITHUB_USERNAME` to get started.\n:::\n\n\n## Recap: Kernel Density Estimation\n\nRemember  that the Kernel Density Estimation (KDE) of $f$ based on $X_1,\\ldots,X_N$ is\n$$\\widehat{f}(x) = \\frac{1}{n h_n} \\sum_{i=1}^n K\\left(\\frac{X_i - x}{h_n} \\right),$$\nwhere the \\textbf{kernel} $K(\\cdot)$ satisfies:\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n1. $K(x) \\geq 0$ for all $x \\in \\mathbb{R}$\n2. $K(- x) = K(x)$ for all $x \\in \\mathbb{R}$\n3. $\\int_\\mathbb{R} K(x) d x = 1$\n:::\n\n::: {.column width=\"10%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"40%\"}\n\n4. $\\lim_{|x| \\to \\infty} |x| K(x) = 0$\n5. $\\sup_x |K(x)| < \\infty.$\n:::\n\n::::\n\n\n\n::: {.cell fig.dim='[10,2.5]'}\n\n```{.r .cell-code}\nplot_kdes <- function(bw){\n  plot(density(faithful$eruptions, kernel=\"gaussian\", bw=bw),\n       main=paste(\"bandwidth = \",bw,sep=\"\"), xlab=\"time [min]\")\n  lines(density(faithful$eruptions, kernel=\"epanechnikov\", bw=bw), col=4)\n  lines(density(faithful$eruptions, kernel=\"rectangular\", bw=bw), col=2)\n}\npar(mfrow=c(1,4), mar = c(3.2, 3, 1.6, 0.2))\nplot_kdes(0.75); plot_kdes(0.5); plot_kdes(0.25); plot_kdes(0.1)\n```\n\n::: {.cell-output-display}\n![Impact of the bandwidth on KDE, where the Gaussian kernel is in black, the Epanechnikov kernel in blue and the rectangular kernel in red.](assignment-02_files/figure-html/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\nwhere the kernels are defined as:\n\n| Kernel Name                | Formula                                               |\n|----------------------------|-------------------------------------------------------|\n| Epanechnikov               | $K(x) \\propto (1-x^2) \\mathbb{1}_{[|x| \\leq 1]}$      |\n| Gaussian                   | $K(x) \\propto \\exp(-x^2/2)$                           |\n| Rectangular                | $K(x)=\\frac{1}{2} 1_{\\{-1<x<1\\}}$              |\n\n## Task\n\nWe saw in the lecture that the choice of the bandwidth $h$ is crucial for the performance of the KDE. In this assignment we will explore this in more detail with a simulation study. Specifically:\n\n* repeat the following $200$ times:\n  - generate $N=100$ samples from the Gaussian mixture (see @sec-GaussianMixture) [^1]\n  - perform density estimation, i.e., obtain $\\widehat{f}$, for\n    - Gaussian, Epanechnikov, and rectangular kernels\n    - bandwidth values $h = 0.1,0.15,0.2,0.25,\\ldots,0.9$\n  - calculate the error measure $\\| f - \\widehat{f} \\|_2$\n* report your findings as a single (well commented) figure [^2]\n\n\n## Gaussian mixture {#sec-GaussianMixture}\n\nLet $X^{(1)}, \\dots, X^{(N)}$ be i.i.d. random variables each with pdf $$ f_{\\boldsymbol{\\theta}}(x) = (1-\\tau) \\ \\varphi_{\\mu_1, \\sigma_1}\\left(x \\right) + \\tau \\ \\varphi_{\\mu_2, \\sigma_2}\\left(x \\right) $$ where $\\boldsymbol{\\theta} = (\\tau, \\mu_1,\\mu_2,\\sigma_1^2, \\sigma_2^2)^\\top$, with\n\n-   $\\varphi_{\\mu, \\sigma}$ is the pdf of a Gaussian with mean $\\mu$ and standard deviation $\\sigma$,\n-   $\\mu_1, \\mu_2$ and $\\sigma_1^2, \\sigma_2^2$ are the means and variances of the mixture components, and\n-   $\\tau \\in (0,1)$ is the mixing proportion\n\n\n\n$$\n  \\ell_{obs}(\\boldsymbol{\\theta}) = \\sum_{n=1}^N \\log \\left\\lbrace (1-\\tau)\\, \\varphi_{\\mu_1, \\sigma_1}\\left(X_n\\right) {\\boldsymbol{+}} \\tau\\, \\varphi_{\\mu_2, \\sigma_2}\\left(X_n\\right) \\right\\rbrace\n$$ **Trick**: add latent i.i.d. indicators $Z^{(n)} \\sim \\operatorname{Bernoulli}(\\tau)$ such that $X^{(n)} \\mid Z^{(n)} = 0 \\sim N(\\mu_1, \\sigma_1^2)$ and $X^{(n)} \\mid Z^{(n)} = 1 \\sim N(\\mu_2, \\sigma_2^2)$.\n\nGiven $Z^{(n)} = z^{(n)}$, $n=1, \\dots, N$, the joint likelihood can be written as $$\n  L_{comp}(\\boldsymbol{\\theta}) = (1-\\tau)^{N_1} \\tau^{N_2} \\prod_{n=1}^{N} \\varphi_{\\mu_1, \\sigma_1}\\left( X^{(n)}\\right)^{(1-Z^{(n)})}  \\varphi_{\\mu_2, \\sigma_2}\\left( X^{(n)}\\right)^{Z^{(n)}}\n$$ with $N_2 = \\sum_{n=1}^{N} Z^{(n)}$ and $N_1 = N - N_2$.\n\nThe two functions below allow for random number generation and density evaluation for the Gaussian mixture distribution \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n# random generation for a mixture of two normal distributions\nrmixnorm <- function(N, mu1, mu2, sigma1, sigma2, tau){\n  ind <- I(runif(N) > tau)\n  X <- rep(0,N)\n  X[ind] <- rnorm(sum(ind), mu1, sigma1)\n  X[!ind] <- rnorm(sum(!ind), mu2, sigma2)\n  return(X)\n}\n\n# density evaluation for a mixture of two normal distributions\ndmixnorm <- function(x, mu1, mu2, sigma1, sigma2, tau){\n  y <- (1-tau)*dnorm(x,mu1,sigma1) + tau*dnorm(x,mu2,sigma2)\n  return(y)\n}\n```\n:::\n\n\nA sample call is below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nN <- 300\nmu1 <- 3\nmu2 <- 0\nsigma1 <- 0.5\nsigma2 <- 1\ntau <- 0.6\n\nX <- rmixnorm(N, mu1, mu2, sigma1, sigma2, tau)\nx <- seq(-3, 6, by = 0.01)\nfx <- dmixnorm(x, mu1, mu2, sigma1, sigma2, tau)\n\nggplot() +\n    theme_bw() +\n    aes(X, after_stat(density)) +\n    geom_histogram(colour = \"#999999\", fill = \"#999999\", binwidth = 0.1) + \n    geom_line(aes(x, fx), colour = \"#E69F00\", linewidth = 1) + \n    labs(\n    title = \"Gaussian mixture density and histogram of 300 samples\",\n    subtitle = \"generated from a Gaussian mixture with tau = 0.6\")    \n```\n\n::: {.cell-output-display}\n![](assignment-02_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nSimilar functions will be provided for Python and Julia in the assignment repository.\n\n\n[^1]: The Gaussian mixture parameters can be choosen as you like, but make sure that the mixture components are well separated, i.e., the means are sufficiently far apart. Check what happens if you choose the means too close together.\n[^2]: Remember the [lecture of week 2](../lectures/02_Exploration.pdf)",
    "supporting": [
      "assignment-02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}